{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/meta_rule/')\n",
    "\n",
    "from lnn_operators import and_lukasiewicz, or_lukasiewicz, negation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xorLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack):\n",
    "        super(xorLNN, self).__init__()\n",
    "        self.op_and1 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_and2 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_or = or_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = x[:,0].view(-1,1)\n",
    "        x1 = x[:,1].view(-1,1)\n",
    "        yhat = self.op_or(torch.cat((self.op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            self.op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xorLNN(0.8, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.00041395798325538635\n",
      "Iteration 1: 0.0003413597878534347\n",
      "Iteration 2: 0.00027990160742774606\n",
      "Iteration 3: 0.00022847841319162399\n",
      "Iteration 4: 0.00018598556926008314\n",
      "Iteration 5: 0.00015122962940949947\n",
      "Iteration 6: 0.0001230025663971901\n",
      "Iteration 7: 0.00010024568473454565\n",
      "Iteration 8: 8.197502756956965e-05\n",
      "Iteration 9: 6.734087219228968e-05\n",
      "Iteration 10: 5.5627755500609055e-05\n",
      "Iteration 11: 4.625439760275185e-05\n",
      "Iteration 12: 3.872895103995688e-05\n",
      "Iteration 13: 3.267884312663227e-05\n",
      "Iteration 14: 2.7761290766648017e-05\n",
      "Iteration 15: 2.3767666789353825e-05\n",
      "Iteration 16: 2.051913361356128e-05\n",
      "Iteration 17: 1.7821967048803344e-05\n",
      "Iteration 18: 1.5646355677745305e-05\n",
      "Iteration 19: 1.3783679605694488e-05\n",
      "Iteration 20: 1.2263739336049184e-05\n",
      "Iteration 21: 1.0967321941279806e-05\n",
      "Iteration 22: 9.89442560239695e-06\n",
      "Iteration 23: 8.970543603936676e-06\n",
      "Iteration 24: 8.165872714016587e-06\n",
      "Iteration 25: 7.480413842131384e-06\n",
      "Iteration 26: 6.899264008097816e-06\n",
      "Iteration 27: 6.407521595974686e-06\n",
      "Iteration 28: 5.975385647616349e-06\n",
      "Iteration 29: 5.587952728092205e-06\n",
      "Iteration 30: 5.260125362838153e-06\n",
      "Iteration 31: 4.932297997584101e-06\n",
      "Iteration 32: 4.678976893046638e-06\n",
      "Iteration 33: 4.395853011374129e-06\n",
      "Iteration 34: 4.231939783494454e-06\n",
      "Iteration 35: 4.023322162538534e-06\n",
      "Iteration 36: 3.829606612271164e-06\n",
      "Iteration 37: 3.6805943182116607e-06\n",
      "Iteration 38: 3.561384346539853e-06\n",
      "Iteration 39: 3.4421748296153964e-06\n",
      "Iteration 40: 3.322964857943589e-06\n",
      "Iteration 41: 3.21865650221298e-06\n",
      "Iteration 42: 3.1292493076762185e-06\n",
      "Iteration 43: 3.024940724571934e-06\n",
      "Iteration 44: 2.920632368841325e-06\n",
      "Iteration 45: 2.831224946930888e-06\n",
      "Iteration 46: 2.786521463349345e-06\n",
      "Iteration 47: 2.7120154300064314e-06\n",
      "Iteration 48: 2.667311719051213e-06\n",
      "Iteration 49: 2.5928056857082993e-06\n",
      "Iteration 50: 2.563003363320604e-06\n",
      "Iteration 51: 2.4735961687838426e-06\n",
      "Iteration 52: 2.4437938463961473e-06\n",
      "Iteration 53: 2.3692878130532335e-06\n",
      "Iteration 54: 2.354386651859386e-06\n",
      "Iteration 55: 2.324584102098015e-06\n",
      "Iteration 56: 2.2500780687551014e-06\n",
      "Iteration 57: 2.2500780687551014e-06\n",
      "Iteration 58: 2.2351769075612538e-06\n",
      "Iteration 59: 2.145769485650817e-06\n",
      "Iteration 60: 2.1308683244569693e-06\n",
      "Iteration 61: 2.1308683244569693e-06\n",
      "Iteration 62: 2.056362518487731e-06\n",
      "Iteration 63: 2.0265601961000357e-06\n",
      "Iteration 64: 2.0265601961000357e-06\n",
      "Iteration 65: 2.0116588075325126e-06\n",
      "Iteration 66: 1.9371530015632743e-06\n",
      "Iteration 67: 1.9371530015632743e-06\n",
      "Iteration 68: 1.907350679175579e-06\n",
      "Iteration 69: 1.907350679175579e-06\n",
      "Iteration 70: 1.8924494042948936e-06\n",
      "Iteration 71: 1.8328446458326653e-06\n",
      "Iteration 72: 1.8179434846388176e-06\n",
      "Iteration 73: 1.80304232344497e-06\n",
      "Iteration 74: 1.7881411622511223e-06\n",
      "Iteration 75: 1.7881411622511223e-06\n",
      "Iteration 76: 1.7136352425950463e-06\n",
      "Iteration 77: 1.7136352425950463e-06\n",
      "Iteration 78: 1.7136352425950463e-06\n",
      "Iteration 79: 1.698733967714361e-06\n",
      "Iteration 80: 1.6838328065205133e-06\n",
      "Iteration 81: 1.6689316453266656e-06\n",
      "Iteration 82: 1.6093267731775995e-06\n",
      "Iteration 83: 1.6093267731775995e-06\n",
      "Iteration 84: 1.5944256119837519e-06\n",
      "Iteration 85: 1.5944256119837519e-06\n",
      "Iteration 86: 1.5944256119837519e-06\n",
      "Iteration 87: 1.5795244507899042e-06\n",
      "Iteration 88: 1.5646232895960566e-06\n",
      "Iteration 89: 1.4901173699399806e-06\n",
      "Iteration 90: 1.4901173699399806e-06\n",
      "Iteration 91: 1.4901173699399806e-06\n",
      "Iteration 92: 1.475216208746133e-06\n",
      "Iteration 93: 1.475216208746133e-06\n",
      "Iteration 94: 1.475216208746133e-06\n",
      "Iteration 95: 1.475216208746133e-06\n",
      "Iteration 96: 1.4454137726715999e-06\n",
      "Iteration 97: 1.3858090142093715e-06\n",
      "Iteration 98: 1.3858090142093715e-06\n",
      "Iteration 99: 1.3709078530155239e-06\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.0005070384358987212\n",
      "Iteration 1: 0.0004226093296892941\n",
      "Iteration 2: 0.00035011349245905876\n",
      "Iteration 3: 0.0002885941066779196\n",
      "Iteration 4: 0.0002370504371356219\n",
      "Iteration 5: 0.00019430331303738058\n",
      "Iteration 6: 0.00015917410200927407\n",
      "Iteration 7: 0.00013051435234956443\n",
      "Iteration 8: 0.0001072355080395937\n",
      "Iteration 9: 8.839827933115885e-05\n",
      "Iteration 10: 7.324236503336579e-05\n",
      "Iteration 11: 6.0992642829660326e-05\n",
      "Iteration 12: 5.106781463837251e-05\n",
      "Iteration 13: 4.3065447243861854e-05\n",
      "Iteration 14: 3.6598037695512176e-05\n",
      "Iteration 15: 3.130791810690425e-05\n",
      "Iteration 16: 2.6941725081996992e-05\n",
      "Iteration 17: 2.3410046196659096e-05\n",
      "Iteration 18: 2.048934402409941e-05\n",
      "Iteration 19: 1.8030596038443036e-05\n",
      "Iteration 20: 1.6003998098312877e-05\n",
      "Iteration 21: 1.4275432477006689e-05\n",
      "Iteration 22: 1.2785292710759677e-05\n",
      "Iteration 23: 1.1593181625357829e-05\n",
      "Iteration 24: 1.0550087608862668e-05\n",
      "Iteration 25: 9.641104952606838e-06\n",
      "Iteration 26: 8.851335223880596e-06\n",
      "Iteration 27: 8.18077660369454e-06\n",
      "Iteration 28: 7.59962586016627e-06\n",
      "Iteration 29: 7.107882993295789e-06\n",
      "Iteration 30: 6.660844519501552e-06\n",
      "Iteration 31: 6.243609277589712e-06\n",
      "Iteration 32: 5.900879841647111e-06\n",
      "Iteration 33: 5.573052021645708e-06\n",
      "Iteration 34: 5.3048297559143975e-06\n",
      "Iteration 35: 5.08131097376463e-06\n",
      "Iteration 36: 4.842891030421015e-06\n",
      "Iteration 37: 4.634273409465095e-06\n",
      "Iteration 38: 4.440557404450374e-06\n",
      "Iteration 39: 4.291544883017195e-06\n",
      "Iteration 40: 4.097828878002474e-06\n",
      "Iteration 41: 3.978619133704342e-06\n",
      "Iteration 42: 3.859408934658859e-06\n",
      "Iteration 43: 3.7551008063019253e-06\n",
      "Iteration 44: 3.635891062003793e-06\n",
      "Iteration 45: 3.5315824788995087e-06\n",
      "Iteration 46: 3.4272738957952242e-06\n",
      "Iteration 47: 3.382570184840006e-06\n",
      "Iteration 48: 3.2931629903032444e-06\n",
      "Iteration 49: 3.2037555683928076e-06\n",
      "Iteration 50: 3.1739532460051123e-06\n",
      "Iteration 51: 3.0845458240946755e-06\n",
      "Iteration 52: 2.995138629557914e-06\n",
      "Iteration 53: 2.9653360797965433e-06\n",
      "Iteration 54: 2.890830273827305e-06\n",
      "Iteration 55: 2.8461265628720867e-06\n",
      "Iteration 56: 2.771620529529173e-06\n",
      "Iteration 57: 2.75671914096165e-06\n",
      "Iteration 58: 2.741817979767802e-06\n",
      "Iteration 59: 2.652410785231041e-06\n",
      "Iteration 60: 2.637509624037193e-06\n",
      "Iteration 61: 2.563003363320604e-06\n",
      "Iteration 62: 2.5481022021267563e-06\n",
      "Iteration 63: 2.518299879739061e-06\n",
      "Iteration 64: 2.4437938463961473e-06\n",
      "Iteration 65: 2.4288926852022996e-06\n",
      "Iteration 66: 2.4288926852022996e-06\n",
      "Iteration 67: 2.413991524008452e-06\n",
      "Iteration 68: 2.3245843294716906e-06\n",
      "Iteration 69: 2.3096829409041675e-06\n",
      "Iteration 70: 2.3096829409041675e-06\n",
      "Iteration 71: 2.29478177971032e-06\n",
      "Iteration 72: 2.220275746367406e-06\n",
      "Iteration 73: 2.220275746367406e-06\n",
      "Iteration 74: 2.2053745851735584e-06\n",
      "Iteration 75: 2.1904734239797108e-06\n",
      "Iteration 76: 2.1159676180104725e-06\n",
      "Iteration 77: 2.1010662294429494e-06\n",
      "Iteration 78: 2.1010662294429494e-06\n",
      "Iteration 79: 2.0861648408754263e-06\n",
      "Iteration 80: 2.0265601961000357e-06\n",
      "Iteration 81: 2.011659034906188e-06\n",
      "Iteration 82: 1.9967578737123404e-06\n",
      "Iteration 83: 1.9818567125184927e-06\n",
      "Iteration 84: 1.9818567125184927e-06\n",
      "Iteration 85: 1.966955551324645e-06\n",
      "Iteration 86: 1.9073504518019035e-06\n",
      "Iteration 87: 1.8924494042948936e-06\n",
      "Iteration 88: 1.8924494042948936e-06\n",
      "Iteration 89: 1.877548243101046e-06\n",
      "Iteration 90: 1.877548243101046e-06\n",
      "Iteration 91: 1.7881411622511223e-06\n",
      "Iteration 92: 1.7881411622511223e-06\n",
      "Iteration 93: 1.7881411622511223e-06\n",
      "Iteration 94: 1.7732397736835992e-06\n",
      "Iteration 95: 1.7732397736835992e-06\n",
      "Iteration 96: 1.7583386124897515e-06\n",
      "Iteration 97: 1.698733967714361e-06\n",
      "Iteration 98: 1.698733967714361e-06\n",
      "Iteration 99: 1.6838328065205133e-06\n",
      "------- Checking outputs (left) vs ground truth (right): -----\n",
      "tensor([[1.9073e-06, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.9073e-06, 0.0000e+00]])\n",
      "--------------- LNN Parameters (post-training) ---------------\n",
      "OR (beta, argument weights): 9.477 [19.059 17.79 ]\n",
      "AND1 (beta, argument weights): 7.542 [14.488 15.328]\n",
      "AND2 (beta, argument weights): 7.761 [14.913 15.211]\n"
     ]
    }
   ],
   "source": [
    "#this is a hyperparameter\n",
    "alpha = 0.8\n",
    "\n",
    "op_and1 = and_lukasiewicz(alpha, 2, False)\n",
    "op_and2 = and_lukasiewicz(alpha, 2, False)\n",
    "op_or = or_lukasiewicz(alpha, 2, False)\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam([{'params': op_or.parameters()}, \\\n",
    "                        {'params': op_and1.parameters()}, \\\n",
    "                        {'params': op_and2.parameters()}], lr=0.1)\n",
    "\n",
    "for iter in range(100):\n",
    "    op_or.train()\n",
    "    op_and1.train()\n",
    "    op_and2.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = x[:,0].view(-1,1)\n",
    "    x1 = x[:,1].view(-1,1)\n",
    "    yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#check to see output of xor post-training\n",
    "x0 = x[:,0].view(-1,1)\n",
    "x1 = x[:,1].view(-1,1)\n",
    "yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                        op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "check_values = torch.cat((yhat, y), 1)\n",
    "print(\"------- Checking outputs (left) vs ground truth (right): -----\")\n",
    "print(check_values.detach())\n",
    "\n",
    "#LNN parameters: post-training (we have 3 sets of beta, argument weights)\n",
    "print(\"--------------- LNN Parameters (post-training) ---------------\")\n",
    "beta_or, argument_wts_or = op_or.AND.cdd()\n",
    "beta_and1, argument_wts_and1 = op_and1.cdd()\n",
    "beta_and2, argument_wts_and2 = op_and2.cdd()\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"OR (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_or.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_or.detach().numpy()))\n",
    "print(\"AND1 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and1.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and1.detach().numpy()))\n",
    "print(\"AND2 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and2.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
