{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/meta_rule/')\n",
    "\n",
    "from lnn_operators import and_lukasiewicz, or_lukasiewicz, negation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "# df = df.loc[22:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df.Features.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5158, 0.0000, 0.1579, 0.4390, 1.0000, 1.0000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4980, 0.8569, 0.4000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4292, 0.8430, 0.5000],\n",
      "        ...,\n",
      "        [0.5225, 0.2308, 0.2564, 0.7010, 0.2003, 0.7100],\n",
      "        [0.5743, 0.5000, 0.4082, 0.8074, 0.2003, 0.8200],\n",
      "        [0.9379, 0.6000, 0.6897, 0.7939, 0.1000, 0.8200]])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "['GMT;Greenwich Mean Time' 'GMT;UTC+02:00' 'GMT;UTC+08:00' ...\n",
      " 'Houses Of Parliament;List of people who have spoken to both Houses of the United Kingdom Parliament'\n",
      " 'Houses Of Parliament;Statue of Margaret Thatcher, Houses of Parliament'\n",
      " 'Houses Of Parliament;Houses of Parliament Act 1837']\n"
     ]
    }
   ],
   "source": [
    "#to train a xor we need its truth table\n",
    "X = torch.from_numpy(features).float()\n",
    "print(X)\n",
    "#the target values for each row in the truth table (xor)\n",
    "Y = torch.from_numpy(df.Label.values).float()\n",
    "print(Y)\n",
    "# mention_labels (cannot convert string explicitly)\n",
    "mention_labels = df.Mention_label.values\n",
    "print(mention_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26755,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, m_labels_train, m_labels_test = train_test_split(X, Y, mention_labels, test_size=0.2,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([21404, 6])\n",
      "val torch.Size([5351, 6])\n"
     ]
    }
   ],
   "source": [
    "print('train', x_train.shape)\n",
    "print('val', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureNameLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(PureNameLNN, self).__init__()\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.sim_disjunction_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(4)])\n",
    "        self.predicate_and = and_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \"\"\"\n",
    "            x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "               normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "        \"\"\"\n",
    "        yhat = None\n",
    "        \n",
    "        ####### RULE 1: lookup predicate #######\n",
    "        lookup_features = x[:,4].view(-1, 1)\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "        \n",
    "        ####### RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb)) #######\n",
    "        feature_list = []\n",
    "        # rule 2 (1) mention==label\n",
    "        mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "        labels = np.array([m[1].lower() for m in mention_labels])\n",
    "        exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float().view(-1,1)\n",
    "        feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "        # rule 2 (2) Jacc(mention, label)\n",
    "        jacc_features = x[:, 1].view(-1,1)\n",
    "#         jacc_features = torch.clamp(jacc_features, min=self.threshold, max=1.0)\n",
    "        jacc_features_ = torch.where(jacc_features>=self.threshold, jacc_features, torch.zeros_like(jacc_features))\n",
    "        feature_list.append(jacc_features_)\n",
    "#         print(\"jacc_features\", jacc_features)\n",
    "#         print(\"jacc_features*mask\", jacc_features_)\n",
    "        \n",
    "        # rule 2 (3) Lev(mention, label)\n",
    "        lev_features = x[:, 2].view(-1,1)\n",
    "        lev_features_ = torch.where(lev_features>=self.threshold, lev_features, torch.zeros_like(lev_features))\n",
    "        feature_list.append(lev_features_)\n",
    "#         print(\"lev_features\", lev_features)\n",
    "#         print(\"lev_features*mask\", lev_features_)\n",
    "        \n",
    "        # rule 2 (4) Jaro(mention, label)\n",
    "        jaro_features = x[:, 0].view(-1,1)\n",
    "        jaro_features_ = torch.where(jaro_features>=self.threshold, jaro_features, torch.zeros_like(jaro_features))\n",
    "        feature_list.append(jaro_features_)\n",
    "#         print(\"jaro_features\", jaro_features)\n",
    "#         print(\"jaro_features*mask\", jaro_features_)\n",
    "        \n",
    "        # disjunction of (1) to (4)\n",
    "        disjunction_result = feature_list[0]\n",
    "        for i in range(0, 3):\n",
    "            disjunction_result = self.sim_disjunction_or_ops[i](torch.cat((disjunction_result, feature_list[i+1]), 1))\n",
    "#             print(\"disjunction_result\", disjunction_result)\n",
    "        \n",
    "        # RULE 1 + RULE 2\n",
    "        yhat = self.predicate_and(torch.cat((lookup_features, disjunction_result), 1))\n",
    "#         print('yhat', yhat)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(ContextLNN, self).__init__()\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.sim_disjunction_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(4)])\n",
    "        self.predicate_and_ops = nn.ModuleList([and_lukasiewicz(alpha, arity, slack) for i in range(2)])\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \"\"\"\n",
    "            x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "               normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "        \"\"\"\n",
    "        yhat = None\n",
    "        ####### RULE 1: lookup predicate #######\n",
    "        lookup_features = x[:,4].view(-1, 1)\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "        ####### RULE 3: contains predicate #######\n",
    "        context_features = x[:,5].view(-1, 1)\n",
    "#         print(\"context_features\", context_features)\n",
    "        # context mask\n",
    "        context_mask = context_features >= 0.25\n",
    "#         print(\"context_mask\", context_mask)\n",
    "        \n",
    "        ####### RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb)) #######\n",
    "        # check: https://github.ibm.com/IBM-Research-AI/enhanced_amr/blob/5501e3af41794353ed9bb147320666622474171a/entity_linking.py#L295\n",
    "        feature_list = []\n",
    "        # rule 2 (1) mention==label\n",
    "        mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "        labels = np.array([m[1].lower() for m in mention_labels])\n",
    "        exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float().view(-1,1)\n",
    "        feature_list.append(exact_match_features)\n",
    "        \n",
    "        # rule 2 (2) Jacc(mention, label)\n",
    "        jacc_features = x[:, 1].view(-1,1)\n",
    "        jacc_features_ = torch.where(jacc_features>=self.threshold-0.3, jacc_features, torch.zeros_like(jacc_features))\n",
    "        feature_list.append(jacc_features_*context_mask)\n",
    "        \n",
    "        # rule 2 (3) Lev(mention, label)\n",
    "        lev_features = x[:, 2].view(-1,1)\n",
    "        lev_features_ = torch.where(lev_features>=self.threshold-0.3, lev_features, torch.zeros_like(lev_features))\n",
    "        feature_list.append(lev_features_*context_mask)\n",
    "#         print(\"lev_features\", lev_features)\n",
    "#         print(\"lev_features*mask\", lev_features_)\n",
    "        \n",
    "        # rule 2 (4) Jaro(mention, label)\n",
    "        jaro_features = x[:, 0].view(-1,1)\n",
    "        jaro_features_ = torch.where(jaro_features>=self.threshold-0.3, jaro_features, torch.zeros_like(jaro_features))\n",
    "        feature_list.append(jaro_features_*context_mask)\n",
    "        \n",
    "        # disjunction of (1) to (4)\n",
    "        disjunction_result = feature_list[0]\n",
    "        for i in range(0, 3):\n",
    "            disjunction_result = self.sim_disjunction_or_ops[i](torch.cat((disjunction_result, feature_list[i+1]), 1))\n",
    "        \n",
    "        # RULE 1 + RULE 2\n",
    "        r1_r2_res = self.predicate_and_ops[0](torch.cat((lookup_features, disjunction_result), 1))\n",
    "        yhat = self.predicate_and_ops[1](torch.cat((r1_r2_res, context_features), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexRuleLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(ComplexRuleLNN, self).__init__()\n",
    "        \n",
    "        self.pureNameRule = PureNameLNN(alpha, arity, None)\n",
    "        self.contextRule = ContextLNN(alpha, arity, None)\n",
    "        self.rule_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(2)])\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \n",
    "        yhat = None\n",
    "        \n",
    "        pure_res = self.pureNameRule(x, mention_labels)\n",
    "        context_res = self.contextRule(x, mention_labels)\n",
    "#         print('context_res', context_res)\n",
    "        pure_context_res = self.rule_or_ops[0](torch.cat((pure_res, context_res), 1))\n",
    "#         print('pure_context_res', pure_context_res)\n",
    "        return pure_context_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1512e-04],\n",
      "        [5.4890e-04],\n",
      "        [1.0000e+00],\n",
      "        ...,\n",
      "        [5.7191e-04],\n",
      "        [5.5116e-04],\n",
      "        [5.5510e-04]], grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = ComplexRuleLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_test, y_test, m_labels_test):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = eval_model(x_test, m_labels_test)\n",
    "        loss = loss_fn(test_pred, y_test)\n",
    "        test_pred_ = test_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, test_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([21404])) that is different to the input size (torch.Size([21404, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([5351])) that is different to the input size (torch.Size([5351, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.8213149905204773\n",
      "val loss tensor(0.7455)\n",
      "f1 w/ 0.5 threshold 0.5440865475833907\n",
      "Iteration 1: 0.685476541519165\n",
      "val loss tensor(0.6155)\n",
      "f1 w/ 0.5 threshold 0.5556839636446299\n",
      "Iteration 2: 0.5657318830490112\n",
      "val loss tensor(0.5243)\n",
      "f1 w/ 0.5 threshold 0.5657118837883148\n",
      "Iteration 3: 0.46959587931632996\n",
      "val loss tensor(0.4244)\n",
      "f1 w/ 0.5 threshold 0.5778845822202021\n",
      "Iteration 4: 0.3860364258289337\n",
      "val loss tensor(0.3476)\n",
      "f1 w/ 0.5 threshold 0.5946052934809039\n",
      "Iteration 5: 0.31820210814476013\n",
      "val loss tensor(0.2908)\n",
      "f1 w/ 0.5 threshold 0.609600847209932\n",
      "Iteration 6: 0.2678575813770294\n",
      "val loss tensor(0.2437)\n",
      "f1 w/ 0.5 threshold 0.6249198910518305\n",
      "Iteration 7: 0.23067352175712585\n",
      "val loss tensor(0.2112)\n",
      "f1 w/ 0.5 threshold 0.6360602749900679\n",
      "Iteration 8: 0.2038542777299881\n",
      "val loss tensor(0.1792)\n",
      "f1 w/ 0.5 threshold 0.6510327736672359\n",
      "Iteration 9: 0.18011809885501862\n",
      "val loss tensor(0.1584)\n",
      "f1 w/ 0.5 threshold 0.6666856997513587\n",
      "Iteration 10: 0.1631675660610199\n",
      "val loss tensor(0.1444)\n",
      "f1 w/ 0.5 threshold 0.6769943313084086\n",
      "Iteration 11: 0.14858391880989075\n",
      "val loss tensor(0.1296)\n",
      "f1 w/ 0.5 threshold 0.6870084582550128\n",
      "Iteration 12: 0.13620465993881226\n",
      "val loss tensor(0.1161)\n",
      "f1 w/ 0.5 threshold 0.7015153862840496\n",
      "Iteration 13: 0.12869060039520264\n",
      "val loss tensor(0.1096)\n",
      "f1 w/ 0.5 threshold 0.7123999166107481\n",
      "Iteration 14: 0.12249219417572021\n",
      "val loss tensor(0.1058)\n",
      "f1 w/ 0.5 threshold 0.7123999166107481\n",
      "Iteration 15: 0.117927186191082\n",
      "val loss tensor(0.0981)\n",
      "f1 w/ 0.5 threshold 0.7244404009887393\n",
      "Iteration 16: 0.11162015050649643\n",
      "val loss tensor(0.0958)\n",
      "f1 w/ 0.5 threshold 0.7244404009887393\n",
      "Iteration 17: 0.107573963701725\n",
      "val loss tensor(0.0922)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 18: 0.10356510430574417\n",
      "val loss tensor(0.0897)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 19: 0.10048194974660873\n",
      "val loss tensor(0.0876)\n",
      "f1 w/ 0.5 threshold 0.7332065499975071\n",
      "Iteration 20: 0.09699290245771408\n",
      "val loss tensor(0.0853)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 21: 0.09364348649978638\n",
      "val loss tensor(0.0834)\n",
      "f1 w/ 0.5 threshold 0.737836964884521\n",
      "Iteration 22: 0.09048117697238922\n",
      "val loss tensor(0.0818)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 23: 0.08727004379034042\n",
      "val loss tensor(0.0810)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 24: 0.08594198524951935\n",
      "val loss tensor(0.0796)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 25: 0.08413245528936386\n",
      "val loss tensor(0.0790)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 26: 0.082894466817379\n",
      "val loss tensor(0.0784)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 27: 0.08179882168769836\n",
      "val loss tensor(0.0779)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 28: 0.08047394454479218\n",
      "val loss tensor(0.0771)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 29: 0.07945118844509125\n",
      "val loss tensor(0.0762)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 30: 0.07851437479257584\n",
      "val loss tensor(0.0755)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 31: 0.07738247513771057\n",
      "val loss tensor(0.0750)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 32: 0.07655822485685349\n",
      "val loss tensor(0.0745)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 33: 0.07536381483078003\n",
      "val loss tensor(0.0742)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 34: 0.07416446506977081\n",
      "val loss tensor(0.0738)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 35: 0.07324668020009995\n",
      "val loss tensor(0.0735)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 36: 0.07247378677129745\n",
      "val loss tensor(0.0730)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 37: 0.07186485081911087\n",
      "val loss tensor(0.0723)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 38: 0.0713769793510437\n",
      "val loss tensor(0.0718)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 39: 0.0709589496254921\n",
      "val loss tensor(0.0708)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 40: 0.06945908069610596\n",
      "val loss tensor(0.0702)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 41: 0.06878668069839478\n",
      "val loss tensor(0.0694)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 42: 0.06849400699138641\n",
      "val loss tensor(0.0690)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 43: 0.06811244785785675\n",
      "val loss tensor(0.0687)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 44: 0.0676860585808754\n",
      "val loss tensor(0.0684)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 45: 0.06742402166128159\n",
      "val loss tensor(0.0682)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 46: 0.06720220297574997\n",
      "val loss tensor(0.0680)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 47: 0.06698725372552872\n",
      "val loss tensor(0.0678)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 48: 0.066624715924263\n",
      "val loss tensor(0.0676)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 49: 0.06608932465314865\n",
      "val loss tensor(0.0674)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 50: 0.06580144166946411\n",
      "val loss tensor(0.0672)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 51: 0.06558693945407867\n",
      "val loss tensor(0.0668)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 52: 0.06533953547477722\n",
      "val loss tensor(0.0664)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 53: 0.06505543738603592\n",
      "val loss tensor(0.0661)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 54: 0.06484439224004745\n",
      "val loss tensor(0.0659)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 55: 0.06468215584754944\n",
      "val loss tensor(0.0657)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 56: 0.06452830880880356\n",
      "val loss tensor(0.0656)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 57: 0.06434828042984009\n",
      "val loss tensor(0.0655)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 58: 0.06411851197481155\n",
      "val loss tensor(0.0654)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 59: 0.06395693123340607\n",
      "val loss tensor(0.0653)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 60: 0.06381186097860336\n",
      "val loss tensor(0.0651)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 61: 0.06368383020162582\n",
      "val loss tensor(0.0648)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 62: 0.06348797678947449\n",
      "val loss tensor(0.0646)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 63: 0.06331570446491241\n",
      "val loss tensor(0.0644)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 64: 0.06317631155252457\n",
      "val loss tensor(0.0643)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 65: 0.06286843121051788\n",
      "val loss tensor(0.0642)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 66: 0.06268518418073654\n",
      "val loss tensor(0.0640)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 67: 0.06252818554639816\n",
      "val loss tensor(0.0639)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 68: 0.062303122133016586\n",
      "val loss tensor(0.0637)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 69: 0.06215929239988327\n",
      "val loss tensor(0.0636)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 70: 0.06202821061015129\n",
      "val loss tensor(0.0634)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 71: 0.06180184707045555\n",
      "val loss tensor(0.0633)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 72: 0.06160592660307884\n",
      "val loss tensor(0.0632)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 73: 0.061507001519203186\n",
      "val loss tensor(0.0631)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 74: 0.06141318380832672\n",
      "val loss tensor(0.0630)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 75: 0.06132398545742035\n",
      "val loss tensor(0.0629)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 76: 0.061239130795001984\n",
      "val loss tensor(0.0627)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 77: 0.061157990247011185\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 78: 0.061080414801836014\n",
      "val loss tensor(0.0621)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 79: 0.06100479140877724\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 80: 0.060882601886987686\n",
      "val loss tensor(0.0619)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 81: 0.06076093018054962\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 82: 0.060659103095531464\n",
      "val loss tensor(0.0616)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 83: 0.0604790523648262\n",
      "val loss tensor(0.0615)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 84: 0.06036108732223511\n",
      "val loss tensor(0.0613)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 85: 0.060254815965890884\n",
      "val loss tensor(0.0609)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 86: 0.060157861560583115\n",
      "val loss tensor(0.0606)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 87: 0.060068558901548386\n",
      "val loss tensor(0.0604)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 88: 0.05998576432466507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0602)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 89: 0.059910848736763\n",
      "val loss tensor(0.0600)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 90: 0.05983874201774597\n",
      "val loss tensor(0.0599)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 91: 0.05976751074194908\n",
      "val loss tensor(0.0598)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 92: 0.05958736315369606\n",
      "val loss tensor(0.0596)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 93: 0.05942615494132042\n",
      "val loss tensor(0.0595)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 94: 0.05931003764271736\n",
      "val loss tensor(0.0593)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 95: 0.05916788801550865\n",
      "val loss tensor(0.0587)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 96: 0.05900969356298447\n",
      "val loss tensor(0.0584)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 97: 0.05888979509472847\n",
      "val loss tensor(0.0582)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 98: 0.058782000094652176\n",
      "val loss tensor(0.0581)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 99: 0.05868467688560486\n",
      "val loss tensor(0.0580)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 100: 0.05859033390879631\n",
      "val loss tensor(0.0580)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 101: 0.0583808608353138\n",
      "val loss tensor(0.0579)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 102: 0.0582503005862236\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 103: 0.058186426758766174\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 104: 0.05812559649348259\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 105: 0.05806612968444824\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 106: 0.058009326457977295\n",
      "val loss tensor(0.0569)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 107: 0.05795358866453171\n",
      "val loss tensor(0.0569)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 108: 0.057899948209524155\n",
      "val loss tensor(0.0568)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 109: 0.05784844979643822\n",
      "val loss tensor(0.0567)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 110: 0.05779736116528511\n",
      "val loss tensor(0.0566)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 111: 0.057748422026634216\n",
      "val loss tensor(0.0565)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 112: 0.057701874524354935\n",
      "val loss tensor(0.0565)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 113: 0.05765504390001297\n",
      "val loss tensor(0.0564)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 114: 0.05760927125811577\n",
      "val loss tensor(0.0564)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 115: 0.05756504461169243\n",
      "val loss tensor(0.0563)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 116: 0.05752038210630417\n",
      "val loss tensor(0.0563)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 117: 0.05746818706393242\n",
      "val loss tensor(0.0562)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 118: 0.05741477757692337\n",
      "val loss tensor(0.0562)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 119: 0.057360392063856125\n",
      "val loss tensor(0.0562)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 120: 0.05730554834008217\n",
      "val loss tensor(0.0561)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 121: 0.0571160726249218\n",
      "val loss tensor(0.0561)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 122: 0.05701961740851402\n",
      "val loss tensor(0.0560)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 123: 0.05697941407561302\n",
      "val loss tensor(0.0560)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 124: 0.056939709931612015\n",
      "val loss tensor(0.0559)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 125: 0.05690058693289757\n",
      "val loss tensor(0.0558)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 126: 0.056852757930755615\n",
      "val loss tensor(0.0557)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 127: 0.056797102093696594\n",
      "val loss tensor(0.0550)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 128: 0.05663139745593071\n",
      "val loss tensor(0.0548)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 129: 0.056497201323509216\n",
      "val loss tensor(0.0545)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 130: 0.05645672604441643\n",
      "val loss tensor(0.0543)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 131: 0.056375496089458466\n",
      "val loss tensor(0.0541)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 132: 0.05604255199432373\n",
      "val loss tensor(0.0536)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 133: 0.0559108592569828\n",
      "val loss tensor(0.0534)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 134: 0.05582074448466301\n",
      "val loss tensor(0.0533)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 135: 0.05574752390384674\n",
      "val loss tensor(0.0532)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 136: 0.055709704756736755\n",
      "val loss tensor(0.0531)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 137: 0.05567392334342003\n",
      "val loss tensor(0.0531)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 138: 0.05562952905893326\n",
      "val loss tensor(0.0531)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 139: 0.05557471513748169\n",
      "val loss tensor(0.0531)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 140: 0.055497583001852036\n",
      "val loss tensor(0.0530)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 141: 0.05524315685033798\n",
      "val loss tensor(0.0530)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 142: 0.055130280554294586\n",
      "val loss tensor(0.0530)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 143: 0.05504468455910683\n",
      "val loss tensor(0.0529)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 144: 0.05497034639120102\n",
      "val loss tensor(0.0529)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 145: 0.05489329248666763\n",
      "val loss tensor(0.0529)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 146: 0.05477061867713928\n",
      "val loss tensor(0.0528)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 147: 0.05468437448143959\n",
      "val loss tensor(0.0528)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 148: 0.05456624552607536\n",
      "val loss tensor(0.0528)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 149: 0.0544775053858757\n",
      "val loss tensor(0.0528)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 150: 0.054411813616752625\n",
      "val loss tensor(0.0527)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 151: 0.054313383996486664\n",
      "val loss tensor(0.0527)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 152: 0.05420761927962303\n",
      "val loss tensor(0.0527)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 153: 0.054136574268341064\n",
      "val loss tensor(0.0526)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 154: 0.054079245775938034\n",
      "val loss tensor(0.0526)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 155: 0.05403321236371994\n",
      "val loss tensor(0.0526)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 156: 0.0539897195994854\n",
      "val loss tensor(0.0526)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 157: 0.05394856631755829\n",
      "val loss tensor(0.0525)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 158: 0.053908392786979675\n",
      "val loss tensor(0.0525)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 159: 0.05387049540877342\n",
      "val loss tensor(0.0525)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 160: 0.053834155201911926\n",
      "val loss tensor(0.0524)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 161: 0.05379940941929817\n",
      "val loss tensor(0.0524)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 162: 0.05376587063074112\n",
      "val loss tensor(0.0524)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 163: 0.05373330041766167\n",
      "val loss tensor(0.0524)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 164: 0.053702108561992645\n",
      "val loss tensor(0.0523)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 165: 0.053671590983867645\n",
      "val loss tensor(0.0523)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 166: 0.053641919046640396\n",
      "val loss tensor(0.0523)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 167: 0.05361305922269821\n",
      "val loss tensor(0.0523)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 168: 0.0535849928855896\n",
      "val loss tensor(0.0522)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 169: 0.05355237424373627\n",
      "val loss tensor(0.0522)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 170: 0.053517915308475494\n",
      "val loss tensor(0.0522)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 171: 0.05348299443721771\n",
      "val loss tensor(0.0522)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 172: 0.05338209494948387\n",
      "val loss tensor(0.0521)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 173: 0.053299225866794586\n",
      "val loss tensor(0.0521)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 174: 0.05315232649445534\n",
      "val loss tensor(0.0521)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 175: 0.053007081151008606\n",
      "val loss tensor(0.0520)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 176: 0.05297160521149635\n",
      "val loss tensor(0.0519)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 177: 0.05293777212500572\n",
      "val loss tensor(0.0503)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 178: 0.052905041724443436\n",
      "val loss tensor(0.0502)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 179: 0.05287386104464531\n",
      "val loss tensor(0.0502)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 180: 0.05284719914197922\n",
      "val loss tensor(0.0501)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 181: 0.052821360528469086\n",
      "val loss tensor(0.0501)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 182: 0.052796345204114914\n",
      "val loss tensor(0.0501)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 183: 0.052772242575883865\n",
      "val loss tensor(0.0501)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 184: 0.05274839326739311\n",
      "val loss tensor(0.0500)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 185: 0.052725739777088165\n",
      "val loss tensor(0.0500)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 186: 0.05270213633775711\n",
      "val loss tensor(0.0500)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 187: 0.05267903581261635\n",
      "val loss tensor(0.0500)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 188: 0.0526563860476017\n",
      "val loss tensor(0.0500)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 189: 0.0526343435049057\n",
      "val loss tensor(0.0499)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 190: 0.05261210352182388\n",
      "val loss tensor(0.0499)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 191: 0.05259016901254654\n",
      "val loss tensor(0.0499)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 192: 0.052568480372428894\n",
      "val loss tensor(0.0499)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 193: 0.05254717543721199\n",
      "val loss tensor(0.0499)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 194: 0.052526071667671204\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 195: 0.05250514671206474\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 196: 0.05243534594774246\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 197: 0.052379682660102844\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 198: 0.05233565717935562\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 199: 0.0522974394261837\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n"
     ]
    }
   ],
   "source": [
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 10000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, test_pred = evaluate(model, x_test, y_test, m_labels_test)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = test_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive 0.5 threshold best f1: 0.7823008205286328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tuning, the best f1 is 0.7996510006501191 w/ threshold 0.9950995099509952\n"
     ]
    }
   ],
   "source": [
    "# y_test and test_pred\n",
    "\n",
    "print(\"Naive 0.5 threshold best f1:\", best_val_f1)\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=10000):\n",
    "    y_test_preds = test_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Mention_label</th>\n",
       "      <th>Features</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Düsseldorf Airport;Düsseldorf International Ai...</td>\n",
       "      <td>[0.8902777433395386, 0.6666666666666666, 0.562...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Düsseldorf Airport;Düsseldorf Airport</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.7528859168415751, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Düsseldorf Airport;Düsseldorf Airport station</td>\n",
       "      <td>[0.9384615421295166, 0.6666666666666666, 0.692...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Düsseldorf Airport;Düsseldorf Airport Terminal...</td>\n",
       "      <td>[0.9028571248054504, 0.5, 0.5142857142857142, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Düsseldorf Airport;Düsseldorf-Unterrath–Düssel...</td>\n",
       "      <td>[0.8198412656784058, 0.2, 0.3214285714285714, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23841</th>\n",
       "      <td>23841</td>\n",
       "      <td>Pluto;Mother Pluto</td>\n",
       "      <td>[0.35555556416511536, 0.5, 0.41666666666666663...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23842</th>\n",
       "      <td>23842</td>\n",
       "      <td>Pluto;Paratoxodera pluto</td>\n",
       "      <td>[0.6299999952316284, 0.5, 0.2777777777777778, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23843</th>\n",
       "      <td>23843</td>\n",
       "      <td>Pluto;HMS Pluto</td>\n",
       "      <td>[0.0, 0.5, 0.5555555555555556, 0.7309595683356...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23844</th>\n",
       "      <td>23844</td>\n",
       "      <td>Pluto;Terry Pluto 1</td>\n",
       "      <td>[0.4256410300731659, 0.3333333333333333, 0.384...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23845</th>\n",
       "      <td>23845</td>\n",
       "      <td>Pluto;The Pluto Files</td>\n",
       "      <td>[0.644444465637207, 0.3333333333333333, 0.3333...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23846 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                      Mention_label  \\\n",
       "0               0  Düsseldorf Airport;Düsseldorf International Ai...   \n",
       "1               1              Düsseldorf Airport;Düsseldorf Airport   \n",
       "2               2      Düsseldorf Airport;Düsseldorf Airport station   \n",
       "3               3  Düsseldorf Airport;Düsseldorf Airport Terminal...   \n",
       "4               4  Düsseldorf Airport;Düsseldorf-Unterrath–Düssel...   \n",
       "...           ...                                                ...   \n",
       "23841       23841                                 Pluto;Mother Pluto   \n",
       "23842       23842                           Pluto;Paratoxodera pluto   \n",
       "23843       23843                                    Pluto;HMS Pluto   \n",
       "23844       23844                                Pluto;Terry Pluto 1   \n",
       "23845       23845                              Pluto;The Pluto Files   \n",
       "\n",
       "                                                Features  Label  \n",
       "0      [0.8902777433395386, 0.6666666666666666, 0.562...      0  \n",
       "1            [1.0, 1.0, 1.0, 1.0, 0.7528859168415751, 0]      1  \n",
       "2      [0.9384615421295166, 0.6666666666666666, 0.692...      0  \n",
       "3      [0.9028571248054504, 0.5, 0.5142857142857142, ...      0  \n",
       "4      [0.8198412656784058, 0.2, 0.3214285714285714, ...      0  \n",
       "...                                                  ...    ...  \n",
       "23841  [0.35555556416511536, 0.5, 0.41666666666666663...      0  \n",
       "23842  [0.6299999952316284, 0.5, 0.2777777777777778, ...      0  \n",
       "23843  [0.0, 0.5, 0.5555555555555556, 0.7309595683356...      0  \n",
       "23844  [0.4256410300731659, 0.3333333333333333, 0.384...      0  \n",
       "23845  [0.644444465637207, 0.3333333333333333, 0.3333...      0  \n",
       "\n",
       "[23846 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for XOR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xorLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack):\n",
    "        super(xorLNN, self).__init__()\n",
    "        self.op_and1 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_and2 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_or = or_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = x[:,0].view(-1,1)\n",
    "        print(x0)\n",
    "        x1 = x[:,1].view(-1,1)\n",
    "        print(x1)\n",
    "        print(torch.cat((x0, negation(x1)), 1))\n",
    "        yhat = self.op_or(torch.cat((self.op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            self.op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.6349e-04],\n",
       "        [9.9932e-01],\n",
       "        [9.9967e-01],\n",
       "        [4.6349e-04]], grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xorLNN(0.8, 2, False)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.00041395798325538635\n",
      "Iteration 1: 0.0003413597878534347\n",
      "Iteration 2: 0.00027990160742774606\n",
      "Iteration 3: 0.00022847841319162399\n",
      "Iteration 4: 0.00018598556926008314\n",
      "Iteration 5: 0.00015122962940949947\n",
      "Iteration 6: 0.0001230025663971901\n",
      "Iteration 7: 0.00010024568473454565\n",
      "Iteration 8: 8.197502756956965e-05\n",
      "Iteration 9: 6.734087219228968e-05\n",
      "Iteration 10: 5.5627755500609055e-05\n",
      "Iteration 11: 4.625439760275185e-05\n",
      "Iteration 12: 3.872895103995688e-05\n",
      "Iteration 13: 3.267884312663227e-05\n",
      "Iteration 14: 2.7761290766648017e-05\n",
      "Iteration 15: 2.3767666789353825e-05\n",
      "Iteration 16: 2.051913361356128e-05\n",
      "Iteration 17: 1.7821967048803344e-05\n",
      "Iteration 18: 1.5646355677745305e-05\n",
      "Iteration 19: 1.3783679605694488e-05\n",
      "Iteration 20: 1.2263739336049184e-05\n",
      "Iteration 21: 1.0967321941279806e-05\n",
      "Iteration 22: 9.89442560239695e-06\n",
      "Iteration 23: 8.970543603936676e-06\n",
      "Iteration 24: 8.165872714016587e-06\n",
      "Iteration 25: 7.480413842131384e-06\n",
      "Iteration 26: 6.899264008097816e-06\n",
      "Iteration 27: 6.407521595974686e-06\n",
      "Iteration 28: 5.975385647616349e-06\n",
      "Iteration 29: 5.587952728092205e-06\n",
      "Iteration 30: 5.260125362838153e-06\n",
      "Iteration 31: 4.932297997584101e-06\n",
      "Iteration 32: 4.678976893046638e-06\n",
      "Iteration 33: 4.395853011374129e-06\n",
      "Iteration 34: 4.231939783494454e-06\n",
      "Iteration 35: 4.023322162538534e-06\n",
      "Iteration 36: 3.829606612271164e-06\n",
      "Iteration 37: 3.6805943182116607e-06\n",
      "Iteration 38: 3.561384346539853e-06\n",
      "Iteration 39: 3.4421748296153964e-06\n",
      "Iteration 40: 3.322964857943589e-06\n",
      "Iteration 41: 3.21865650221298e-06\n",
      "Iteration 42: 3.1292493076762185e-06\n",
      "Iteration 43: 3.024940724571934e-06\n",
      "Iteration 44: 2.920632368841325e-06\n",
      "Iteration 45: 2.831224946930888e-06\n",
      "Iteration 46: 2.786521463349345e-06\n",
      "Iteration 47: 2.7120154300064314e-06\n",
      "Iteration 48: 2.667311719051213e-06\n",
      "Iteration 49: 2.5928056857082993e-06\n",
      "Iteration 50: 2.563003363320604e-06\n",
      "Iteration 51: 2.4735961687838426e-06\n",
      "Iteration 52: 2.4437938463961473e-06\n",
      "Iteration 53: 2.3692878130532335e-06\n",
      "Iteration 54: 2.354386651859386e-06\n",
      "Iteration 55: 2.324584102098015e-06\n",
      "Iteration 56: 2.2500780687551014e-06\n",
      "Iteration 57: 2.2500780687551014e-06\n",
      "Iteration 58: 2.2351769075612538e-06\n",
      "Iteration 59: 2.145769485650817e-06\n",
      "Iteration 60: 2.1308683244569693e-06\n",
      "Iteration 61: 2.1308683244569693e-06\n",
      "Iteration 62: 2.056362518487731e-06\n",
      "Iteration 63: 2.0265601961000357e-06\n",
      "Iteration 64: 2.0265601961000357e-06\n",
      "Iteration 65: 2.0116588075325126e-06\n",
      "Iteration 66: 1.9371530015632743e-06\n",
      "Iteration 67: 1.9371530015632743e-06\n",
      "Iteration 68: 1.907350679175579e-06\n",
      "Iteration 69: 1.907350679175579e-06\n",
      "Iteration 70: 1.8924494042948936e-06\n",
      "Iteration 71: 1.8328446458326653e-06\n",
      "Iteration 72: 1.8179434846388176e-06\n",
      "Iteration 73: 1.80304232344497e-06\n",
      "Iteration 74: 1.7881411622511223e-06\n",
      "Iteration 75: 1.7881411622511223e-06\n",
      "Iteration 76: 1.7136352425950463e-06\n",
      "Iteration 77: 1.7136352425950463e-06\n",
      "Iteration 78: 1.7136352425950463e-06\n",
      "Iteration 79: 1.698733967714361e-06\n",
      "Iteration 80: 1.6838328065205133e-06\n",
      "Iteration 81: 1.6689316453266656e-06\n",
      "Iteration 82: 1.6093267731775995e-06\n",
      "Iteration 83: 1.6093267731775995e-06\n",
      "Iteration 84: 1.5944256119837519e-06\n",
      "Iteration 85: 1.5944256119837519e-06\n",
      "Iteration 86: 1.5944256119837519e-06\n",
      "Iteration 87: 1.5795244507899042e-06\n",
      "Iteration 88: 1.5646232895960566e-06\n",
      "Iteration 89: 1.4901173699399806e-06\n",
      "Iteration 90: 1.4901173699399806e-06\n",
      "Iteration 91: 1.4901173699399806e-06\n",
      "Iteration 92: 1.475216208746133e-06\n",
      "Iteration 93: 1.475216208746133e-06\n",
      "Iteration 94: 1.475216208746133e-06\n",
      "Iteration 95: 1.475216208746133e-06\n",
      "Iteration 96: 1.4454137726715999e-06\n",
      "Iteration 97: 1.3858090142093715e-06\n",
      "Iteration 98: 1.3858090142093715e-06\n",
      "Iteration 99: 1.3709078530155239e-06\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.0005070384358987212\n",
      "Iteration 1: 0.0004226093296892941\n",
      "Iteration 2: 0.00035011349245905876\n",
      "Iteration 3: 0.0002885941066779196\n",
      "Iteration 4: 0.0002370504371356219\n",
      "Iteration 5: 0.00019430331303738058\n",
      "Iteration 6: 0.00015917410200927407\n",
      "Iteration 7: 0.00013051435234956443\n",
      "Iteration 8: 0.0001072355080395937\n",
      "Iteration 9: 8.839827933115885e-05\n",
      "Iteration 10: 7.324236503336579e-05\n",
      "Iteration 11: 6.0992642829660326e-05\n",
      "Iteration 12: 5.106781463837251e-05\n",
      "Iteration 13: 4.3065447243861854e-05\n",
      "Iteration 14: 3.6598037695512176e-05\n",
      "Iteration 15: 3.130791810690425e-05\n",
      "Iteration 16: 2.6941725081996992e-05\n",
      "Iteration 17: 2.3410046196659096e-05\n",
      "Iteration 18: 2.048934402409941e-05\n",
      "Iteration 19: 1.8030596038443036e-05\n",
      "Iteration 20: 1.6003998098312877e-05\n",
      "Iteration 21: 1.4275432477006689e-05\n",
      "Iteration 22: 1.2785292710759677e-05\n",
      "Iteration 23: 1.1593181625357829e-05\n",
      "Iteration 24: 1.0550087608862668e-05\n",
      "Iteration 25: 9.641104952606838e-06\n",
      "Iteration 26: 8.851335223880596e-06\n",
      "Iteration 27: 8.18077660369454e-06\n",
      "Iteration 28: 7.59962586016627e-06\n",
      "Iteration 29: 7.107882993295789e-06\n",
      "Iteration 30: 6.660844519501552e-06\n",
      "Iteration 31: 6.243609277589712e-06\n",
      "Iteration 32: 5.900879841647111e-06\n",
      "Iteration 33: 5.573052021645708e-06\n",
      "Iteration 34: 5.3048297559143975e-06\n",
      "Iteration 35: 5.08131097376463e-06\n",
      "Iteration 36: 4.842891030421015e-06\n",
      "Iteration 37: 4.634273409465095e-06\n",
      "Iteration 38: 4.440557404450374e-06\n",
      "Iteration 39: 4.291544883017195e-06\n",
      "Iteration 40: 4.097828878002474e-06\n",
      "Iteration 41: 3.978619133704342e-06\n",
      "Iteration 42: 3.859408934658859e-06\n",
      "Iteration 43: 3.7551008063019253e-06\n",
      "Iteration 44: 3.635891062003793e-06\n",
      "Iteration 45: 3.5315824788995087e-06\n",
      "Iteration 46: 3.4272738957952242e-06\n",
      "Iteration 47: 3.382570184840006e-06\n",
      "Iteration 48: 3.2931629903032444e-06\n",
      "Iteration 49: 3.2037555683928076e-06\n",
      "Iteration 50: 3.1739532460051123e-06\n",
      "Iteration 51: 3.0845458240946755e-06\n",
      "Iteration 52: 2.995138629557914e-06\n",
      "Iteration 53: 2.9653360797965433e-06\n",
      "Iteration 54: 2.890830273827305e-06\n",
      "Iteration 55: 2.8461265628720867e-06\n",
      "Iteration 56: 2.771620529529173e-06\n",
      "Iteration 57: 2.75671914096165e-06\n",
      "Iteration 58: 2.741817979767802e-06\n",
      "Iteration 59: 2.652410785231041e-06\n",
      "Iteration 60: 2.637509624037193e-06\n",
      "Iteration 61: 2.563003363320604e-06\n",
      "Iteration 62: 2.5481022021267563e-06\n",
      "Iteration 63: 2.518299879739061e-06\n",
      "Iteration 64: 2.4437938463961473e-06\n",
      "Iteration 65: 2.4288926852022996e-06\n",
      "Iteration 66: 2.4288926852022996e-06\n",
      "Iteration 67: 2.413991524008452e-06\n",
      "Iteration 68: 2.3245843294716906e-06\n",
      "Iteration 69: 2.3096829409041675e-06\n",
      "Iteration 70: 2.3096829409041675e-06\n",
      "Iteration 71: 2.29478177971032e-06\n",
      "Iteration 72: 2.220275746367406e-06\n",
      "Iteration 73: 2.220275746367406e-06\n",
      "Iteration 74: 2.2053745851735584e-06\n",
      "Iteration 75: 2.1904734239797108e-06\n",
      "Iteration 76: 2.1159676180104725e-06\n",
      "Iteration 77: 2.1010662294429494e-06\n",
      "Iteration 78: 2.1010662294429494e-06\n",
      "Iteration 79: 2.0861648408754263e-06\n",
      "Iteration 80: 2.0265601961000357e-06\n",
      "Iteration 81: 2.011659034906188e-06\n",
      "Iteration 82: 1.9967578737123404e-06\n",
      "Iteration 83: 1.9818567125184927e-06\n",
      "Iteration 84: 1.9818567125184927e-06\n",
      "Iteration 85: 1.966955551324645e-06\n",
      "Iteration 86: 1.9073504518019035e-06\n",
      "Iteration 87: 1.8924494042948936e-06\n",
      "Iteration 88: 1.8924494042948936e-06\n",
      "Iteration 89: 1.877548243101046e-06\n",
      "Iteration 90: 1.877548243101046e-06\n",
      "Iteration 91: 1.7881411622511223e-06\n",
      "Iteration 92: 1.7881411622511223e-06\n",
      "Iteration 93: 1.7881411622511223e-06\n",
      "Iteration 94: 1.7732397736835992e-06\n",
      "Iteration 95: 1.7732397736835992e-06\n",
      "Iteration 96: 1.7583386124897515e-06\n",
      "Iteration 97: 1.698733967714361e-06\n",
      "Iteration 98: 1.698733967714361e-06\n",
      "Iteration 99: 1.6838328065205133e-06\n",
      "------- Checking outputs (left) vs ground truth (right): -----\n",
      "tensor([[1.9073e-06, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.9073e-06, 0.0000e+00]])\n",
      "--------------- LNN Parameters (post-training) ---------------\n",
      "OR (beta, argument weights): 9.477 [19.059 17.79 ]\n",
      "AND1 (beta, argument weights): 7.542 [14.488 15.328]\n",
      "AND2 (beta, argument weights): 7.761 [14.913 15.211]\n"
     ]
    }
   ],
   "source": [
    "#this is a hyperparameter\n",
    "alpha = 0.8\n",
    "\n",
    "op_and1 = and_lukasiewicz(alpha, 2, False)\n",
    "op_and2 = and_lukasiewicz(alpha, 2, False)\n",
    "op_or = or_lukasiewicz(alpha, 2, False)\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam([{'params': op_or.parameters()}, \\\n",
    "                        {'params': op_and1.parameters()}, \\\n",
    "                        {'params': op_and2.parameters()}], lr=0.1)\n",
    "\n",
    "for iter in range(100):\n",
    "    op_or.train()\n",
    "    op_and1.train()\n",
    "    op_and2.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = x[:,0].view(-1,1)\n",
    "    x1 = x[:,1].view(-1,1)\n",
    "    yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#check to see output of xor post-training\n",
    "x0 = x[:,0].view(-1,1)\n",
    "x1 = x[:,1].view(-1,1)\n",
    "yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                        op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "check_values = torch.cat((yhat, y), 1)\n",
    "print(\"------- Checking outputs (left) vs ground truth (right): -----\")\n",
    "print(check_values.detach())\n",
    "\n",
    "#LNN parameters: post-training (we have 3 sets of beta, argument weights)\n",
    "print(\"--------------- LNN Parameters (post-training) ---------------\")\n",
    "beta_or, argument_wts_or = op_or.AND.cdd()\n",
    "beta_and1, argument_wts_and1 = op_and1.cdd()\n",
    "beta_and2, argument_wts_and2 = op_and2.cdd()\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"OR (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_or.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_or.detach().numpy()))\n",
    "print(\"AND1 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and1.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and1.detach().numpy()))\n",
    "print(\"AND2 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and2.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arity should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PureNameLNN(nn.Module):\n",
    "#     def __init__(self, alpha, sim_arity=4, rule_arity=2, slack=None):\n",
    "#         super(PureNameLNN, self).__init__()\n",
    "#         self.threshold = 0.5\n",
    "        \n",
    "#         self.sim_disjunction_or = or_lukasiewicz(alpha, sim_arity, slack)\n",
    "    \n",
    "#     def forward(self, x, mention_labels=None):\n",
    "#         \"\"\"\n",
    "#             x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "#                normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "#         \"\"\"\n",
    "#         yhat = None\n",
    "        \n",
    "#         # RULE 1: lookup predicate\n",
    "#         lookup_features = x[:,5]\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "#         # RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb))\n",
    "#         feature_list = []\n",
    "#         # rule 2 (1) mention==label\n",
    "#         mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "#         labels = np.array([m[1].lower() for m in mention_labels])\n",
    "#         exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float()\n",
    "#         feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "#         # rule 2 (2)-(4) Jaro(m, lb) AND Jacc(m, lb) AND Lev(m, lb))\n",
    "#         sim_features = x[:, 0:3]\n",
    "#         print(sim_features)\n",
    "\n",
    "#         return yhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
