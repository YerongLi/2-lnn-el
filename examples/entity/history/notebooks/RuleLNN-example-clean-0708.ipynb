{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fe3201c7e10>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/meta_rule/')\n",
    "\n",
    "from lnn_operators import and_lukasiewicz, or_lukasiewicz, negation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8444, 0.5000, 0.3333, 0.6638, 0.5465, 1.0000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4734, 0.5313, 0.4000],\n",
      "        [0.6375, 0.0000, 0.1250, 0.4944, 0.5258, 1.0000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4476, 0.5201, 0.4000]]) torch.Size([4, 6])\n",
      "tensor([1., 0., 0., 0.]) torch.Size([4])\n",
      "['GMT;GMT Games' 'GMT;UTC+10:00' 'GMT;Giant Magellan Telescope'\n",
      " 'GMT;UTC+12:00']\n"
     ]
    }
   ],
   "source": [
    "# train and val\n",
    "\n",
    "df_train_val = pd.read_csv(\"train.csv\")\n",
    "df_train_val = df_train_val.loc[22:25]\n",
    "\n",
    "features_train_val = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train_val.Features.values])\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "X_train_val = torch.from_numpy(features_train_val).float()\n",
    "print(X_train_val, X_train_val.shape)\n",
    "#the target values for each row in the truth table (xor)\n",
    "Y_train_val = torch.from_numpy(df_train_val.Label.values).float()\n",
    "print(Y_train_val, Y_train_val.shape)\n",
    "# mention_labels (cannot convert string explicitly)\n",
    "mention_labels_train_val = df_train_val.Mention_label.values\n",
    "print(mention_labels_train_val)\n",
    "\n",
    "x_train, x_val, y_train, y_val, m_labels_train, m_labels_val = \\\n",
    "    train_test_split(X_train_val, Y_train_val, mention_labels_train_val, test_size=0.2,train_size=0.8, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8903, 0.6667, 0.5625, 0.8557, 1.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.7529, 0.0000],\n",
      "        [0.9385, 0.6667, 0.6923, 0.9036, 0.1000, 1.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5000, 0.5556, 0.7310, 0.1000, 0.0000],\n",
      "        [0.4256, 0.3333, 0.3846, 0.5935, 0.1000, 0.0000],\n",
      "        [0.6444, 0.3333, 0.3333, 0.5635, 0.1000, 1.0000]]) torch.Size([23846, 6])\n",
      "tensor([0., 1., 0.,  ..., 0., 0., 0.]) torch.Size([23846])\n",
      "['Düsseldorf Airport;Düsseldorf International Airport'\n",
      " 'Düsseldorf Airport;Düsseldorf Airport'\n",
      " 'Düsseldorf Airport;Düsseldorf Airport station' ... 'Pluto;HMS Pluto'\n",
      " 'Pluto;Terry Pluto 1' 'Pluto;The Pluto Files']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "features_test = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_test.Features.values])\n",
    "\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "print(x_test, x_test.shape)\n",
    "y_test = torch.from_numpy(df_test.Label.values).float()\n",
    "print(y_test, y_test.shape)\n",
    "m_labels_test = df_test.Mention_label.values\n",
    "print(m_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([3, 6])\n",
      "val torch.Size([23846, 6])\n"
     ]
    }
   ],
   "source": [
    "print('train', x_train.shape)\n",
    "print('val', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureNameLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(PureNameLNN, self).__init__()\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.sim_disjunction_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(3)])\n",
    "        self.predicate_and = and_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \"\"\"\n",
    "            x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "               normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "        \"\"\"\n",
    "        yhat = None\n",
    "        \n",
    "        ####### RULE 1: lookup predicate #######\n",
    "        lookup_features = x[:,4].view(-1, 1)\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "        \n",
    "        ####### RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb)) #######\n",
    "        feature_list = []\n",
    "        # rule 2 (1) mention==label\n",
    "        mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "        labels = np.array([m[1].lower() for m in mention_labels])\n",
    "        exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float().view(-1,1)\n",
    "        feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "        # rule 2 (2) Jacc(mention, label)\n",
    "        jacc_features = x[:, 1].view(-1,1)\n",
    "#         jacc_features = torch.clamp(jacc_features, min=self.threshold, max=1.0)\n",
    "        jacc_features_ = torch.where(jacc_features>=self.threshold, jacc_features, torch.zeros_like(jacc_features))\n",
    "        feature_list.append(jacc_features_)\n",
    "#         print(\"jacc_features\", jacc_features)\n",
    "#         print(\"jacc_features*mask\", jacc_features_)\n",
    "        \n",
    "        # rule 2 (3) Lev(mention, label)\n",
    "        lev_features = x[:, 2].view(-1,1)\n",
    "        lev_features_ = torch.where(lev_features>=self.threshold, lev_features, torch.zeros_like(lev_features))\n",
    "        feature_list.append(lev_features_)\n",
    "#         print(\"lev_features\", lev_features)\n",
    "#         print(\"lev_features*mask\", lev_features_)\n",
    "        \n",
    "        # rule 2 (4) Jaro(mention, label)\n",
    "        jaro_features = x[:, 0].view(-1,1)\n",
    "        jaro_features_ = torch.where(jaro_features>=self.threshold, jaro_features, torch.zeros_like(jaro_features))\n",
    "        feature_list.append(jaro_features_)\n",
    "#         print(\"jaro_features\", jaro_features)\n",
    "#         print(\"jaro_features*mask\", jaro_features_)\n",
    "        \n",
    "        # disjunction of (1) to (4)\n",
    "        disjunction_result = feature_list[0]\n",
    "        for i in range(0, 3):\n",
    "            disjunction_result = self.sim_disjunction_or_ops[i](torch.cat((disjunction_result, feature_list[i+1]), 1))\n",
    "#             print(\"disjunction_result\", disjunction_result)\n",
    "        \n",
    "        # RULE 1 + RULE 2\n",
    "        yhat = self.predicate_and(torch.cat((lookup_features, disjunction_result), 1))\n",
    "#         print('yhat', yhat)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(ContextLNN, self).__init__()\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.sim_disjunction_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(4)])\n",
    "        self.predicate_and_ops = nn.ModuleList([and_lukasiewicz(alpha, arity, slack) for i in range(2)])\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \"\"\"\n",
    "            x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "               normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "        \"\"\"\n",
    "        yhat = None\n",
    "        ####### RULE 1: lookup predicate #######\n",
    "        lookup_features = x[:,4].view(-1, 1)\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "        ####### RULE 3: contains predicate #######\n",
    "        context_features = x[:,5].view(-1, 1)\n",
    "#         print(\"context_features\", context_features)\n",
    "        # context mask\n",
    "        context_mask = context_features >= 0.25\n",
    "#         print(\"context_mask\", context_mask)\n",
    "        \n",
    "        ####### RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb)) #######\n",
    "        # check: https://github.ibm.com/IBM-Research-AI/enhanced_amr/blob/5501e3af41794353ed9bb147320666622474171a/entity_linking.py#L295\n",
    "        feature_list = []\n",
    "        # rule 2 (1) mention==label\n",
    "        mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "        labels = np.array([m[1].lower() for m in mention_labels])\n",
    "        exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float().view(-1,1)\n",
    "        feature_list.append(exact_match_features)\n",
    "        \n",
    "        # rule 2 (2) Jacc(mention, label)\n",
    "        jacc_features = x[:, 1].view(-1,1)\n",
    "        jacc_features_ = torch.where(jacc_features>=self.threshold-0.3, jacc_features, torch.zeros_like(jacc_features))\n",
    "        feature_list.append(jacc_features_*context_mask)\n",
    "        \n",
    "        # rule 2 (3) Lev(mention, label)\n",
    "        lev_features = x[:, 2].view(-1,1)\n",
    "        lev_features_ = torch.where(lev_features>=self.threshold-0.3, lev_features, torch.zeros_like(lev_features))\n",
    "        feature_list.append(lev_features_*context_mask)\n",
    "#         print(\"lev_features\", lev_features)\n",
    "#         print(\"lev_features*mask\", lev_features_)\n",
    "        \n",
    "        # rule 2 (4) Jaro(mention, label)\n",
    "        jaro_features = x[:, 0].view(-1,1)\n",
    "        jaro_features_ = torch.where(jaro_features>=self.threshold-0.3, jaro_features, torch.zeros_like(jaro_features))\n",
    "        feature_list.append(jaro_features_*context_mask)\n",
    "        \n",
    "        # disjunction of (1) to (4)\n",
    "        disjunction_result = feature_list[0]\n",
    "        for i in range(0, 3):\n",
    "            disjunction_result = self.sim_disjunction_or_ops[i](torch.cat((disjunction_result, feature_list[i+1]), 1))\n",
    "        \n",
    "        # RULE 1 + RULE 2\n",
    "        r1_r2_res = self.predicate_and_ops[0](torch.cat((lookup_features, disjunction_result), 1))\n",
    "        yhat = self.predicate_and_ops[1](torch.cat((r1_r2_res, context_features), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexRuleLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(ComplexRuleLNN, self).__init__()\n",
    "        \n",
    "        self.pureNameRule = PureNameLNN(alpha, arity, None)\n",
    "        self.contextRule = ContextLNN(alpha, arity, None)\n",
    "        self.rule_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(2)])\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \n",
    "        yhat = None\n",
    "        \n",
    "        pure_res = self.pureNameRule(x, mention_labels)\n",
    "        context_res = self.contextRule(x, mention_labels)\n",
    "#         print('context_res', context_res)\n",
    "        pure_context_res = self.rule_or_ops[0](torch.cat((pure_res, context_res), 1))\n",
    "#         print('pure_context_res', pure_context_res)\n",
    "        return pure_context_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PureNameLNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.4524e-03],\n",
      "        [1.7927e-07],\n",
      "        [9.9078e-01],\n",
      "        ...,\n",
      "        [5.0680e-03],\n",
      "        [1.0939e-05],\n",
      "        [8.2041e-04]], grad_fn=<SWhereBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = PureNameLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, val_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.03993843123316765\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 1: 0.03992585465312004\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 2: 0.0399133674800396\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 3: 0.039900947362184525\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 4: 0.039888542145490646\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 5: 0.03987625241279602\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 6: 0.03986399993300438\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 7: 0.03985206410288811\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 8: 0.039843134582042694\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 9: 0.03983452543616295\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 10: 0.03982608765363693\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 11: 0.0398179292678833\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 12: 0.03980989381670952\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 13: 0.039802055805921555\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 14: 0.03979429230093956\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 15: 0.0397869348526001\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 16: 0.03977975249290466\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 17: 0.03977269306778908\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 18: 0.039765626192092896\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 19: 0.039758749306201935\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 20: 0.03975190594792366\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 21: 0.03974528983235359\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 22: 0.03973866254091263\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 23: 0.03973221033811569\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 24: 0.03972586244344711\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 25: 0.03971952199935913\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 26: 0.039712872356176376\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 27: 0.03970608487725258\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 28: 0.0396992564201355\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 29: 0.0396924689412117\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 30: 0.03968581184744835\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 31: 0.039679184556007385\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 32: 0.03967249020934105\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 33: 0.03966381773352623\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 34: 0.039651256054639816\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 35: 0.0396384559571743\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 36: 0.039625685662031174\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 37: 0.03961307182908058\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 38: 0.03960070386528969\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 39: 0.039588674902915955\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 40: 0.03957698494195938\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 41: 0.039565764367580414\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 42: 0.03955484554171562\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 43: 0.03954426571726799\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 44: 0.039533983916044235\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 45: 0.03952401131391525\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 46: 0.0395144447684288\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 47: 0.03950515761971474\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 48: 0.039496101438999176\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 49: 0.03948730602860451\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 50: 0.03947875648736954\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 51: 0.03947051614522934\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 52: 0.03946242108941078\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 53: 0.03945460915565491\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 54: 0.039446983486413956\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 55: 0.03943956270813942\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 56: 0.03943231329321861\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 57: 0.03942522779107094\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 58: 0.039418309926986694\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 59: 0.039411574602127075\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 60: 0.039404962211847305\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 61: 0.03939851373434067\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 62: 0.03939219191670418\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 63: 0.03938606008887291\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 64: 0.03937999904155731\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 65: 0.039374154061079025\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 66: 0.039368320256471634\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 67: 0.03936261311173439\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 68: 0.039357010275125504\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 69: 0.0393514484167099\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 70: 0.03934603929519653\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 71: 0.03934074565768242\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 72: 0.039335545152425766\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 73: 0.03933044150471687\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 74: 0.03932533040642738\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 75: 0.03932039812207222\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 76: 0.03931553289294243\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 77: 0.039310675114393234\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 78: 0.03930598124861717\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 79: 0.0393013134598732\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 80: 0.039296723902225494\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 81: 0.03929222747683525\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 82: 0.03928772732615471\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 83: 0.039283327758312225\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 84: 0.03927896171808243\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 85: 0.03927464038133621\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 86: 0.03927035257220268\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 87: 0.03926617279648781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 88: 0.03926197066903114\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 89: 0.039257876574993134\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 90: 0.0392538383603096\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 91: 0.03924982249736786\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 92: 0.0392458438873291\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 93: 0.039241887629032135\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 94: 0.039237964898347855\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 95: 0.039234090596437454\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 96: 0.039230335503816605\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 97: 0.03922651708126068\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 98: 0.03922282159328461\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 99: 0.039219167083501816\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 100: 0.039215460419654846\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 101: 0.03921187296509743\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 102: 0.039208319038152695\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 103: 0.03920479491353035\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 104: 0.03920130431652069\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 105: 0.039197858422994614\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 106: 0.039194390177726746\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 107: 0.03919104486703873\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 108: 0.03918768838047981\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 109: 0.03918435797095299\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 110: 0.03918113932013512\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 111: 0.03917799890041351\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 112: 0.039174884557724\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 113: 0.03917178511619568\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 114: 0.039168693125247955\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 115: 0.039165694266557693\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 116: 0.03916274011135101\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 117: 0.03915972262620926\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 118: 0.03915679454803467\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 119: 0.03915388137102127\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 120: 0.03915100544691086\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 121: 0.03914817422628403\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 122: 0.039145346730947495\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 123: 0.039142537862062454\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 124: 0.03913978114724159\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 125: 0.039137035608291626\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 126: 0.03913429006934166\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 127: 0.03913155198097229\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 128: 0.03912884369492531\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 129: 0.039126135408878326\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 130: 0.03912346065044403\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 131: 0.03912081569433212\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 132: 0.0391182005405426\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 133: 0.03911559283733368\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 134: 0.039112940430641174\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 135: 0.03911041468381882\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 136: 0.03910788893699646\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 137: 0.03910530358552933\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 138: 0.03910282626748085\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 139: 0.039100270718336105\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 140: 0.039097849279642105\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 141: 0.039095353335142136\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 142: 0.03909287601709366\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 143: 0.03909044712781906\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 144: 0.03908810019493103\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 145: 0.039085693657398224\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 146: 0.03908329829573631\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 147: 0.039080940186977386\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 148: 0.03907860815525055\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 149: 0.039076294749975204\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 150: 0.03907400369644165\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 151: 0.039071712642908096\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 152: 0.03906945884227753\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 153: 0.039067141711711884\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 154: 0.0390649251639843\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 155: 0.03906270116567612\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 156: 0.03906051814556122\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 157: 0.03905835002660751\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 158: 0.039056118577718735\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 159: 0.039053983986377716\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 160: 0.03905186057090759\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 161: 0.03904975950717926\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 162: 0.03904758393764496\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 163: 0.03904552385210991\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 164: 0.03904345631599426\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 165: 0.03904134780168533\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 166: 0.03903929889202118\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 167: 0.03903729096055031\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 168: 0.03903529420495033\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 169: 0.039033208042383194\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 170: 0.0390312485396862\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 171: 0.03902927786111832\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 172: 0.039027318358421326\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 173: 0.03902532160282135\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 174: 0.03902338072657585\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 175: 0.03902146965265274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 176: 0.03901956230401993\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 177: 0.039017580449581146\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 178: 0.03901569917798042\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 179: 0.03901384025812149\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 180: 0.039011966437101364\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 181: 0.03901011869311333\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 182: 0.03900829330086708\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 183: 0.03900645300745964\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 184: 0.039004623889923096\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 185: 0.039002858102321625\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 186: 0.039001066237688065\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 187: 0.038999270647764206\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 188: 0.03899749368429184\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 189: 0.038995709270238876\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 190: 0.0389939546585083\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 191: 0.03899220749735832\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 192: 0.038990478962659836\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 193: 0.038988735526800156\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 194: 0.03898701071739197\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 195: 0.03898538276553154\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 196: 0.03898366540670395\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 197: 0.03898196294903755\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 198: 0.03898026794195175\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 199: 0.038978662341833115\n",
      "val loss tensor(0.0397)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n"
     ]
    }
   ],
   "source": [
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 1000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = val_pred\n",
    "        torch.save(model.state_dict(), \"best_PureNameLNN.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- The best f1 is 0.7727933173225794 w/ naive threshold 0.5\n",
      "Val -- After tuning, the best f1 is 0.9029920974347565 w/ threshold 0.997997997997998\n"
     ]
    }
   ],
   "source": [
    "# tune on val set\n",
    "\n",
    "print(\"Val -- The best f1 is {} w/ naive threshold 0.5\".format(best_val_f1))\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=1000):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -- f1 is 0.8817386090617398 w/ threshold 0.997997997997998\n"
     ]
    }
   ],
   "source": [
    "bestModel = PureNameLNN(0.8, 2, False)\n",
    "bestModel.load_state_dict(torch.load(\"best_PureNameLNN.pt\"))\n",
    "bestModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = bestModel(x_test, m_labels_test)\n",
    "    test_pred = test_pred >= best_tuned_threshold\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='macro')\n",
    "    print(\"Test -- f1 is {} w/ threshold {}\".format(f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(8.6251, grad_fn=<SelectBackward>), tensor([19.7417, 10.8246], grad_fn=<SliceBackward>))\n",
      "(tensor(8.4585, grad_fn=<SelectBackward>), tensor([19.1825, 11.0743], grad_fn=<SliceBackward>))\n",
      "(tensor(7.0526, grad_fn=<SelectBackward>), tensor([16.0262,  8.7171], grad_fn=<SliceBackward>))\n",
      "(tensor(4.1313, grad_fn=<SelectBackward>), tensor([7.7373, 6.6682], grad_fn=<SliceBackward>))\n",
      "=========predicate_and=========\n",
      "and_lukasiewicz(\n",
      "  (cdd): cdd_lnn()\n",
      ") (tensor(2.8722, grad_fn=<SelectBackward>), tensor([6.9262, 3.3761], grad_fn=<SliceBackward>))\n"
     ]
    }
   ],
   "source": [
    "for name, mod in bestModel.named_children():\n",
    "    print(\"========={}=========\".format(name))\n",
    "    if type(mod) == nn.ModuleList:\n",
    "        for each_op in mod:\n",
    "            print(each_op.AND.cdd())\n",
    "    else:\n",
    "        print(mod, mod.cdd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (post-training): 2.8722004890441895\n",
      "argument weights (post-training): tensor([6.9262, 3.3761])\n",
      "beta (post-training): 8.625083923339844\n",
      "argument weights (post-training): tensor([19.7417, 10.8246])\n",
      "beta (post-training): 8.45850658416748\n",
      "argument weights (post-training): tensor([19.1825, 11.0743])\n",
      "beta (post-training): 7.052649021148682\n",
      "argument weights (post-training): tensor([16.0262,  8.7171])\n",
      "beta (post-training): 4.1312785148620605\n",
      "argument weights (post-training): tensor([7.7373, 6.6682])\n"
     ]
    }
   ],
   "source": [
    "# beta, argument_wts = bestModel.predicate_and.cdd()\n",
    "# print(\"beta (post-training): \" + str(beta.item()))\n",
    "# print(\"argument weights (post-training): \" + str(argument_wts.detach()))\n",
    "\n",
    "# for each_op in bestModel.sim_disjunction_or_ops:\n",
    "#     #lets check the LNN conjunction parameters post-training\n",
    "#     #do these look different from the pre-training settings?\n",
    "#     beta, argument_wts = each_op.AND.cdd()\n",
    "#     print(\"beta (post-training): \" + str(beta.item()))\n",
    "#     print(\"argument weights (post-training): \" + str(argument_wts.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ContextLNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9200e-04],\n",
      "        [1.5054e-07],\n",
      "        [9.9943e-01],\n",
      "        ...,\n",
      "        [2.6379e-04],\n",
      "        [1.0349e-05],\n",
      "        [2.5376e-04]], grad_fn=<SWhereBackward>)\n",
      "Iteration 0: 0.2859273850917816\n",
      "val loss tensor(0.2682)\n",
      "f1 w/ 0.5 threshold 0.5848549075800495\n",
      "Iteration 1: 0.2410481721162796\n",
      "val loss tensor(0.2264)\n",
      "f1 w/ 0.5 threshold 0.5960588363785881\n",
      "Iteration 2: 0.20673899352550507\n",
      "val loss tensor(0.1875)\n",
      "f1 w/ 0.5 threshold 0.6139278208619465\n",
      "Iteration 3: 0.1752336174249649\n",
      "val loss tensor(0.1554)\n",
      "f1 w/ 0.5 threshold 0.6279422651402675\n",
      "Iteration 4: 0.1513768881559372\n",
      "val loss tensor(0.1329)\n",
      "f1 w/ 0.5 threshold 0.6470300283758199\n",
      "Iteration 5: 0.1327628642320633\n",
      "val loss tensor(0.1150)\n",
      "f1 w/ 0.5 threshold 0.6585167134759234\n",
      "Iteration 6: 0.11826548725366592\n",
      "val loss tensor(0.1046)\n",
      "f1 w/ 0.5 threshold 0.6729966604011413\n",
      "Iteration 7: 0.10744988918304443\n",
      "val loss tensor(0.0941)\n",
      "f1 w/ 0.5 threshold 0.6797538720201313\n",
      "Iteration 8: 0.09956284612417221\n",
      "val loss tensor(0.0883)\n",
      "f1 w/ 0.5 threshold 0.6855150368395402\n",
      "Iteration 9: 0.09405536949634552\n",
      "val loss tensor(0.0819)\n",
      "f1 w/ 0.5 threshold 0.6998018009294376\n",
      "Iteration 10: 0.08972397446632385\n",
      "val loss tensor(0.0748)\n",
      "f1 w/ 0.5 threshold 0.70502583002583\n",
      "Iteration 11: 0.08479087799787521\n",
      "val loss tensor(0.0687)\n",
      "f1 w/ 0.5 threshold 0.7123999166107481\n",
      "Iteration 12: 0.07821345329284668\n",
      "val loss tensor(0.0626)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 13: 0.07433254271745682\n",
      "val loss tensor(0.0605)\n",
      "f1 w/ 0.5 threshold 0.737836964884521\n",
      "Iteration 14: 0.07177156209945679\n",
      "val loss tensor(0.0586)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 15: 0.06951772421598434\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 16: 0.0673786923289299\n",
      "val loss tensor(0.0563)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 17: 0.06599988788366318\n",
      "val loss tensor(0.0553)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 18: 0.06360871344804764\n",
      "val loss tensor(0.0546)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 19: 0.06222320720553398\n",
      "val loss tensor(0.0540)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 20: 0.0609227754175663\n",
      "val loss tensor(0.0536)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 21: 0.059561342000961304\n",
      "val loss tensor(0.0532)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 22: 0.05859719589352608\n",
      "val loss tensor(0.0529)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 23: 0.05768325924873352\n",
      "val loss tensor(0.0525)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 24: 0.0565960668027401\n",
      "val loss tensor(0.0522)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 25: 0.056002408266067505\n",
      "val loss tensor(0.0517)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 26: 0.05505039915442467\n",
      "val loss tensor(0.0508)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 27: 0.054295796900987625\n",
      "val loss tensor(0.0504)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 28: 0.05367742478847504\n",
      "val loss tensor(0.0501)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 29: 0.05320493504405022\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 30: 0.052634648978710175\n",
      "val loss tensor(0.0496)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 31: 0.05208703130483627\n",
      "val loss tensor(0.0493)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 32: 0.05166373401880264\n",
      "val loss tensor(0.0490)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 33: 0.0513063445687294\n",
      "val loss tensor(0.0487)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 34: 0.05101126432418823\n",
      "val loss tensor(0.0486)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 35: 0.05073432996869087\n",
      "val loss tensor(0.0484)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 36: 0.050489455461502075\n",
      "val loss tensor(0.0483)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 37: 0.05017496645450592\n",
      "val loss tensor(0.0481)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 38: 0.04991751164197922\n",
      "val loss tensor(0.0477)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 39: 0.049715667963027954\n",
      "val loss tensor(0.0474)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 40: 0.049543701112270355\n",
      "val loss tensor(0.0472)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 41: 0.049394797533750534\n",
      "val loss tensor(0.0471)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 42: 0.04923468083143234\n",
      "val loss tensor(0.0470)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 43: 0.049067750573158264\n",
      "val loss tensor(0.0469)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 44: 0.04891333729028702\n",
      "val loss tensor(0.0466)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 45: 0.04860537871718407\n",
      "val loss tensor(0.0463)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 46: 0.04796835780143738\n",
      "val loss tensor(0.0462)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 47: 0.047833457589149475\n",
      "val loss tensor(0.0460)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 48: 0.04767162352800369\n",
      "val loss tensor(0.0458)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 49: 0.047547291964292526\n",
      "val loss tensor(0.0455)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 50: 0.047445669770240784\n",
      "val loss tensor(0.0454)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 51: 0.04711690917611122\n",
      "val loss tensor(0.0448)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 52: 0.047032974660396576\n",
      "val loss tensor(0.0446)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 53: 0.04695623368024826\n",
      "val loss tensor(0.0443)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 54: 0.04679656773805618\n",
      "val loss tensor(0.0441)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 55: 0.04669475927948952\n",
      "val loss tensor(0.0440)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 56: 0.046544693410396576\n",
      "val loss tensor(0.0437)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 57: 0.046396803110837936\n",
      "val loss tensor(0.0434)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 58: 0.04628228396177292\n",
      "val loss tensor(0.0431)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 59: 0.04618437588214874\n",
      "val loss tensor(0.0429)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 60: 0.04609444737434387\n",
      "val loss tensor(0.0427)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 61: 0.046013571321964264\n",
      "val loss tensor(0.0426)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 62: 0.045939259231090546\n",
      "val loss tensor(0.0425)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 63: 0.045811302959918976\n",
      "val loss tensor(0.0424)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 64: 0.04570474848151207\n",
      "val loss tensor(0.0423)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 65: 0.045614879578351974\n",
      "val loss tensor(0.0422)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 66: 0.04554110765457153\n",
      "val loss tensor(0.0421)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 67: 0.04547547549009323\n",
      "val loss tensor(0.0420)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 68: 0.04541512578725815\n",
      "val loss tensor(0.0420)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 69: 0.04535946995019913\n",
      "val loss tensor(0.0419)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 70: 0.04527847096323967\n",
      "val loss tensor(0.0418)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 71: 0.04513586685061455\n",
      "val loss tensor(0.0418)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 72: 0.04502824693918228\n",
      "val loss tensor(0.0417)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 73: 0.04495023563504219\n",
      "val loss tensor(0.0416)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 74: 0.044884614646434784\n",
      "val loss tensor(0.0416)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 75: 0.044823843985795975\n",
      "val loss tensor(0.0415)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 76: 0.04476819932460785\n",
      "val loss tensor(0.0415)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 77: 0.04471709951758385\n",
      "val loss tensor(0.0414)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 78: 0.044670116156339645\n",
      "val loss tensor(0.0414)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 79: 0.04462619125843048\n",
      "val loss tensor(0.0413)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 80: 0.04458530247211456\n",
      "val loss tensor(0.0413)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 81: 0.04454651474952698\n",
      "val loss tensor(0.0411)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 82: 0.044510629028081894\n",
      "val loss tensor(0.0408)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 83: 0.04447885975241661\n",
      "val loss tensor(0.0407)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 84: 0.04444865137338638\n",
      "val loss tensor(0.0406)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 85: 0.04440293461084366\n",
      "val loss tensor(0.0406)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 86: 0.044356729835271835\n",
      "val loss tensor(0.0405)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87: 0.04431500658392906\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 88: 0.04427635297179222\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 89: 0.04423997551202774\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 90: 0.04420547932386398\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 91: 0.044172924011945724\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 92: 0.04414163529872894\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 93: 0.044053349643945694\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 94: 0.043939944356679916\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 95: 0.04386923462152481\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 96: 0.04381221905350685\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 97: 0.04376303777098656\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 98: 0.04368624836206436\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 99: 0.0436105914413929\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 100: 0.0434652678668499\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 101: 0.04335663467645645\n",
      "val loss tensor(0.0394)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 102: 0.04319017753005028\n",
      "val loss tensor(0.0392)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 103: 0.04307891055941582\n",
      "val loss tensor(0.0391)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 104: 0.042995989322662354\n",
      "val loss tensor(0.0390)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 105: 0.042910780757665634\n",
      "val loss tensor(0.0390)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 106: 0.042817696928977966\n",
      "val loss tensor(0.0389)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 107: 0.04273880645632744\n",
      "val loss tensor(0.0388)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 108: 0.0426710806787014\n",
      "val loss tensor(0.0388)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 109: 0.042611151933670044\n",
      "val loss tensor(0.0388)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 110: 0.04253305867314339\n",
      "val loss tensor(0.0387)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 111: 0.04244228079915047\n",
      "val loss tensor(0.0387)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 112: 0.04236641898751259\n",
      "val loss tensor(0.0387)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 113: 0.04229912534356117\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 114: 0.04223693162202835\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 115: 0.04217987507581711\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 116: 0.04212726652622223\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7856135050703569\n",
      "Iteration 117: 0.04205819591879845\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 118: 0.041994500905275345\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 119: 0.04193778336048126\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 120: 0.04188627749681473\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 121: 0.04183843731880188\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 122: 0.04179354012012482\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 123: 0.04175136238336563\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 124: 0.041711993515491486\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 125: 0.041674450039863586\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 126: 0.04163876175880432\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 127: 0.04160492494702339\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 128: 0.041500069200992584\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 129: 0.041263189166784286\n",
      "val loss tensor(0.0383)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 130: 0.041042622178792953\n",
      "val loss tensor(0.0383)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 131: 0.04091261699795723\n",
      "val loss tensor(0.0383)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 132: 0.04081980884075165\n",
      "val loss tensor(0.0382)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 133: 0.04074649512767792\n",
      "val loss tensor(0.0382)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 134: 0.04068475216627121\n",
      "val loss tensor(0.0381)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 135: 0.040630701929330826\n",
      "val loss tensor(0.0381)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 136: 0.04058527201414108\n",
      "val loss tensor(0.0381)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 137: 0.040549539029598236\n",
      "val loss tensor(0.0376)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 138: 0.040504664182662964\n",
      "val loss tensor(0.0375)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 139: 0.04046022146940231\n",
      "val loss tensor(0.0374)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 140: 0.040420640259981155\n",
      "val loss tensor(0.0374)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 141: 0.04038676247000694\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 142: 0.0403556264936924\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 143: 0.040326155722141266\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 144: 0.04029879719018936\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 145: 0.04027266800403595\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 146: 0.04024891182780266\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 147: 0.04022563248872757\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 148: 0.04020383208990097\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 149: 0.040183085948228836\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 150: 0.0401633158326149\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 151: 0.04014444351196289\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 152: 0.04012640193104744\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 153: 0.04010897874832153\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 154: 0.040092285722494125\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 155: 0.040076058357954025\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 156: 0.04006019979715347\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 157: 0.04004453867673874\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 158: 0.04002942889928818\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 159: 0.04001491516828537\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 160: 0.04000096395611763\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 161: 0.03998759388923645\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 162: 0.03997430205345154\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 163: 0.03996171057224274\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 164: 0.03994910418987274\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 165: 0.03993723541498184\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 166: 0.039925575256347656\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 167: 0.03991397097706795\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 168: 0.03990329056978226\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 169: 0.039892349392175674\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 170: 0.03988182544708252\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 171: 0.03987161070108414\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 172: 0.03985513746738434\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 173: 0.03983491659164429\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 174: 0.03981638699769974\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 175: 0.03979814052581787\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 176: 0.03978121280670166\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 177: 0.03976478427648544\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 178: 0.039749402552843094\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 179: 0.03973430022597313\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 180: 0.03972012177109718\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 181: 0.03970617800951004\n",
      "val loss tensor(0.0366)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 182: 0.039692845195531845\n",
      "val loss tensor(0.0366)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 183: 0.03968033939599991\n",
      "val loss tensor(0.0365)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 184: 0.03966781497001648\n",
      "val loss tensor(0.0364)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 185: 0.03964666277170181\n",
      "val loss tensor(0.0364)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 186: 0.0396248921751976\n",
      "val loss tensor(0.0364)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 187: 0.039604369550943375\n",
      "val loss tensor(0.0363)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 188: 0.03958461061120033\n",
      "val loss tensor(0.0363)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 189: 0.03956630080938339\n",
      "val loss tensor(0.0363)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 190: 0.039548978209495544\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 191: 0.03953227400779724\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 192: 0.039516348391771317\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 193: 0.03950135037302971\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 194: 0.03948691859841347\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 195: 0.039473388344049454\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 196: 0.039460111409425735\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 197: 0.03944764286279678\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 198: 0.03943556547164917\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 199: 0.039423707872629166\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = ContextLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, val_pred\n",
    "\n",
    "\n",
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 10000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = val_pred\n",
    "        torch.save(model.state_dict(), \"best_ContextLNN.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- The best f1 is 0.7960179928064651 w/ naive threshold 0.5\n",
      "Val -- After tuning, the best f1 is 0.8071826023682409 w/ threshold 0.992992992992993\n",
      "Test -- f1 is 0.811596787435352 w/ threshold 0.992992992992993\n",
      "prec, recall, f1 0.7739705893126639 0.8616414428353842 0.811596787435352\n"
     ]
    }
   ],
   "source": [
    "# tune on val set\n",
    "\n",
    "print(\"Val -- The best f1 is {} w/ naive threshold 0.5\".format(best_val_f1))\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=1000):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))\n",
    "\n",
    "\n",
    "bestModel = ContextLNN(0.8, 2, False)\n",
    "bestModel.load_state_dict(torch.load(\"best_ContextLNN.pt\"))\n",
    "bestModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = bestModel(x_test, m_labels_test)\n",
    "    test_pred = test_pred >= best_tuned_threshold\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='macro')\n",
    "    print(\"Test -- f1 is {} w/ threshold {}\".format(f1, best_tuned_threshold))\n",
    "    print(\"prec, recall, f1\", prec, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(5.8912, grad_fn=<SelectBackward>), tensor([12.1745,  8.3585], grad_fn=<SliceBackward>))\n",
      "(tensor(8.8607, grad_fn=<SelectBackward>), tensor([19.9964, 11.0276], grad_fn=<SliceBackward>))\n",
      "(tensor(7.7926, grad_fn=<SelectBackward>), tensor([17.1775,  9.7577], grad_fn=<SliceBackward>))\n",
      "(tensor(4.7304, grad_fn=<SelectBackward>), tensor([8.7144, 7.7403], grad_fn=<SliceBackward>))\n",
      "=========predicate_and_ops=========\n",
      "(tensor(5.3318, grad_fn=<SelectBackward>), tensor([15.9246,  6.5601], grad_fn=<SliceBackward>))\n",
      "(tensor(1.8147, grad_fn=<SelectBackward>), tensor([2.9074, 2.0872], grad_fn=<SliceBackward>))\n"
     ]
    }
   ],
   "source": [
    "for name, mod in bestModel.named_children():\n",
    "    print(\"========={}=========\".format(name))\n",
    "    if name == 'sim_disjunction_or_ops':\n",
    "        for each_op in mod:\n",
    "            print(each_op.AND.cdd())\n",
    "    elif name == 'predicate_and_ops':\n",
    "        for each_op in mod:\n",
    "            print(each_op.cdd())\n",
    "    else:\n",
    "        print(mod, mod.cdd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.6863e-03],\n",
      "        [3.4976e-04],\n",
      "        [1.0000e+00],\n",
      "        ...,\n",
      "        [3.7682e-04],\n",
      "        [3.5048e-04],\n",
      "        [3.5691e-04]], grad_fn=<RsubBackward1>)\n",
      "Iteration 0: 0.9573822617530823\n",
      "val loss tensor(0.8644)\n",
      "f1 w/ 0.5 threshold 0.5389048163593573\n",
      "Iteration 1: 0.7983121871948242\n",
      "val loss tensor(0.7087)\n",
      "f1 w/ 0.5 threshold 0.5500277705571773\n",
      "Iteration 2: 0.6616075038909912\n",
      "val loss tensor(0.5831)\n",
      "f1 w/ 0.5 threshold 0.5611543117468767\n",
      "Iteration 3: 0.5385414958000183\n",
      "val loss tensor(0.4938)\n",
      "f1 w/ 0.5 threshold 0.5713164135102224\n",
      "Iteration 4: 0.45025917887687683\n",
      "val loss tensor(0.4145)\n",
      "f1 w/ 0.5 threshold 0.5832352587535825\n",
      "Iteration 5: 0.3785131275653839\n",
      "val loss tensor(0.3597)\n",
      "f1 w/ 0.5 threshold 0.5946052934809039\n",
      "Iteration 6: 0.3310108780860901\n",
      "val loss tensor(0.3109)\n",
      "f1 w/ 0.5 threshold 0.6049645533978529\n",
      "Iteration 7: 0.2867151200771332\n",
      "val loss tensor(0.2659)\n",
      "f1 w/ 0.5 threshold 0.6171883180043028\n",
      "Iteration 8: 0.25164327025413513\n",
      "val loss tensor(0.2293)\n",
      "f1 w/ 0.5 threshold 0.6294995328918541\n",
      "Iteration 9: 0.220547154545784\n",
      "val loss tensor(0.1972)\n",
      "f1 w/ 0.5 threshold 0.6460581903307537\n",
      "Iteration 10: 0.19456565380096436\n",
      "val loss tensor(0.1765)\n",
      "f1 w/ 0.5 threshold 0.656312603798053\n",
      "Iteration 11: 0.17733366787433624\n",
      "val loss tensor(0.1556)\n",
      "f1 w/ 0.5 threshold 0.6704213487052771\n",
      "Iteration 12: 0.16357427835464478\n",
      "val loss tensor(0.1433)\n",
      "f1 w/ 0.5 threshold 0.6797538720201313\n",
      "Iteration 13: 0.15118233859539032\n",
      "val loss tensor(0.1338)\n",
      "f1 w/ 0.5 threshold 0.6825929022563186\n",
      "Iteration 14: 0.14243221282958984\n",
      "val loss tensor(0.1251)\n",
      "f1 w/ 0.5 threshold 0.6932097317622531\n",
      "Iteration 15: 0.1357273906469345\n",
      "val loss tensor(0.1165)\n",
      "f1 w/ 0.5 threshold 0.70502583002583\n",
      "Iteration 16: 0.12959443032741547\n",
      "val loss tensor(0.1104)\n",
      "f1 w/ 0.5 threshold 0.7086520107101221\n",
      "Iteration 17: 0.12384503334760666\n",
      "val loss tensor(0.1069)\n",
      "f1 w/ 0.5 threshold 0.7086520107101221\n",
      "Iteration 18: 0.11996263265609741\n",
      "val loss tensor(0.1017)\n",
      "f1 w/ 0.5 threshold 0.7105103577470134\n",
      "Iteration 19: 0.11568249762058258\n",
      "val loss tensor(0.0974)\n",
      "f1 w/ 0.5 threshold 0.7202869958181574\n",
      "Iteration 20: 0.11134393513202667\n",
      "val loss tensor(0.0955)\n",
      "f1 w/ 0.5 threshold 0.7223454216183892\n",
      "Iteration 21: 0.10840903222560883\n",
      "val loss tensor(0.0932)\n",
      "f1 w/ 0.5 threshold 0.7223454216183892\n",
      "Iteration 22: 0.10485221445560455\n",
      "val loss tensor(0.0908)\n",
      "f1 w/ 0.5 threshold 0.7265729402400928\n",
      "Iteration 23: 0.10254904627799988\n",
      "val loss tensor(0.0893)\n",
      "f1 w/ 0.5 threshold 0.7265729402400928\n",
      "Iteration 24: 0.10086236149072647\n",
      "val loss tensor(0.0874)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 25: 0.09865526109933853\n",
      "val loss tensor(0.0862)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 26: 0.09689716249704361\n",
      "val loss tensor(0.0847)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 27: 0.09454996138811111\n",
      "val loss tensor(0.0832)\n",
      "f1 w/ 0.5 threshold 0.730954911724363\n",
      "Iteration 28: 0.09267484396696091\n",
      "val loss tensor(0.0807)\n",
      "f1 w/ 0.5 threshold 0.7332065499975071\n",
      "Iteration 29: 0.09102161973714828\n",
      "val loss tensor(0.0799)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 30: 0.08946888148784637\n",
      "val loss tensor(0.0787)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 31: 0.08827196061611176\n",
      "val loss tensor(0.0779)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 32: 0.08675168454647064\n",
      "val loss tensor(0.0774)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 33: 0.0847276821732521\n",
      "val loss tensor(0.0769)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 34: 0.08372052758932114\n",
      "val loss tensor(0.0760)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 35: 0.08295141905546188\n",
      "val loss tensor(0.0754)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 36: 0.08228398114442825\n",
      "val loss tensor(0.0751)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 37: 0.0816381573677063\n",
      "val loss tensor(0.0747)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 38: 0.08116227388381958\n",
      "val loss tensor(0.0744)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 39: 0.08070433139801025\n",
      "val loss tensor(0.0742)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 40: 0.07984835654497147\n",
      "val loss tensor(0.0739)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 41: 0.0793939083814621\n",
      "val loss tensor(0.0736)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 42: 0.07900255918502808\n",
      "val loss tensor(0.0734)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 43: 0.07865896075963974\n",
      "val loss tensor(0.0732)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 44: 0.07833129167556763\n",
      "val loss tensor(0.0730)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 45: 0.07802063226699829\n",
      "val loss tensor(0.0728)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 46: 0.07764691859483719\n",
      "val loss tensor(0.0724)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 47: 0.07718026638031006\n",
      "val loss tensor(0.0716)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 48: 0.07664802670478821\n",
      "val loss tensor(0.0712)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 49: 0.0762629359960556\n",
      "val loss tensor(0.0710)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 50: 0.07594947516918182\n",
      "val loss tensor(0.0708)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 51: 0.0755188912153244\n",
      "val loss tensor(0.0706)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 52: 0.07502666860818863\n",
      "val loss tensor(0.0705)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 53: 0.07459666579961777\n",
      "val loss tensor(0.0703)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 54: 0.07389184832572937\n",
      "val loss tensor(0.0701)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 55: 0.07346701622009277\n",
      "val loss tensor(0.0700)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 56: 0.07292782515287399\n",
      "val loss tensor(0.0698)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 57: 0.07259827107191086\n",
      "val loss tensor(0.0697)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 58: 0.07232951372861862\n",
      "val loss tensor(0.0696)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 59: 0.07205768674612045\n",
      "val loss tensor(0.0694)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 60: 0.07182914763689041\n",
      "val loss tensor(0.0693)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 61: 0.07158388197422028\n",
      "val loss tensor(0.0692)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 62: 0.0713578313589096\n",
      "val loss tensor(0.0690)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 63: 0.07100388407707214\n",
      "val loss tensor(0.0685)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 64: 0.07003523409366608\n",
      "val loss tensor(0.0681)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 65: 0.06932520121335983\n",
      "val loss tensor(0.0677)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 66: 0.06890159100294113\n",
      "val loss tensor(0.0675)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 67: 0.06862232089042664\n",
      "val loss tensor(0.0673)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 68: 0.06845177710056305\n",
      "val loss tensor(0.0672)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 69: 0.06822183728218079\n",
      "val loss tensor(0.0670)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 70: 0.06799834966659546\n",
      "val loss tensor(0.0668)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 71: 0.06773873418569565\n",
      "val loss tensor(0.0666)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 72: 0.06756342947483063\n",
      "val loss tensor(0.0665)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 73: 0.06741606444120407\n",
      "val loss tensor(0.0663)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 74: 0.06728531420230865\n",
      "val loss tensor(0.0662)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 75: 0.06716883927583694\n",
      "val loss tensor(0.0661)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 76: 0.06695881485939026\n",
      "val loss tensor(0.0659)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 77: 0.06670712679624557\n",
      "val loss tensor(0.0658)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 78: 0.06643928587436676\n",
      "val loss tensor(0.0657)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 79: 0.06626658141613007\n",
      "val loss tensor(0.0656)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 80: 0.06612136960029602\n",
      "val loss tensor(0.0655)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 81: 0.0659990906715393\n",
      "val loss tensor(0.0654)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 82: 0.06589484959840775\n",
      "val loss tensor(0.0651)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 83: 0.06571509689092636\n",
      "val loss tensor(0.0648)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 84: 0.06558798998594284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0646)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 85: 0.06548275798559189\n",
      "val loss tensor(0.0645)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 86: 0.0653853565454483\n",
      "val loss tensor(0.0644)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 87: 0.06529440730810165\n",
      "val loss tensor(0.0643)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 88: 0.06520730257034302\n",
      "val loss tensor(0.0642)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 89: 0.06512439996004105\n",
      "val loss tensor(0.0641)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 90: 0.06504955887794495\n",
      "val loss tensor(0.0641)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 91: 0.0649760365486145\n",
      "val loss tensor(0.0640)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 92: 0.06483722478151321\n",
      "val loss tensor(0.0639)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 93: 0.06472838670015335\n",
      "val loss tensor(0.0639)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 94: 0.06463892012834549\n",
      "val loss tensor(0.0638)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 95: 0.06455688923597336\n",
      "val loss tensor(0.0638)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 96: 0.06448031961917877\n",
      "val loss tensor(0.0637)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 97: 0.06440705806016922\n",
      "val loss tensor(0.0637)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 98: 0.0643375962972641\n",
      "val loss tensor(0.0636)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 99: 0.06427059322595596\n",
      "val loss tensor(0.0636)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 100: 0.06420588493347168\n",
      "val loss tensor(0.0635)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 101: 0.064143106341362\n",
      "val loss tensor(0.0635)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 102: 0.06402061134576797\n",
      "val loss tensor(0.0634)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 103: 0.0639280378818512\n",
      "val loss tensor(0.0634)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 104: 0.06385088711977005\n",
      "val loss tensor(0.0633)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 105: 0.06378060579299927\n",
      "val loss tensor(0.0633)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 106: 0.06371445953845978\n",
      "val loss tensor(0.0632)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 107: 0.06358391791582108\n",
      "val loss tensor(0.0632)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 108: 0.06343980133533478\n",
      "val loss tensor(0.0631)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 109: 0.06330463290214539\n",
      "val loss tensor(0.0630)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 110: 0.06320517510175705\n",
      "val loss tensor(0.0630)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 111: 0.06312074512243271\n",
      "val loss tensor(0.0629)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 112: 0.063046894967556\n",
      "val loss tensor(0.0629)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 113: 0.06297903507947922\n",
      "val loss tensor(0.0628)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 114: 0.06291625648736954\n",
      "val loss tensor(0.0628)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 115: 0.06285570561885834\n",
      "val loss tensor(0.0627)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 116: 0.06280048191547394\n",
      "val loss tensor(0.0627)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 117: 0.06274885684251785\n",
      "val loss tensor(0.0626)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 118: 0.06270213425159454\n",
      "val loss tensor(0.0626)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 119: 0.0626557394862175\n",
      "val loss tensor(0.0625)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 120: 0.06261108070611954\n",
      "val loss tensor(0.0625)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 121: 0.0625675618648529\n",
      "val loss tensor(0.0624)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 122: 0.06252395361661911\n",
      "val loss tensor(0.0624)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 123: 0.06248168647289276\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 124: 0.06244070827960968\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 125: 0.062399618327617645\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 126: 0.06235966086387634\n",
      "val loss tensor(0.0622)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 127: 0.06232014670968056\n",
      "val loss tensor(0.0622)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 128: 0.062281761318445206\n",
      "val loss tensor(0.0621)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 129: 0.06224387139081955\n",
      "val loss tensor(0.0621)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 130: 0.062206242233514786\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 131: 0.06216920167207718\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 132: 0.06213327497243881\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 133: 0.06209752708673477\n",
      "val loss tensor(0.0619)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 134: 0.06206225976347923\n",
      "val loss tensor(0.0619)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 135: 0.062021106481552124\n",
      "val loss tensor(0.0618)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 136: 0.06191370263695717\n",
      "val loss tensor(0.0618)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 137: 0.06184849515557289\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 138: 0.06179441884160042\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 139: 0.061745792627334595\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 140: 0.06170079484581947\n",
      "val loss tensor(0.0616)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 141: 0.06165821850299835\n",
      "val loss tensor(0.0616)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 142: 0.06161757931113243\n",
      "val loss tensor(0.0615)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 143: 0.06157737970352173\n",
      "val loss tensor(0.0615)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 144: 0.061534181237220764\n",
      "val loss tensor(0.0614)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 145: 0.061480652540922165\n",
      "val loss tensor(0.0614)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 146: 0.06142852082848549\n",
      "val loss tensor(0.0614)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 147: 0.06137336045503616\n",
      "val loss tensor(0.0612)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 148: 0.06129705533385277\n",
      "val loss tensor(0.0610)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 149: 0.06113814562559128\n",
      "val loss tensor(0.0608)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 150: 0.061001256108284\n",
      "val loss tensor(0.0606)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 151: 0.060761064291000366\n",
      "val loss tensor(0.0604)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 152: 0.06063036993145943\n",
      "val loss tensor(0.0596)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 153: 0.060562700033187866\n",
      "val loss tensor(0.0595)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 154: 0.06050149351358414\n",
      "val loss tensor(0.0594)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 155: 0.0604444183409214\n",
      "val loss tensor(0.0593)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 156: 0.06039157882332802\n",
      "val loss tensor(0.0592)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 157: 0.06034117937088013\n",
      "val loss tensor(0.0592)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 158: 0.06029430404305458\n",
      "val loss tensor(0.0591)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 159: 0.060198571532964706\n",
      "val loss tensor(0.0590)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 160: 0.06009576842188835\n",
      "val loss tensor(0.0589)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 161: 0.060009051114320755\n",
      "val loss tensor(0.0589)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 162: 0.05992720276117325\n",
      "val loss tensor(0.0588)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 163: 0.05985967442393303\n",
      "val loss tensor(0.0585)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 164: 0.0598125122487545\n",
      "val loss tensor(0.0583)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 165: 0.059768196195364\n",
      "val loss tensor(0.0582)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 166: 0.05972554162144661\n",
      "val loss tensor(0.0581)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 167: 0.05968529358506203\n",
      "val loss tensor(0.0581)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 168: 0.05964692682027817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0580)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 169: 0.059609394520521164\n",
      "val loss tensor(0.0579)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 170: 0.05957398936152458\n",
      "val loss tensor(0.0579)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 171: 0.0595400333404541\n",
      "val loss tensor(0.0578)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 172: 0.059509534388780594\n",
      "val loss tensor(0.0578)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 173: 0.059479229152202606\n",
      "val loss tensor(0.0577)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 174: 0.05945027247071266\n",
      "val loss tensor(0.0577)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 175: 0.0594213642179966\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 176: 0.05939330905675888\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 177: 0.05936622619628906\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 178: 0.05933916196227074\n",
      "val loss tensor(0.0575)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 179: 0.05931293964385986\n",
      "val loss tensor(0.0575)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 180: 0.05928676202893257\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 181: 0.05926106497645378\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 182: 0.05923580005764961\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 183: 0.05920995771884918\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 184: 0.0591849610209465\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 185: 0.05916082113981247\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 186: 0.05913598835468292\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 187: 0.05911250784993172\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 188: 0.05908971279859543\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 189: 0.05906708911061287\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 190: 0.059044282883405685\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 191: 0.059022217988967896\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 192: 0.059000566601753235\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 193: 0.05897759646177292\n",
      "val loss tensor(0.0570)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 194: 0.05894763767719269\n",
      "val loss tensor(0.0570)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 195: 0.058852002024650574\n",
      "val loss tensor(0.0570)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 196: 0.05866466462612152\n",
      "val loss tensor(0.0569)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 197: 0.058525096625089645\n",
      "val loss tensor(0.0565)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 198: 0.058421872556209564\n",
      "val loss tensor(0.0564)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 199: 0.058340393006801605\n",
      "val loss tensor(0.0563)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = ComplexRuleLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, val_pred\n",
    "\n",
    "\n",
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 10000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = val_pred\n",
    "        torch.save(model.state_dict(), \"best_complex.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- The best f1 is 0.7638835403228394 w/ naive threshold 0.5\n",
      "Val -- After tuning, the best f1 is 0.7856135050703569 w/ threshold 0.992992992992993\n",
      "Test -- f1 is 0.8164090967900519 w/ threshold 0.992992992992993\n"
     ]
    }
   ],
   "source": [
    "# tune on val set\n",
    "\n",
    "print(\"Val -- The best f1 is {} w/ naive threshold 0.5\".format(best_val_f1))\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=1000):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))\n",
    "\n",
    "\n",
    "bestModel = ComplexRuleLNN(0.8, 2, False)\n",
    "bestModel.load_state_dict(torch.load(\"best_complex.pt\"))\n",
    "bestModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = bestModel(x_test, m_labels_test)\n",
    "    test_pred = test_pred >= best_tuned_threshold\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='macro')\n",
    "    print(\"Test -- f1 is {} w/ threshold {}\".format(f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(5.1019, grad_fn=<SelectBackward>), tensor([10.4697,  8.0609], grad_fn=<SliceBackward>))\n",
      "(tensor(5.7117, grad_fn=<SelectBackward>), tensor([ 8.2640, 11.7477], grad_fn=<SliceBackward>))\n",
      "(tensor(6.4294, grad_fn=<SelectBackward>), tensor([12.9908,  8.3084], grad_fn=<SliceBackward>))\n",
      "(tensor(3.8584, grad_fn=<SelectBackward>), tensor([6.8998, 6.3426], grad_fn=<SliceBackward>))\n",
      "=========predicate_and=========\n",
      "predicate_and (tensor(4.6245, grad_fn=<SelectBackward>), tensor([13.1790,  5.6998], grad_fn=<SliceBackward>))\n",
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(6.3645, grad_fn=<SelectBackward>), tensor([13.3272,  9.0812], grad_fn=<SliceBackward>))\n",
      "(tensor(6.9162, grad_fn=<SelectBackward>), tensor([14.1029,  9.5137], grad_fn=<SliceBackward>))\n",
      "(tensor(6.0238, grad_fn=<SelectBackward>), tensor([13.0749,  7.8272], grad_fn=<SliceBackward>))\n",
      "(tensor(4.1672, grad_fn=<SelectBackward>), tensor([6.7004, 7.7094], grad_fn=<SliceBackward>))\n",
      "=========predicate_and_ops=========\n",
      "(tensor(4.4623, grad_fn=<SelectBackward>), tensor([12.3697,  5.6580], grad_fn=<SliceBackward>))\n",
      "(tensor(4.3978, grad_fn=<SelectBackward>), tensor([12.1416,  5.4737], grad_fn=<SliceBackward>))\n",
      "=========0=========\n",
      "0 (tensor(1.5519, grad_fn=<SelectBackward>), tensor([1.7932, 1.7873], grad_fn=<SliceBackward>))\n",
      "=========1=========\n",
      "1 (tensor(4.0893, grad_fn=<SelectBackward>), tensor([6.6543, 6.9582], grad_fn=<SliceBackward>))\n"
     ]
    }
   ],
   "source": [
    "for name1, mod1 in bestModel.named_children():\n",
    "    for name, mod in mod1.named_children():\n",
    "        print(\"========={}=========\".format(name))\n",
    "        if name == 'sim_disjunction_or_ops':\n",
    "            for each_op in mod:\n",
    "                print(each_op.AND.cdd())\n",
    "        elif name == 'predicate_and_ops':\n",
    "            for each_op in mod:\n",
    "                print(each_op.cdd())\n",
    "        elif 'and' in name:\n",
    "            print(name, mod.cdd())\n",
    "        else:\n",
    "            print(name, mod.AND.cdd())\n",
    "#     if 'and' in name1:\n",
    "#         print(name1, mod1.cdd())\n",
    "#     elif 'or' in name1:\n",
    "#         print(name1, mod1.AND.cdd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pureNameRule.sim_disjunction_or_ops.0.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.6911]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.0.AND.cdd.mu Parameter containing:\n",
      "tensor([[1.5432, 0.1545, 0.8296]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.1.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.4595]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.1.AND.cdd.mu Parameter containing:\n",
      "tensor([[-0.3108,  1.7886,  1.6416]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.2.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.6444]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.2.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 1.9484, -1.4607,  2.6722]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.3.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.1029]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.3.AND.cdd.mu Parameter containing:\n",
      "tensor([[0.4294, 0.0292, 0.2391]], requires_grad=True)\n",
      "pureNameRule.predicate_and.cdd.gamma Parameter containing:\n",
      "tensor([[0.0540]], requires_grad=True)\n",
      "pureNameRule.predicate_and.cdd.mu Parameter containing:\n",
      "tensor([[ 3.0113, -2.6593, -2.2786]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.0.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.5774]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.0.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 2.1371, -0.3097,  2.0438]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.1.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.3351]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.1.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 2.1755, -0.5724,  2.7221]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.2.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.7987]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.2.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 2.2144, -1.4073,  1.9542]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.3.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.1124]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.3.AND.cdd.mu Parameter containing:\n",
      "tensor([[0.0068, 0.6955, 0.4942]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.0.cdd.gamma Parameter containing:\n",
      "tensor([[0.6384]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.0.cdd.mu Parameter containing:\n",
      "tensor([[ 2.7550, -1.9580, -2.1179]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.1.cdd.gamma Parameter containing:\n",
      "tensor([[0.9517]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.1.cdd.mu Parameter containing:\n",
      "tensor([[ 2.6922, -2.3561, -1.8245]], requires_grad=True)\n",
      "rule_or_ops.0.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.3596]], requires_grad=True)\n",
      "rule_or_ops.0.AND.cdd.mu Parameter containing:\n",
      "tensor([[-3.1657, -3.2260, -2.5996]], requires_grad=True)\n",
      "rule_or_ops.1.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.7368]], requires_grad=True)\n",
      "rule_or_ops.1.AND.cdd.mu Parameter containing:\n",
      "tensor([[0.0473, 0.2724, 0.7452]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in bestModel.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (post-training): 4.196691513061523\n",
      "argument weights (post-training): tensor([7.9233, 7.2595])\n",
      "beta (post-training): 3.902745246887207\n",
      "argument weights (post-training): tensor([6.9828, 6.3890])\n",
      "beta (post-training): 4.169642925262451\n",
      "argument weights (post-training): tensor([7.6434, 7.0587])\n",
      "beta (post-training): 4.3188300132751465\n",
      "argument weights (post-training): tensor([7.2704, 8.2539])\n",
      "beta (post-training): 4.196691513061523\n",
      "argument weights (post-training): tensor([7.9233, 7.2595])\n",
      "beta (post-training): 3.902745246887207\n",
      "argument weights (post-training): tensor([6.9828, 6.3890])\n",
      "beta (post-training): 4.169642925262451\n",
      "argument weights (post-training): tensor([7.6434, 7.0587])\n",
      "beta (post-training): 4.3188300132751465\n",
      "argument weights (post-training): tensor([7.2704, 8.2539])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for each_op in bestModel.sim_disjunction_or_ops:\n",
    "    #lets check the LNN conjunction parameters post-training\n",
    "    #do these look different from the pre-training settings?\n",
    "    beta, argument_wts = each_op.AND.cdd()\n",
    "    print(\"beta (post-training): \" + str(beta.item()))\n",
    "    print(\"argument weights (post-training): \" + str(argument_wts.detach()))\n",
    "\n",
    "for each_op in bestModel.sim_disjunction_or_ops:\n",
    "    #lets check the LNN conjunction parameters post-training\n",
    "    #do these look different from the pre-training settings?\n",
    "    beta, argument_wts = each_op.AND.cdd()\n",
    "    print(\"beta (post-training): \" + str(beta.item()))\n",
    "    print(\"argument weights (post-training): \" + str(argument_wts.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for XOR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xorLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack):\n",
    "        super(xorLNN, self).__init__()\n",
    "        self.op_and1 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_and2 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_or = or_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = x[:,0].view(-1,1)\n",
    "        print(x0)\n",
    "        x1 = x[:,1].view(-1,1)\n",
    "        print(x1)\n",
    "        print(torch.cat((x0, negation(x1)), 1))\n",
    "        yhat = self.op_or(torch.cat((self.op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            self.op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.6349e-04],\n",
       "        [9.9932e-01],\n",
       "        [9.9967e-01],\n",
       "        [4.6349e-04]], grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xorLNN(0.8, 2, False)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.00041395798325538635\n",
      "Iteration 1: 0.0003413597878534347\n",
      "Iteration 2: 0.00027990160742774606\n",
      "Iteration 3: 0.00022847841319162399\n",
      "Iteration 4: 0.00018598556926008314\n",
      "Iteration 5: 0.00015122962940949947\n",
      "Iteration 6: 0.0001230025663971901\n",
      "Iteration 7: 0.00010024568473454565\n",
      "Iteration 8: 8.197502756956965e-05\n",
      "Iteration 9: 6.734087219228968e-05\n",
      "Iteration 10: 5.5627755500609055e-05\n",
      "Iteration 11: 4.625439760275185e-05\n",
      "Iteration 12: 3.872895103995688e-05\n",
      "Iteration 13: 3.267884312663227e-05\n",
      "Iteration 14: 2.7761290766648017e-05\n",
      "Iteration 15: 2.3767666789353825e-05\n",
      "Iteration 16: 2.051913361356128e-05\n",
      "Iteration 17: 1.7821967048803344e-05\n",
      "Iteration 18: 1.5646355677745305e-05\n",
      "Iteration 19: 1.3783679605694488e-05\n",
      "Iteration 20: 1.2263739336049184e-05\n",
      "Iteration 21: 1.0967321941279806e-05\n",
      "Iteration 22: 9.89442560239695e-06\n",
      "Iteration 23: 8.970543603936676e-06\n",
      "Iteration 24: 8.165872714016587e-06\n",
      "Iteration 25: 7.480413842131384e-06\n",
      "Iteration 26: 6.899264008097816e-06\n",
      "Iteration 27: 6.407521595974686e-06\n",
      "Iteration 28: 5.975385647616349e-06\n",
      "Iteration 29: 5.587952728092205e-06\n",
      "Iteration 30: 5.260125362838153e-06\n",
      "Iteration 31: 4.932297997584101e-06\n",
      "Iteration 32: 4.678976893046638e-06\n",
      "Iteration 33: 4.395853011374129e-06\n",
      "Iteration 34: 4.231939783494454e-06\n",
      "Iteration 35: 4.023322162538534e-06\n",
      "Iteration 36: 3.829606612271164e-06\n",
      "Iteration 37: 3.6805943182116607e-06\n",
      "Iteration 38: 3.561384346539853e-06\n",
      "Iteration 39: 3.4421748296153964e-06\n",
      "Iteration 40: 3.322964857943589e-06\n",
      "Iteration 41: 3.21865650221298e-06\n",
      "Iteration 42: 3.1292493076762185e-06\n",
      "Iteration 43: 3.024940724571934e-06\n",
      "Iteration 44: 2.920632368841325e-06\n",
      "Iteration 45: 2.831224946930888e-06\n",
      "Iteration 46: 2.786521463349345e-06\n",
      "Iteration 47: 2.7120154300064314e-06\n",
      "Iteration 48: 2.667311719051213e-06\n",
      "Iteration 49: 2.5928056857082993e-06\n",
      "Iteration 50: 2.563003363320604e-06\n",
      "Iteration 51: 2.4735961687838426e-06\n",
      "Iteration 52: 2.4437938463961473e-06\n",
      "Iteration 53: 2.3692878130532335e-06\n",
      "Iteration 54: 2.354386651859386e-06\n",
      "Iteration 55: 2.324584102098015e-06\n",
      "Iteration 56: 2.2500780687551014e-06\n",
      "Iteration 57: 2.2500780687551014e-06\n",
      "Iteration 58: 2.2351769075612538e-06\n",
      "Iteration 59: 2.145769485650817e-06\n",
      "Iteration 60: 2.1308683244569693e-06\n",
      "Iteration 61: 2.1308683244569693e-06\n",
      "Iteration 62: 2.056362518487731e-06\n",
      "Iteration 63: 2.0265601961000357e-06\n",
      "Iteration 64: 2.0265601961000357e-06\n",
      "Iteration 65: 2.0116588075325126e-06\n",
      "Iteration 66: 1.9371530015632743e-06\n",
      "Iteration 67: 1.9371530015632743e-06\n",
      "Iteration 68: 1.907350679175579e-06\n",
      "Iteration 69: 1.907350679175579e-06\n",
      "Iteration 70: 1.8924494042948936e-06\n",
      "Iteration 71: 1.8328446458326653e-06\n",
      "Iteration 72: 1.8179434846388176e-06\n",
      "Iteration 73: 1.80304232344497e-06\n",
      "Iteration 74: 1.7881411622511223e-06\n",
      "Iteration 75: 1.7881411622511223e-06\n",
      "Iteration 76: 1.7136352425950463e-06\n",
      "Iteration 77: 1.7136352425950463e-06\n",
      "Iteration 78: 1.7136352425950463e-06\n",
      "Iteration 79: 1.698733967714361e-06\n",
      "Iteration 80: 1.6838328065205133e-06\n",
      "Iteration 81: 1.6689316453266656e-06\n",
      "Iteration 82: 1.6093267731775995e-06\n",
      "Iteration 83: 1.6093267731775995e-06\n",
      "Iteration 84: 1.5944256119837519e-06\n",
      "Iteration 85: 1.5944256119837519e-06\n",
      "Iteration 86: 1.5944256119837519e-06\n",
      "Iteration 87: 1.5795244507899042e-06\n",
      "Iteration 88: 1.5646232895960566e-06\n",
      "Iteration 89: 1.4901173699399806e-06\n",
      "Iteration 90: 1.4901173699399806e-06\n",
      "Iteration 91: 1.4901173699399806e-06\n",
      "Iteration 92: 1.475216208746133e-06\n",
      "Iteration 93: 1.475216208746133e-06\n",
      "Iteration 94: 1.475216208746133e-06\n",
      "Iteration 95: 1.475216208746133e-06\n",
      "Iteration 96: 1.4454137726715999e-06\n",
      "Iteration 97: 1.3858090142093715e-06\n",
      "Iteration 98: 1.3858090142093715e-06\n",
      "Iteration 99: 1.3709078530155239e-06\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.0005070384358987212\n",
      "Iteration 1: 0.0004226093296892941\n",
      "Iteration 2: 0.00035011349245905876\n",
      "Iteration 3: 0.0002885941066779196\n",
      "Iteration 4: 0.0002370504371356219\n",
      "Iteration 5: 0.00019430331303738058\n",
      "Iteration 6: 0.00015917410200927407\n",
      "Iteration 7: 0.00013051435234956443\n",
      "Iteration 8: 0.0001072355080395937\n",
      "Iteration 9: 8.839827933115885e-05\n",
      "Iteration 10: 7.324236503336579e-05\n",
      "Iteration 11: 6.0992642829660326e-05\n",
      "Iteration 12: 5.106781463837251e-05\n",
      "Iteration 13: 4.3065447243861854e-05\n",
      "Iteration 14: 3.6598037695512176e-05\n",
      "Iteration 15: 3.130791810690425e-05\n",
      "Iteration 16: 2.6941725081996992e-05\n",
      "Iteration 17: 2.3410046196659096e-05\n",
      "Iteration 18: 2.048934402409941e-05\n",
      "Iteration 19: 1.8030596038443036e-05\n",
      "Iteration 20: 1.6003998098312877e-05\n",
      "Iteration 21: 1.4275432477006689e-05\n",
      "Iteration 22: 1.2785292710759677e-05\n",
      "Iteration 23: 1.1593181625357829e-05\n",
      "Iteration 24: 1.0550087608862668e-05\n",
      "Iteration 25: 9.641104952606838e-06\n",
      "Iteration 26: 8.851335223880596e-06\n",
      "Iteration 27: 8.18077660369454e-06\n",
      "Iteration 28: 7.59962586016627e-06\n",
      "Iteration 29: 7.107882993295789e-06\n",
      "Iteration 30: 6.660844519501552e-06\n",
      "Iteration 31: 6.243609277589712e-06\n",
      "Iteration 32: 5.900879841647111e-06\n",
      "Iteration 33: 5.573052021645708e-06\n",
      "Iteration 34: 5.3048297559143975e-06\n",
      "Iteration 35: 5.08131097376463e-06\n",
      "Iteration 36: 4.842891030421015e-06\n",
      "Iteration 37: 4.634273409465095e-06\n",
      "Iteration 38: 4.440557404450374e-06\n",
      "Iteration 39: 4.291544883017195e-06\n",
      "Iteration 40: 4.097828878002474e-06\n",
      "Iteration 41: 3.978619133704342e-06\n",
      "Iteration 42: 3.859408934658859e-06\n",
      "Iteration 43: 3.7551008063019253e-06\n",
      "Iteration 44: 3.635891062003793e-06\n",
      "Iteration 45: 3.5315824788995087e-06\n",
      "Iteration 46: 3.4272738957952242e-06\n",
      "Iteration 47: 3.382570184840006e-06\n",
      "Iteration 48: 3.2931629903032444e-06\n",
      "Iteration 49: 3.2037555683928076e-06\n",
      "Iteration 50: 3.1739532460051123e-06\n",
      "Iteration 51: 3.0845458240946755e-06\n",
      "Iteration 52: 2.995138629557914e-06\n",
      "Iteration 53: 2.9653360797965433e-06\n",
      "Iteration 54: 2.890830273827305e-06\n",
      "Iteration 55: 2.8461265628720867e-06\n",
      "Iteration 56: 2.771620529529173e-06\n",
      "Iteration 57: 2.75671914096165e-06\n",
      "Iteration 58: 2.741817979767802e-06\n",
      "Iteration 59: 2.652410785231041e-06\n",
      "Iteration 60: 2.637509624037193e-06\n",
      "Iteration 61: 2.563003363320604e-06\n",
      "Iteration 62: 2.5481022021267563e-06\n",
      "Iteration 63: 2.518299879739061e-06\n",
      "Iteration 64: 2.4437938463961473e-06\n",
      "Iteration 65: 2.4288926852022996e-06\n",
      "Iteration 66: 2.4288926852022996e-06\n",
      "Iteration 67: 2.413991524008452e-06\n",
      "Iteration 68: 2.3245843294716906e-06\n",
      "Iteration 69: 2.3096829409041675e-06\n",
      "Iteration 70: 2.3096829409041675e-06\n",
      "Iteration 71: 2.29478177971032e-06\n",
      "Iteration 72: 2.220275746367406e-06\n",
      "Iteration 73: 2.220275746367406e-06\n",
      "Iteration 74: 2.2053745851735584e-06\n",
      "Iteration 75: 2.1904734239797108e-06\n",
      "Iteration 76: 2.1159676180104725e-06\n",
      "Iteration 77: 2.1010662294429494e-06\n",
      "Iteration 78: 2.1010662294429494e-06\n",
      "Iteration 79: 2.0861648408754263e-06\n",
      "Iteration 80: 2.0265601961000357e-06\n",
      "Iteration 81: 2.011659034906188e-06\n",
      "Iteration 82: 1.9967578737123404e-06\n",
      "Iteration 83: 1.9818567125184927e-06\n",
      "Iteration 84: 1.9818567125184927e-06\n",
      "Iteration 85: 1.966955551324645e-06\n",
      "Iteration 86: 1.9073504518019035e-06\n",
      "Iteration 87: 1.8924494042948936e-06\n",
      "Iteration 88: 1.8924494042948936e-06\n",
      "Iteration 89: 1.877548243101046e-06\n",
      "Iteration 90: 1.877548243101046e-06\n",
      "Iteration 91: 1.7881411622511223e-06\n",
      "Iteration 92: 1.7881411622511223e-06\n",
      "Iteration 93: 1.7881411622511223e-06\n",
      "Iteration 94: 1.7732397736835992e-06\n",
      "Iteration 95: 1.7732397736835992e-06\n",
      "Iteration 96: 1.7583386124897515e-06\n",
      "Iteration 97: 1.698733967714361e-06\n",
      "Iteration 98: 1.698733967714361e-06\n",
      "Iteration 99: 1.6838328065205133e-06\n",
      "------- Checking outputs (left) vs ground truth (right): -----\n",
      "tensor([[1.9073e-06, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.9073e-06, 0.0000e+00]])\n",
      "--------------- LNN Parameters (post-training) ---------------\n",
      "OR (beta, argument weights): 9.477 [19.059 17.79 ]\n",
      "AND1 (beta, argument weights): 7.542 [14.488 15.328]\n",
      "AND2 (beta, argument weights): 7.761 [14.913 15.211]\n"
     ]
    }
   ],
   "source": [
    "#this is a hyperparameter\n",
    "alpha = 0.8\n",
    "\n",
    "op_and1 = and_lukasiewicz(alpha, 2, False)\n",
    "op_and2 = and_lukasiewicz(alpha, 2, False)\n",
    "op_or = or_lukasiewicz(alpha, 2, False)\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam([{'params': op_or.parameters()}, \\\n",
    "                        {'params': op_and1.parameters()}, \\\n",
    "                        {'params': op_and2.parameters()}], lr=0.1)\n",
    "\n",
    "for iter in range(100):\n",
    "    op_or.train()\n",
    "    op_and1.train()\n",
    "    op_and2.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = x[:,0].view(-1,1)\n",
    "    x1 = x[:,1].view(-1,1)\n",
    "    yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#check to see output of xor post-training\n",
    "x0 = x[:,0].view(-1,1)\n",
    "x1 = x[:,1].view(-1,1)\n",
    "yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                        op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "check_values = torch.cat((yhat, y), 1)\n",
    "print(\"------- Checking outputs (left) vs ground truth (right): -----\")\n",
    "print(check_values.detach())\n",
    "\n",
    "#LNN parameters: post-training (we have 3 sets of beta, argument weights)\n",
    "print(\"--------------- LNN Parameters (post-training) ---------------\")\n",
    "beta_or, argument_wts_or = op_or.AND.cdd()\n",
    "beta_and1, argument_wts_and1 = op_and1.cdd()\n",
    "beta_and2, argument_wts_and2 = op_and2.cdd()\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"OR (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_or.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_or.detach().numpy()))\n",
    "print(\"AND1 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and1.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and1.detach().numpy()))\n",
    "print(\"AND2 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and2.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arity should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PureNameLNN(nn.Module):\n",
    "#     def __init__(self, alpha, sim_arity=4, rule_arity=2, slack=None):\n",
    "#         super(PureNameLNN, self).__init__()\n",
    "#         self.threshold = 0.5\n",
    "        \n",
    "#         self.sim_disjunction_or = or_lukasiewicz(alpha, sim_arity, slack)\n",
    "    \n",
    "#     def forward(self, x, mention_labels=None):\n",
    "#         \"\"\"\n",
    "#             x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "#                normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "#         \"\"\"\n",
    "#         yhat = None\n",
    "        \n",
    "#         # RULE 1: lookup predicate\n",
    "#         lookup_features = x[:,5]\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "#         # RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb))\n",
    "#         feature_list = []\n",
    "#         # rule 2 (1) mention==label\n",
    "#         mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "#         labels = np.array([m[1].lower() for m in mention_labels])\n",
    "#         exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float()\n",
    "#         feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "#         # rule 2 (2)-(4) Jaro(m, lb) AND Jacc(m, lb) AND Lev(m, lb))\n",
    "#         sim_features = x[:, 0:3]\n",
    "#         print(sim_features)\n",
    "\n",
    "#         return yhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}