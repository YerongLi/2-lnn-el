{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## form tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_norm.txt\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['purename', 'False', 0.85, 0.845, 0.7584, 0.7993, 0.7378388489499601]\n",
      "['context', 'False', 0.85, 0.877, 0.7896, 0.831, 0.5]\n",
      "['complex', 'False', 0.85, 0.878, 0.7913, 0.8324, 0.991991991991992]\n",
      "['purename', 'False', 0.9, 0.8346, 0.7581, 0.7945, 0.7070707070707072]\n",
      "['context', 'False', 0.9, 0.877, 0.7896, 0.831, 0.5]\n",
      "['complex', 'False', 0.9, 0.8346, 0.7581, 0.7945, 0.7777777777777778]\n",
      "['purename', 'False', 0.95, 0.8346, 0.7581, 0.7945, 0.7474747474747475]\n",
      "['context', 'False', 0.95, 0.876, 0.792, 0.8319, 0.9964964964964965]\n",
      "['complex', 'False', 0.95, 0.878, 0.7913, 0.8324, 0.994994994994995]\n",
      "[['purename', 'False', 0.85, 0.845, 0.7584, 0.7993, 0.7378388489499601], ['context', 'False', 0.85, 0.877, 0.7896, 0.831, 0.5], ['complex', 'False', 0.85, 0.878, 0.7913, 0.8324, 0.991991991991992], ['purename', 'False', 0.9, 0.8346, 0.7581, 0.7945, 0.7070707070707072], ['context', 'False', 0.9, 0.877, 0.7896, 0.831, 0.5], ['complex', 'False', 0.9, 0.8346, 0.7581, 0.7945, 0.7777777777777778], ['purename', 'False', 0.95, 0.8346, 0.7581, 0.7945, 0.7474747474747475], ['context', 'False', 0.95, 0.876, 0.792, 0.8319, 0.9964964964964965], ['complex', 'False', 0.95, 0.878, 0.7913, 0.8324, 0.994994994994995]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "rows = []\n",
    "columns = ['model', 'binary', 'alpha', 'precision', 'recall', 'f1', 'threshold']\n",
    "for line in lines:\n",
    "    items = line.split('; ')\n",
    "    items = [item.split('=')[-1].strip() for item in items]\n",
    "    for i in range(2, 7):\n",
    "        items[i] = float(items[i])\n",
    "    print(items)\n",
    "    rows.append(items)\n",
    "\n",
    "print(rows)\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.to_csv(\"output_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from el_evaluation import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"main training script for training lnn entity linking models\")\n",
    "parser.add_argument(\"--train_data\", type=str, default=\"./data/train.csv\", help=\"train csv\")\n",
    "parser.add_argument(\"--test_data\", type=str, default=\"./data/test.csv\", help=\"test csv\")\n",
    "parser.add_argument(\"--checkpoint_name\", type=str, default=\"checkpoint/best_model.pt\", help=\"checkpoint path\")\n",
    "parser.add_argument(\"--output_file_name\", type=str, default=\"output/purename_nway_alpha095.csv\", help=\"checkpoint path\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"purename\", help=\"which model we choose\")\n",
    "# args for dividing the corpus\n",
    "parser.add_argument('--alpha', type=float, default=0.95, help='alpha for LNN')\n",
    "parser.add_argument('--num_epoch', type=int, default=200, help='training epochs for LNN')\n",
    "parser.add_argument(\"--use_binary\", action=\"store_true\", help=\"default is to use binary`, otherwise use stem\")\n",
    "parser.add_argument(\"-f\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.use_binary = True\n",
    "\n",
    "if args.use_binary:\n",
    "    from RuleLNN_binary import *\n",
    "else:\n",
    "    from RuleLNN_nway import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val = pd.read_csv(args.train_data)\n",
    "df_test = pd.read_csv(args.test_data)\n",
    "\n",
    "# train\n",
    "features_train_val = np.array(\n",
    "    [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train_val.Features.values])\n",
    "X_train_val = torch.from_numpy(features_train_val).float()\n",
    "Y_train_val = torch.from_numpy(df_train_val.Label.values).float()\n",
    "mention_labels_train_val = df_train_val.Mention_label.values\n",
    "questions_train_val = df_train_val.Question.values\n",
    "x_train, x_val, y_train, y_val, m_labels_train, m_labels_val, ques_train, ques_val = \\\n",
    "    train_test_split(X_train_val, Y_train_val, mention_labels_train_val, questions_train_val,\n",
    "                     test_size=0.2, train_size=0.8, random_state=100)\n",
    "\n",
    "# test\n",
    "features_test = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_test.Features.values])\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(df_test.Label.values).float()\n",
    "m_labels_test = df_test.Mention_label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Mention_label</th>\n",
       "      <th>Features</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who was the wife of U.S. president Lincoln?</td>\n",
       "      <td>U.S.;United States</td>\n",
       "      <td>[0.5961538553237915, 0.0, 0.15384615384615385,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Who was the wife of U.S. president Lincoln?</td>\n",
       "      <td>U.S.;National Register of Historic Places</td>\n",
       "      <td>[0.42592594027519226, 0.0, 0.02777777777777779...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Who was the wife of U.S. president Lincoln?</td>\n",
       "      <td>U.S.;United States Census Bureau</td>\n",
       "      <td>[0.5722222328186035, 0.0, 0.07407407407407407,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Who was the wife of U.S. president Lincoln?</td>\n",
       "      <td>U.S.;Hispanic (U.S. Census)</td>\n",
       "      <td>[0.6439393758773804, 0.0, 0.18181818181818177,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Who was the wife of U.S. president Lincoln?</td>\n",
       "      <td>U.S.;United States Navy</td>\n",
       "      <td>[0.5833333134651184, 0.0, 0.11111111111111116,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                     Question  \\\n",
       "0           0  Who was the wife of U.S. president Lincoln?   \n",
       "1           1  Who was the wife of U.S. president Lincoln?   \n",
       "2           2  Who was the wife of U.S. president Lincoln?   \n",
       "3           3  Who was the wife of U.S. president Lincoln?   \n",
       "4           4  Who was the wife of U.S. president Lincoln?   \n",
       "\n",
       "                               Mention_label  \\\n",
       "0                         U.S.;United States   \n",
       "1  U.S.;National Register of Historic Places   \n",
       "2           U.S.;United States Census Bureau   \n",
       "3                U.S.;Hispanic (U.S. Census)   \n",
       "4                    U.S.;United States Navy   \n",
       "\n",
       "                                            Features  Label  \n",
       "0  [0.5961538553237915, 0.0, 0.15384615384615385,...      0  \n",
       "1  [0.42592594027519226, 0.0, 0.02777777777777779...      0  \n",
       "2  [0.5722222328186035, 0.0, 0.07407407407407407,...      0  \n",
       "3  [0.6439393758773804, 0.0, 0.18181818181818177,...      0  \n",
       "4  [0.5833333134651184, 0.0, 0.11111111111111116,...      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df_test.Question.values))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_model(model_name, alpha):\n",
    "    if model_name == \"purename\":\n",
    "        return PureNameLNN(alpha, 2, False)\n",
    "    elif model_name == \"context\":\n",
    "        return ContextLNN(alpha, 2, False)\n",
    "    elif model_name == \"complex\":\n",
    "        return ComplexRuleLNN(alpha, 2, False)\n",
    "    else:\n",
    "        print(\"WRONG name input\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_qald_metrics(val_pred, val_y, val_m_labels, ques_val):\n",
    "    \"\"\"val_pred are 0/1 s after applying a threshold\"\"\"\n",
    "    rows = []\n",
    "    question_rows_map = defaultdict(list)\n",
    "\n",
    "    for i, pred in enumerate(val_pred):\n",
    "        pred = pred.data.tolist()[0]\n",
    "        if pred:\n",
    "            men_entity_label = '_'.join(val_m_labels[i].split(';')[-1].split())\n",
    "            question_rows_map[ques_val[i]].append(('http://dbpedia.org/resource/{}'.format(men_entity_label), 1.0))\n",
    "#             print(ques_val[i], question_rows_map[ques_val[i]])\n",
    "\n",
    "    for key, value in question_rows_map.items():\n",
    "        rows.append([key, [value]])\n",
    "\n",
    "    df_output = pd.DataFrame(rows, columns=['Question', 'Entities'])\n",
    "    df_output['Classes'] = str([])\n",
    "    df_output.head()\n",
    "    \n",
    "    # gold \n",
    "    benchmark = pd.read_csv('../../../data/gt_sparql.csv')\n",
    "    benchmark = benchmark.set_index('Question')\n",
    "    benchmark = benchmark.replace(np.nan, '', regex=True)\n",
    "    benchmark['Entities'] = benchmark['Entities'].astype(object)\n",
    "    is_qald_gt = True\n",
    "    \n",
    "    # pred \n",
    "    predictions = df_output\n",
    "    predictions = predictions.set_index('Question')\n",
    "    predictions['Entities'] = predictions['Entities']\n",
    "    predictions['Classes'] = predictions['Classes']\n",
    "\n",
    "    metrics = compute_metrics(benchmark=benchmark, predictions=predictions, limit=410, is_qald_gt=is_qald_gt, eval='full')\n",
    "\n",
    "    scores = metrics['macro']['named']\n",
    "    prec, recall, f1 = scores['precision'], scores['recall'], scores['f1']\n",
    "    print(prec, recall, f1)\n",
    "    return prec, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval, ques_eval, loss_fn, threshold=0.5):\n",
    "    \"\"\"evaluate a model on validation data\"\"\"\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > threshold\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1 = get_qald_metrics(val_pred_, y_eval, m_labels_eval, ques_eval)\n",
    "#         prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 is {} w/ threshold {} \".format(f1, threshold))\n",
    "#     return loss, f1, val_pred\n",
    "\n",
    "    \n",
    "    return loss, f1, val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0571)\n",
      "0.4792 0.5417 0.5085\n",
      "f1 is 0.5085 w/ threshold 0.5 \n"
     ]
    }
   ],
   "source": [
    "model = pick_model(args.model_name, args.alpha)\n",
    "loss_fn = nn.BCELoss()\n",
    "loss, f1, val_pred = evaluate(model, x_val, y_val, m_labels_val, ques_val, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5 0.5652 0.5306\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5111 0.5778 0.5424\n",
      "0.5227 0.5909 0.5547\n",
      "0.5227 0.5909 0.5547\n",
      "0.5227 0.5909 0.5547\n",
      "0.5227 0.5909 0.5547\n",
      "0.5227 0.5909 0.5547\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5349 0.6047 0.5676\n",
      "0.5476 0.619 0.5811\n",
      "0.5476 0.619 0.5811\n",
      "0.5476 0.619 0.5811\n",
      "0.5476 0.619 0.5811\n",
      "0.5476 0.619 0.5811\n",
      "0.575 0.65 0.6102\n",
      "0.575 0.65 0.6102\n",
      "0.575 0.65 0.6102\n",
      "0.575 0.65 0.6102\n",
      "0.575 0.65 0.6102\n",
      "0.575 0.65 0.6102\n",
      "0.575 0.65 0.6102\n",
      "0.6154 0.6923 0.6516\n",
      "0.6154 0.6923 0.6516\n",
      "0.6154 0.6923 0.6516\n",
      "0.6154 0.6923 0.6516\n",
      "0.6053 0.6842 0.6423\n",
      "0.6053 0.6842 0.6423\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "Val -- After tuning, the best f1 is 0.6516 w/ threshold 0.9987878787878788\n"
     ]
    }
   ],
   "source": [
    "best_pred = val_pred\n",
    "best_val_f1, best_val_loss = 0, 1000\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "for threshold_ in np.linspace(0.99, 1.0, num=100):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1 = get_qald_metrics(y_val_preds, y_val, m_labels_val, ques_val)\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0524)\n",
      "0.8058 0.7326 0.7674\n",
      "f1 is 0.7674 w/ threshold 0.9994949494949495 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0524),\n",
       " 0.7674,\n",
       " tensor([[9.9950e-01],\n",
       "         [5.2919e-14],\n",
       "         [2.0628e-04],\n",
       "         ...,\n",
       "         [2.0741e-10],\n",
       "         [5.3063e-20],\n",
       "         [2.0741e-10]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_test = df_test.Question\n",
    "\n",
    "evaluate(model, x_test, y_test, m_labels_test, ques_test, loss_fn, threshold=.9994949494949495)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model and evaluate\n",
    "# print(model(x_train, m_labels_train))\n",
    "# best_tuned_threshold = train(model, x_train, y_train, m_labels_train, ques_train, x_val, y_val, m_labels_val, ques_val, args.checkpoint_name, args.num_epoch)\n",
    "# test_pred = test(x_test, y_test, m_labels_test, best_tuned_threshold, args.alpha, args.checkpoint_name, args.model_name)\n",
    "# write_output(df_test, m_labels_test, test_pred, args.output_file_name)\n",
    "\n",
    "# print(args.use_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from el_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_file_path = '../../../data/gt_sparql.csv'\n",
    "prediction_file = 'output/purename_nway_alpha095.csv' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(gold_file_path, prediction_file):\n",
    "    \n",
    "    # gold \n",
    "    benchmark = pd.read_csv(gold_file_path)\n",
    "    benchmark = benchmark.set_index('Question')\n",
    "    benchmark = benchmark.replace(np.nan, '', regex=True)\n",
    "    benchmark['Entities'] = benchmark['Entities'].astype(object)\n",
    "    is_qald_gt = True\n",
    "    \n",
    "    # pred \n",
    "    predictions = pd.read_csv(prediction_file)\n",
    "    predictions = predictions.set_index('Question')\n",
    "    predictions['Entities'] = predictions['Entities']\n",
    "    predictions['Classes'] = predictions['Classes']\n",
    "\n",
    "    metrics = compute_metrics(benchmark=benchmark, predictions=predictions, limit=410, is_qald_gt=is_qald_gt, eval='full')\n",
    "\n",
    "    scores = metrics['macro']['named']\n",
    "    prec, recall, f1 = scores['precision'], scores['recall'], scores['f1']\n",
    "    print(prec, recall, f1)\n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== output/complex_binary_alpha085.csv =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gold_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d7698bbe9c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprediction_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=====\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"=====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gold_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "columns = ['filename', 'alpha', 'precision', 'recall', 'f1']\n",
    "for prediction_file in sorted(glob.glob(\"output/*.csv\")):\n",
    "    print(\"=====\", prediction_file,\"=====\")\n",
    "    prec, rec, f1 = eval(gold_file_path, prediction_file)\n",
    "    filename = prediction_file.split('/')[-1][:-4]\n",
    "    alpha = filename[-2:]\n",
    "    print(filename, alpha, prec, rec, f1)\n",
    "    rows.append([filename, alpha, prec, rec, f1])\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.to_csv('eval_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
