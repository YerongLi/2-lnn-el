{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from el_evaluation import *\n",
    "from utils import MyBatchSampler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from RuleLNN_nway import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qald_metrics(pred_, m_labels_, ques_, mode='val'):\n",
    "    \"\"\"pred_ are 0/1 s after applying a threshold\"\"\"\n",
    "    rows = []\n",
    "    question_rows_map = {}\n",
    "    question_mention_set = set()\n",
    "    for i, pred in enumerate(pred_):\n",
    "        pred = pred.data.tolist()[0]\n",
    "        question = ques_[i]\n",
    "        if question not in question_rows_map:\n",
    "            question_rows_map[ques_[i]] = []\n",
    "        if pred:\n",
    "            men_entity_label = '_'.join(m_labels_[i].split(';')[-1].split())\n",
    "            men_entity_mention = '_'.join(m_labels_[i].split(';')[0].split())\n",
    "            if '-'.join([question, men_entity_mention]) in question_mention_set:\n",
    "                question_rows_map[ques_[i]][-1].add(\n",
    "                    ('http://dbpedia.org/resource/{}'.format(men_entity_label), pred))\n",
    "            else:\n",
    "                question_mention_set.add('-'.join([question, men_entity_mention]))\n",
    "                question_rows_map[ques_[i]].append(set())\n",
    "                question_rows_map[ques_[i]][-1].add(\n",
    "                    ('http://dbpedia.org/resource/{}'.format(men_entity_label), pred))\n",
    "    for key, preds_list_mentions in question_rows_map.items():\n",
    "        if len(preds_list_mentions) > 1:\n",
    "            rows.append([key, []])\n",
    "            for preds_set in preds_list_mentions:\n",
    "                sorted_values = sorted(list(preds_set), key=lambda x: x[1], reverse=True)[:5]\n",
    "                rows[-1][1].append(sorted_values)\n",
    "        elif len(preds_list_mentions) == 1:\n",
    "            sorted_values = sorted(list(preds_list_mentions[0]), key=lambda x: x[1], reverse=True)[:5]\n",
    "            rows.append([key, [sorted_values]])\n",
    "        else:\n",
    "            rows.append([key, []])\n",
    "\n",
    "    df_output = pd.DataFrame(rows, columns=['Question', 'Entities'])\n",
    "    df_output['Classes'] = str([])\n",
    "\n",
    "    # generate the csv\n",
    "    if mode == 'test':\n",
    "        df_missing = pd.read_csv(\"data/missing.csv\", header=None)\n",
    "        df_missing.columns = ['Unnamed:0', 'Question', 'Entities', 'Classes']\n",
    "        df_missing = df_missing[['Question', 'Entities', 'Classes']]\n",
    "        df_output = df_output[['Question', 'Entities', 'Classes']]\n",
    "        df_output = pd.concat([df_output, df_missing], ignore_index=True)\n",
    "        print(\"df_output\", df_output.shape)\n",
    "\n",
    "    # gold\n",
    "    benchmark = pd.read_csv('../../../data/gt_sparql.csv')\n",
    "    benchmark = benchmark.set_index('Question')\n",
    "    benchmark = benchmark.replace(np.nan, '', regex=True)\n",
    "    benchmark['Entities'] = benchmark['Entities'].astype(object)\n",
    "    is_qald_gt = True\n",
    "\n",
    "    # pred\n",
    "    predictions = df_output\n",
    "    # print(df_output.shape)\n",
    "    predictions = predictions.set_index('Question')\n",
    "    predictions['Entities'] = predictions['Entities']\n",
    "    predictions['Classes'] = predictions['Classes']\n",
    "\n",
    "    metrics = compute_metrics(benchmark=benchmark, predictions=predictions, limit=410, is_qald_gt=is_qald_gt, eval='full')\n",
    "\n",
    "    scores = metrics['macro']['named']\n",
    "    prec, recall, f1 = scores['precision'], scores['recall'], scores['f1']\n",
    "    return prec, recall, f1, df_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_test, m_labels_test, ques_test, best_tuned_threshold, alpha, checkpoint_name, model_name, output_file_name):\n",
    "    \"\"\"make predictions on test set\"\"\"\n",
    "    bestModel = pick_model(model_name, alpha)\n",
    "    bestModel.load_state_dict(torch.load(checkpoint_name))\n",
    "    bestModel.eval()\n",
    "    best_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pred = bestModel(x_test, m_labels_test)\n",
    "        prec, recall, f1, df_output = get_qald_metrics(test_pred, m_labels_test, ques_test, mode='test')\n",
    "        df_output.to_csv(output_file_name)\n",
    "        print(\"Test -- f1 is {} \".format(f1))\n",
    "        print(\"Test -- prec, recall, f1\", prec, recall, f1)\n",
    "        best_scores['precision'] = prec\n",
    "        best_scores['recall'] = recall\n",
    "        best_scores['f1'] = f1\n",
    "\n",
    "    # for name, mod in bestModel.named_modules():\n",
    "    #     if type(mod) == nn.ModuleList:\n",
    "    #         for name1, mod1 in mod.named_modules():\n",
    "    #             if 'cdd' not in name1 and 'AND' not in name1:\n",
    "    #                 if 'batch' in name1.lower():\n",
    "    #                     continue\n",
    "    #                 elif 'or_max' in name1.lower():\n",
    "    #                     continue\n",
    "    #                 elif 'and' in name1.lower():\n",
    "    #                     print(name1, mod1.cdd())\n",
    "    #                 elif 'or' in name1.lower():\n",
    "    #                     print(name1, mod1.AND.cdd())\n",
    "    #     else:\n",
    "    #         if 'cdd' not in name and 'AND' not in name:\n",
    "    #             if 'batch' in name.lower():\n",
    "    #                 continue\n",
    "    #             elif 'or_max' in name.lower():\n",
    "    #                 continue\n",
    "    #             elif 'and' in name.lower():\n",
    "    #                 print(name, mod.cdd())\n",
    "    #             elif 'or' in name.lower():\n",
    "    #                 print(name, mod.AND.cdd())\n",
    "    return test_pred, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, x_, y_, m_labels_, ques_, loss_fn):\n",
    "    \"\"\"evaluate a model on validation data\"\"\"\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_ = eval_model(x_, m_labels_)\n",
    "        loss = loss_fn(pred_, y_)\n",
    "        prec, recall, f1, _ = get_qald_metrics(pred_, m_labels_, ques_, mode='val') # train and val both use 'val' mode\n",
    "\n",
    "    return loss, f1, pred_\n",
    "\n",
    "\n",
    "def pick_model(model_name, alpha):\n",
    "    if model_name == \"purename\":\n",
    "        return PureNameLNN(alpha, -1, False)\n",
    "    elif model_name == \"context\":\n",
    "        return ContextLNN(alpha, -1, False)\n",
    "    elif model_name == \"complex\":\n",
    "        return ComplexRuleLNN(alpha, -1, False)\n",
    "    elif model_name == \"lr\":\n",
    "        return LogitsRegression()\n",
    "    else:\n",
    "        print(\"WRONG name input\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 6]) torch.Size([23, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, Sampler\n",
    "import numpy as np\n",
    "class QuestionSampler(Sampler):\n",
    "    r\"\"\"Samples elements sequentially, always in the same order.\n",
    "\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler, labels, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.labels = labels\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(idx)\n",
    "            if self.labels[idx] == 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "\n",
    "# for idxes in QuestionSampler(torch.utils.data.SequentialSampler(range(len(y_train))), y_train, False):\n",
    "#     print(y_train[idxes])\n",
    "# list(iter([2,2,3]))\n",
    "\n",
    "loader = DataLoader(dataset_train, batch_sampler=QuestionSampler(torch.utils.data.SequentialSampler(range(len(y_train))), y_train, False), shuffle=False)\n",
    "for batch in loader:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, checkpoint_name, num_epochs):\n",
    "    \"\"\"train model and tune on validation set\"\"\"\n",
    "    \n",
    "    # unwrapping the data\n",
    "    x_train, y_train, m_labels_train, ques_train = train_data\n",
    "    x_val, y_val, m_labels_val, ques_val = val_data\n",
    "    x_test, y_test, m_labels_test, ques_test = test_data\n",
    "    \n",
    "    \n",
    "    # inialize the loss function and optimizer    \n",
    "    loss_fn = nn.BCELoss()  # MSELoss(), did not work neither\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    best_pred = None\n",
    "    best_val_f1, best_val_loss = 0, 1000\n",
    "    batch_size = 256\n",
    "\n",
    "    # stats before training\n",
    "    print(\"=========BEFORE TRAINING============\")\n",
    "    train_loss, train_f1, train_pred = evaluate(model, x_train, y_train, m_labels_train, ques_train, loss_fn)\n",
    "    print(\"Train -- loss is {}; F1 is {}\".format(train_loss, train_f1))\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val, ques_val, loss_fn)\n",
    "    print(\"Val --  loss is {}; F1 is {}\".format(val_loss, val_f1))\n",
    "    test_loss, test_f1, test_pred = evaluate(model, x_test, y_test, m_labels_test, ques_test, loss_fn)\n",
    "    print(\"Test -- loss is {}; F1 is {}\".format(test_loss, test_f1))\n",
    "    \n",
    "    # start training\n",
    "    print(\"=========TRAINING============\")\n",
    "    dataset_train = TensorDataset(x_train, y_train)\n",
    "    loader = DataLoader(dataset_train, sampler=MyBatchSampler(y_train), batch_size=256, shuffle=False)  # always False\n",
    "#     loader = DataLoader(dataset_train, sampler=torch.utils.data.SequentialSampler(dataset_train), batch_size=batch_size, shuffle=False)  # always False\n",
    "    # loader = DataLoader(dataset_train, sampler=torch.utils.data.WeightedRandomSampler(torch.FloatTensor([1, 100]), len(x_train), replacement=True), batch_size=64, shuffle=False)  # always False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            model.train()  # set train to true\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(xb, yb)\n",
    "            yb = yb.reshape(-1, 1)\n",
    "#             print('yb', yb.shape)\n",
    "#             print('yhat', yhat.shape)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            total_loss += loss.item()*batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(name, 'param -- data', param.data, 'grad -- ', param.grad)\n",
    "        \n",
    "        # show status after each epoch\n",
    "        avg_loss = total_loss / (len(loader)*batch_size)\n",
    "        train_loss, train_f1, train_pred = evaluate(model, x_train, y_train, m_labels_train, ques_train, loss_fn)\n",
    "        print(\"Epoch \" + str(epoch) + \": avg train loss -- \" + str(avg_loss))\n",
    "        print(\"Train -- loss is {}; F1 is {}\".format(train_loss, train_f1))\n",
    "        val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val, ques_val, loss_fn)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_f1 = val_f1\n",
    "            best_pred = val_pred\n",
    "            torch.save(model.state_dict(), checkpoint_name)\n",
    "        print(\"Val --  best loss is {}; F1 is {}\".format(best_val_loss, best_val_f1))\n",
    "    \n",
    "    # show stats after training\n",
    "    print(\"=========AFTER TRAINING============\")\n",
    "    train_loss, train_f1, train_pred = evaluate(model, x_train, y_train, m_labels_train, ques_train, loss_fn)\n",
    "    print(\"Train -- loss is {}; F1 is {}\".format(train_loss, train_f1))\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val, ques_val, loss_fn)\n",
    "    print(\"Val --  loss is {}; F1 is {}\".format(val_loss, val_f1))\n",
    "    test_loss, test_f1, test_pred = evaluate(model, x_test, y_test, m_labels_test, ques_test, loss_fn)\n",
    "    print(\"Test -- loss is {}; F1 is {}\".format(test_loss, test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  complex 0.85\n",
      "tensor([[9.9432e-01],\n",
      "        [4.2742e-04],\n",
      "        [4.3708e-04],\n",
      "        ...,\n",
      "        [4.2063e-04],\n",
      "        [4.2099e-04],\n",
      "        [4.2099e-04]], grad_fn=<RsubBackward1>)\n",
      "torch.Size([25078, 6]) torch.Size([4991, 6])\n",
      "=========BEFORE TRAINING============\n",
      "Train -- loss is 1.002746343612671; F1 is 0.7086\n",
      "Val --  loss is 0.6703747510910034; F1 is 0.7312\n",
      "Test -- loss is 0.7552366256713867; F1 is 0.754\n",
      "=========TRAINING============\n",
      "Epoch 0: avg train loss -- 0.47536606574669865\n",
      "Train -- loss is 0.24638526141643524; F1 is 0.7569\n",
      "Val --  best loss is 0.2525876760482788; F1 is 0.7933\n",
      "Epoch 1: avg train loss -- 0.45008327762285866\n",
      "Train -- loss is 0.22747252881526947; F1 is 0.7583\n",
      "Val --  best loss is 0.23762181401252747; F1 is 0.7748\n",
      "Epoch 2: avg train loss -- 0.4456175231016599\n",
      "Train -- loss is 0.20541809499263763; F1 is 0.7613\n",
      "Val --  best loss is 0.21387669444084167; F1 is 0.7748\n",
      "Epoch 3: avg train loss -- 0.45831723763392523\n",
      "Train -- loss is 0.12779732048511505; F1 is 0.7657\n",
      "Val --  best loss is 0.11590004712343216; F1 is 0.7933\n",
      "Epoch 4: avg train loss -- 0.4657852766605524\n",
      "Train -- loss is 0.11324743926525116; F1 is 0.7673\n",
      "Val --  best loss is 0.10767009109258652; F1 is 0.7933\n",
      "Epoch 5: avg train loss -- 0.45526094604761175\n",
      "Train -- loss is 0.09490149468183517; F1 is 0.7673\n",
      "Val --  best loss is 0.08838352560997009; F1 is 0.7933\n",
      "Epoch 6: avg train loss -- 0.4523284284732281\n",
      "Train -- loss is 0.08476845175027847; F1 is 0.7689\n",
      "Val --  best loss is 0.07377070188522339; F1 is 0.7933\n",
      "Epoch 7: avg train loss -- 0.4411929414058343\n",
      "Train -- loss is 0.07628698647022247; F1 is 0.7673\n",
      "Val --  best loss is 0.06822923570871353; F1 is 0.7933\n",
      "Epoch 8: avg train loss -- 0.43131293478684546\n",
      "Train -- loss is 0.07172183692455292; F1 is 0.7689\n",
      "Val --  best loss is 0.06530346721410751; F1 is 0.7933\n",
      "Epoch 9: avg train loss -- 0.42396351817326666\n",
      "Train -- loss is 0.07002908736467361; F1 is 0.7689\n",
      "Val --  best loss is 0.06358910351991653; F1 is 0.7933\n",
      "=========AFTER TRAINING============\n",
      "Train -- loss is 0.07002908736467361; F1 is 0.7689\n",
      "Val --  loss is 0.06358910351991653; F1 is 0.7933\n",
      "Test -- loss is 0.06937964260578156; F1 is 0.7888\n"
     ]
    }
   ],
   "source": [
    "args.num_epoch = 10\n",
    "args.alpha = 0.85\n",
    "args.model_name = \"complex\"\n",
    "\n",
    "# train model and evaluate\n",
    "model = pick_model(args.model_name, args.alpha)\n",
    "print(\"model: \", args.model_name, args.alpha)\n",
    "print(model(x_train, m_labels_train))\n",
    "print(x_train.shape, x_val.shape)\n",
    "\n",
    "best_tuned_threshold = train(model, train_data, val_data, test_data, args.checkpoint_name, args.num_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_output (157, 3)\n",
      "Test -- f1 is 0.7834 \n",
      "Test -- prec, recall, f1 0.8047 0.7633 0.7834\n",
      "model=complex; use_binary=False; alpha=0.85; p=0.8047; r=0.7633; f1=0.7834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred, best_scores = test(x_test, m_labels_test, ques_test, best_tuned_threshold, args.alpha, args.checkpoint_name, args.model_name, args.output_file_name)\n",
    "with open(\"output_w_spacy.txt\", 'a') as f:\n",
    "    f.write(\"model={}; use_binary={}; alpha={}; p={}; r={}; f1={}\\n\".format(args.model_name, args.use_binary, args.alpha,\n",
    "                                                                best_scores['precision'], best_scores['recall'],\n",
    "                                                                best_scores['f1']))\n",
    "    print(\"model={}; use_binary={}; alpha={}; p={}; r={}; f1={}\\n\".format(args.model_name, args.use_binary, args.alpha,\n",
    "                                                                best_scores['precision'], best_scores['recall'],\n",
    "                                                                best_scores['f1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"main training script for training lnn entity linking models\")\n",
    "parser.add_argument(\"--train_data\", type=str, default=\"./data/train.csv\", help=\"train csv\")\n",
    "parser.add_argument(\"--test_data\", type=str, default=\"./data/test.csv\", help=\"test csv\")\n",
    "parser.add_argument(\"--checkpoint_name\", type=str, default=\"checkpoint/best_model.pt\", help=\"checkpoint path\")\n",
    "parser.add_argument(\"--output_file_name\", type=str, default=\"output/purename_nway_alpha09.txt\", help=\"checkpoint path\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"purename\", help=\"which model we choose\")\n",
    "# args for dividing the corpus\n",
    "parser.add_argument('--alpha', type=float, default=0.9, help='alpha for LNN')\n",
    "parser.add_argument('--num_epoch', type=int, default=50, help='training epochs for LNN')\n",
    "parser.add_argument(\"--use_binary\", action=\"store_true\", help=\"default is to use binary`, otherwise use stem\")\n",
    "parser.add_argument(\"-f\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "df_train_val = pd.read_csv(args.train_data)\n",
    "train_ques_set = list(df_train_val.Question.unique())[:166]\n",
    "val_ques_set = list(df_train_val.Question.unique())[166:]\n",
    "df_train = df_train_val[df_train_val.Question.isin(train_ques_set)]\n",
    "df_val = df_train_val[df_train_val.Question.isin(val_ques_set)]\n",
    "df_test = pd.read_csv(args.test_data)\n",
    "\n",
    "# train\n",
    "features_train = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train.Features.values])\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(df_train.Label.values).float().reshape(-1, 1)\n",
    "m_labels_train = df_train.Mention_label.values\n",
    "ques_train = df_train.Question.values\n",
    "\n",
    "# val\n",
    "features_val = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_val.Features.values])\n",
    "x_val = torch.from_numpy(features_val).float()\n",
    "y_val = torch.from_numpy(df_val.Label.values).float().reshape(-1, 1)\n",
    "m_labels_val = df_val.Mention_label.values\n",
    "ques_val = df_val.Question.values\n",
    "\n",
    "# train\n",
    "# features_train_val = np.array(\n",
    "#     [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train_val.Features.values])\n",
    "# X_train_val = torch.from_numpy(features_train_val).float()\n",
    "# Y_train_val = torch.from_numpy(df_train_val.Label.values).float().reshape(-1, 1)\n",
    "# mention_labels_train_val = df_train_val.Mention_label.values\n",
    "# questions_train_val = df_train_val.Question.values\n",
    "# x_train, x_val, y_train, y_val, m_labels_train, m_labels_val, ques_train, ques_val = \\\n",
    "#     train_test_split(X_train_val, Y_train_val, mention_labels_train_val, questions_train_val,\n",
    "#                      test_size=0.2, train_size=0.8, random_state=200, stratify=Y_train_val)\n",
    "\n",
    "\n",
    "# test\n",
    "features_test = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_test.Features.values])\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(df_test.Label.values).float().reshape(-1, 1)\n",
    "m_labels_test = df_test.Mention_label.values\n",
    "ques_test = df_test.Question.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: torch.Size([25078, 6]) torch.Size([25078, 1]) (25078,) (25078,)\n",
      "val: torch.Size([4991, 6]) torch.Size([4991, 1]) (4991,) (4991,)\n",
      "test: torch.Size([17808, 6]) torch.Size([17808, 1]) (17808,) (17808,)\n"
     ]
    }
   ],
   "source": [
    "train_data = (x_train, y_train, m_labels_train, ques_train)\n",
    "print(\"train:\", x_train.shape, y_train.shape, m_labels_train.shape, ques_train.shape)\n",
    "val_data = (x_val, y_val, m_labels_val, ques_val)\n",
    "print(\"val:\", x_val.shape, y_val.shape, m_labels_val.shape, ques_val.shape)\n",
    "test_data = (x_test, y_test, m_labels_test, ques_test)\n",
    "print(\"test:\", x_test.shape, y_test.shape, m_labels_test.shape, ques_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train sum tensor([175.]) tensor([0.0070])\n",
      "y_val sum tensor([38.]) tensor([0.0076])\n",
      "y_test sum tensor([162.]) tensor([0.0091])\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "print(\"y_train sum\", sum(y_train), sum(y_train)/len(y_train))\n",
    "print(\"y_val sum\", sum(y_val), sum(y_val)/len(y_val))\n",
    "# print(\"TRAIN_VAL\", sum(Y_train_val), sum(Y_train_val)/len(Y_train_val))\n",
    "print(\"y_test sum\", sum(y_test), sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.0722891566265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ques_train) / len(set(ques_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# my summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
