{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/meta_rule/')\n",
    "\n",
    "from lnn_operators import and_lukasiewicz, or_lukasiewicz, negation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "# df = df.loc[22:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df.Features.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5158, 0.0000, 0.1579, 0.4390, 1.0000, 1.0000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4980, 0.8569, 0.4000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4292, 0.8430, 0.5000],\n",
      "        ...,\n",
      "        [0.5225, 0.2308, 0.2564, 0.7010, 0.2003, 0.7100],\n",
      "        [0.5743, 0.5000, 0.4082, 0.8074, 0.2003, 0.8200],\n",
      "        [0.9379, 0.6000, 0.6897, 0.7939, 0.1000, 0.8200]])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "['GMT;Greenwich Mean Time' 'GMT;UTC+02:00' 'GMT;UTC+08:00' ...\n",
      " 'Houses Of Parliament;List of people who have spoken to both Houses of the United Kingdom Parliament'\n",
      " 'Houses Of Parliament;Statue of Margaret Thatcher, Houses of Parliament'\n",
      " 'Houses Of Parliament;Houses of Parliament Act 1837']\n"
     ]
    }
   ],
   "source": [
    "#to train a xor we need its truth table\n",
    "X = torch.from_numpy(features).float()\n",
    "print(X)\n",
    "#the target values for each row in the truth table (xor)\n",
    "Y = torch.from_numpy(df.Label.values).float()\n",
    "print(Y)\n",
    "# mention_labels (cannot convert string explicitly)\n",
    "mention_labels = df.Mention_label.values\n",
    "print(mention_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26755,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, m_labels_train, m_labels_test = train_test_split(X, Y, mention_labels, test_size=0.2,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8480, 0.2500, 0.2400, 0.7001, 0.1076, 1.0000],\n",
       "        [0.4423, 0.0000, 0.0769, 0.3746, 0.2174, 0.7500],\n",
       "        [0.9059, 0.0000, 0.5294, 0.3579, 0.1502, 1.0000],\n",
       "        ...,\n",
       "        [0.5569, 0.0000, 0.2051, 0.4970, 0.4131, 0.5000],\n",
       "        [0.6450, 0.0000, 0.1081, 0.3771, 0.3007, 0.6000],\n",
       "        [0.6111, 0.0000, 0.3333, 0.6454, 0.2704, 0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([21404, 6])\n",
      "val torch.Size([5351, 6])\n"
     ]
    }
   ],
   "source": [
    "print('train', x_train.shape)\n",
    "print('val', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureNameLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack=None):\n",
    "        super(PureNameLNN, self).__init__()\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.sim_disjunction_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(4)])\n",
    "        self.predicate_and = and_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x, mention_labels=None):\n",
    "        \"\"\"\n",
    "            x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "               normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "        \"\"\"\n",
    "        yhat = None\n",
    "        \n",
    "        ####### RULE 1: lookup predicate #######\n",
    "        lookup_features = x[:,4].view(-1, 1)\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "        \n",
    "        ####### RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb)) #######\n",
    "        feature_list = []\n",
    "        # rule 2 (1) mention==label\n",
    "        mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "        labels = np.array([m[1].lower() for m in mention_labels])\n",
    "        exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float().view(-1,1)\n",
    "        feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "        # rule 2 (2) Jacc(mention, label)\n",
    "        jacc_features = x[:, 1].view(-1,1)\n",
    "#         jacc_features = torch.clamp(jacc_features, min=self.threshold, max=1.0)\n",
    "        jacc_features_ = torch.where(jacc_features>=self.threshold, jacc_features, torch.zeros_like(jacc_features))\n",
    "        feature_list.append(jacc_features_)\n",
    "#         print(\"jacc_features\", jacc_features)\n",
    "#         print(\"jacc_features*mask\", jacc_features_)\n",
    "        \n",
    "        # rule 2 (3) Lev(mention, label)\n",
    "        lev_features = x[:, 2].view(-1,1)\n",
    "        lev_features_ = torch.where(lev_features>=self.threshold, lev_features, torch.zeros_like(lev_features))\n",
    "        feature_list.append(lev_features_)\n",
    "#         print(\"lev_features\", lev_features)\n",
    "#         print(\"lev_features*mask\", lev_features_)\n",
    "        \n",
    "        # rule 2 (4) Jaro(mention, label)\n",
    "        jaro_features = x[:, 0].view(-1,1)\n",
    "        jaro_features_ = torch.where(jaro_features>=self.threshold, jaro_features, torch.zeros_like(jaro_features))\n",
    "        feature_list.append(jaro_features_)\n",
    "#         print(\"jaro_features\", jaro_features)\n",
    "#         print(\"jaro_features*mask\", jaro_features_)\n",
    "        \n",
    "        # disjunction of (1) to (4)\n",
    "        disjunction_result = feature_list[0]\n",
    "        for i in range(0, 3):\n",
    "            disjunction_result = self.sim_disjunction_or_ops[i](torch.cat((disjunction_result, feature_list[i+1]), 1))\n",
    "#             print(\"disjunction_result\", disjunction_result)\n",
    "        \n",
    "        # RULE 1 + RULE 2\n",
    "        yhat = self.predicate_and(torch.cat((lookup_features, disjunction_result), 1))\n",
    "#         print('yhat', yhat)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1936e-04],\n",
      "        [7.5843e-04],\n",
      "        [3.6290e-04],\n",
      "        ...,\n",
      "        [4.9775e-06],\n",
      "        [4.8795e-04],\n",
      "        [6.4969e-04]], grad_fn=<SWhereBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = PureNameLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_test, y_test, m_labels_test):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = eval_model(x_test, m_labels_test)\n",
    "        loss = loss_fn(test_pred, y_test)\n",
    "        test_pred_ = test_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, test_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([21404])) that is different to the input size (torch.Size([21404, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([5351])) that is different to the input size (torch.Size([5351, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.31788986921310425\n",
      "val loss tensor(0.1379)\n",
      "f1 0.6267516883007955\n",
      "Iteration 1: 0.29160672426223755\n",
      "val loss tensor(0.1152)\n",
      "f1 0.6425400850238854\n",
      "Iteration 2: 0.2677815556526184\n",
      "val loss tensor(0.0993)\n",
      "f1 0.6619434842645824\n",
      "Iteration 3: 0.24897222220897675\n",
      "val loss tensor(0.0876)\n",
      "f1 0.679162468121619\n",
      "Iteration 4: 0.2356056421995163\n",
      "val loss tensor(0.0803)\n",
      "f1 0.6903493737964728\n",
      "Iteration 5: 0.2250492423772812\n",
      "val loss tensor(0.0744)\n",
      "f1 0.6985336672659015\n",
      "Iteration 6: 0.21670205891132355\n",
      "val loss tensor(0.0694)\n",
      "f1 0.707385029876227\n",
      "Iteration 7: 0.20997031033039093\n",
      "val loss tensor(0.0642)\n",
      "f1 0.7153339942863262\n",
      "Iteration 8: 0.2028883397579193\n",
      "val loss tensor(0.0588)\n",
      "f1 0.7349749657805633\n",
      "Iteration 9: 0.19772924482822418\n",
      "val loss tensor(0.0555)\n",
      "f1 0.7471649971649972\n",
      "Iteration 10: 0.19393067061901093\n",
      "val loss tensor(0.0533)\n",
      "f1 0.7514973032527512\n",
      "Iteration 11: 0.19061918556690216\n",
      "val loss tensor(0.0522)\n",
      "f1 0.753717767038504\n",
      "Iteration 12: 0.18781499564647675\n",
      "val loss tensor(0.0510)\n",
      "f1 0.7559756868212408\n",
      "Iteration 13: 0.18535678088665009\n",
      "val loss tensor(0.0496)\n",
      "f1 0.7582720399505547\n",
      "Iteration 14: 0.18238089978694916\n",
      "val loss tensor(0.0477)\n",
      "f1 0.7703670008239494\n",
      "Iteration 15: 0.1804226189851761\n",
      "val loss tensor(0.0470)\n",
      "f1 0.775512344513666\n",
      "Iteration 16: 0.17828866839408875\n",
      "val loss tensor(0.0464)\n",
      "f1 0.7781558044042993\n",
      "Iteration 17: 0.17599114775657654\n",
      "val loss tensor(0.0459)\n",
      "f1 0.7739728674332808\n",
      "Iteration 18: 0.17381355166435242\n",
      "val loss tensor(0.0456)\n",
      "f1 0.7739728674332808\n",
      "Iteration 19: 0.1717986762523651\n",
      "val loss tensor(0.0453)\n",
      "f1 0.7739728674332808\n",
      "Iteration 20: 0.16993595659732819\n",
      "val loss tensor(0.0452)\n",
      "f1 0.7739728674332808\n",
      "Iteration 21: 0.16809239983558655\n",
      "val loss tensor(0.0452)\n",
      "f1 0.7739728674332808\n",
      "Iteration 22: 0.16618892550468445\n",
      "val loss tensor(0.0448)\n",
      "f1 0.7739728674332808\n",
      "Iteration 23: 0.1642005443572998\n",
      "val loss tensor(0.0446)\n",
      "f1 0.7739728674332808\n",
      "Iteration 24: 0.16240538656711578\n",
      "val loss tensor(0.0444)\n",
      "f1 0.7739728674332808\n",
      "Iteration 25: 0.1606759876012802\n",
      "val loss tensor(0.0440)\n",
      "f1 0.7713198999978632\n",
      "Iteration 26: 0.15897409617900848\n",
      "val loss tensor(0.0438)\n",
      "f1 0.7713198999978632\n",
      "Iteration 27: 0.15733744204044342\n",
      "val loss tensor(0.0437)\n",
      "f1 0.7687156258683121\n",
      "Iteration 28: 0.15577000379562378\n",
      "val loss tensor(0.0436)\n",
      "f1 0.7687156258683121\n",
      "Iteration 29: 0.15425224602222443\n",
      "val loss tensor(0.0435)\n",
      "f1 0.7661586922017354\n",
      "Iteration 30: 0.1527043879032135\n",
      "val loss tensor(0.0429)\n",
      "f1 0.7587591368159776\n",
      "Iteration 31: 0.15122218430042267\n",
      "val loss tensor(0.0428)\n",
      "f1 0.7611816808569074\n",
      "Iteration 32: 0.14989255368709564\n",
      "val loss tensor(0.0429)\n",
      "f1 0.7611816808569074\n",
      "Iteration 33: 0.14868074655532837\n",
      "val loss tensor(0.0429)\n",
      "f1 0.7611816808569074\n",
      "Iteration 34: 0.1473531723022461\n",
      "val loss tensor(0.0429)\n",
      "f1 0.7587591368159776\n",
      "Iteration 35: 0.1462617814540863\n",
      "val loss tensor(0.0430)\n",
      "f1 0.7563789963577499\n",
      "Iteration 36: 0.1452205628156662\n",
      "val loss tensor(0.0431)\n",
      "f1 0.7563789963577499\n",
      "Iteration 37: 0.14426030218601227\n",
      "val loss tensor(0.0433)\n",
      "f1 0.7540401334829849\n",
      "Iteration 38: 0.14338241517543793\n",
      "val loss tensor(0.0434)\n",
      "f1 0.7563789963577499\n",
      "Iteration 39: 0.14260461926460266\n",
      "val loss tensor(0.0434)\n",
      "f1 0.7517414617010982\n",
      "Iteration 40: 0.14189119637012482\n",
      "val loss tensor(0.0421)\n",
      "f1 0.7563789963577499\n",
      "Iteration 41: 0.14077943563461304\n",
      "val loss tensor(0.0420)\n",
      "f1 0.7494819323123929\n",
      "Iteration 42: 0.14009201526641846\n",
      "val loss tensor(0.0416)\n",
      "f1 0.7450762851802059\n",
      "Iteration 43: 0.1394081562757492\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7472605327791422\n",
      "Iteration 44: 0.13882914185523987\n",
      "val loss tensor(0.0412)\n",
      "f1 0.7472605327791422\n",
      "Iteration 45: 0.13812248408794403\n",
      "val loss tensor(0.0412)\n",
      "f1 0.7472605327791422\n",
      "Iteration 46: 0.13761679828166962\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7472605327791422\n",
      "Iteration 47: 0.13715559244155884\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7472605327791422\n",
      "Iteration 48: 0.13666656613349915\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7450762851802059\n",
      "Iteration 49: 0.13599969446659088\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7429282447442254\n",
      "Iteration 50: 0.135536789894104\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7408154984567766\n",
      "Iteration 51: 0.13509927690029144\n",
      "val loss tensor(0.0413)\n",
      "f1 0.7387371637371637\n",
      "Iteration 52: 0.13473471999168396\n",
      "val loss tensor(0.0411)\n",
      "f1 0.7387371637371637\n",
      "Iteration 53: 0.13436734676361084\n",
      "val loss tensor(0.0409)\n",
      "f1 0.7387371637371637\n",
      "Iteration 54: 0.13402271270751953\n",
      "val loss tensor(0.0408)\n",
      "f1 0.7387371637371637\n",
      "Iteration 55: 0.13364100456237793\n",
      "val loss tensor(0.0407)\n",
      "f1 0.7387371637371637\n",
      "Iteration 56: 0.1332644522190094\n",
      "val loss tensor(0.0406)\n",
      "f1 0.7387371637371637\n",
      "Iteration 57: 0.13295462727546692\n",
      "val loss tensor(0.0404)\n",
      "f1 0.7387371637371637\n",
      "Iteration 58: 0.13267387449741364\n",
      "val loss tensor(0.0402)\n",
      "f1 0.7387371637371637\n",
      "Iteration 59: 0.13238999247550964\n",
      "val loss tensor(0.0400)\n",
      "f1 0.7387371637371637\n",
      "Iteration 60: 0.13207487761974335\n",
      "val loss tensor(0.0398)\n",
      "f1 0.7387371637371637\n",
      "Iteration 61: 0.13180656731128693\n",
      "val loss tensor(0.0397)\n",
      "f1 0.7387371637371637\n",
      "Iteration 62: 0.13156510889530182\n",
      "val loss tensor(0.0396)\n",
      "f1 0.7387371637371637\n",
      "Iteration 63: 0.1313384473323822\n",
      "val loss tensor(0.0394)\n",
      "f1 0.7387371637371637\n",
      "Iteration 64: 0.13111735880374908\n",
      "val loss tensor(0.0393)\n",
      "f1 0.7408154984567766\n",
      "Iteration 65: 0.13090945780277252\n",
      "val loss tensor(0.0392)\n",
      "f1 0.7429282447442254\n",
      "Iteration 66: 0.1307154893875122\n",
      "val loss tensor(0.0391)\n",
      "f1 0.7450762851802059\n",
      "Iteration 67: 0.1305309236049652\n",
      "val loss tensor(0.0390)\n",
      "f1 0.7450762851802059\n",
      "Iteration 68: 0.13034452497959137\n",
      "val loss tensor(0.0388)\n",
      "f1 0.7472605327791422\n",
      "Iteration 69: 0.13015685975551605\n",
      "val loss tensor(0.0387)\n",
      "f1 0.7472605327791422\n",
      "Iteration 70: 0.1299838423728943\n",
      "val loss tensor(0.0386)\n",
      "f1 0.7494819323123929\n",
      "Iteration 71: 0.12982258200645447\n",
      "val loss tensor(0.0385)\n",
      "f1 0.7494819323123929\n",
      "Iteration 72: 0.12964065372943878\n",
      "val loss tensor(0.0384)\n",
      "f1 0.7494819323123929\n",
      "Iteration 73: 0.12947550415992737\n",
      "val loss tensor(0.0383)\n",
      "f1 0.7494819323123929\n",
      "Iteration 74: 0.12932126224040985\n",
      "val loss tensor(0.0382)\n",
      "f1 0.7494819323123929\n",
      "Iteration 75: 0.12917760014533997\n",
      "val loss tensor(0.0381)\n",
      "f1 0.7494819323123929\n",
      "Iteration 76: 0.12903743982315063\n",
      "val loss tensor(0.0381)\n",
      "f1 0.7494819323123929\n",
      "Iteration 77: 0.1289050281047821\n",
      "val loss tensor(0.0380)\n",
      "f1 0.7494819323123929\n",
      "Iteration 78: 0.12874700129032135\n",
      "val loss tensor(0.0380)\n",
      "f1 0.7494819323123929\n",
      "Iteration 79: 0.12860272824764252\n",
      "val loss tensor(0.0379)\n",
      "f1 0.7494819323123929\n",
      "Iteration 80: 0.12847639620304108\n",
      "val loss tensor(0.0379)\n",
      "f1 0.7494819323123929\n",
      "Iteration 81: 0.12836158275604248\n",
      "val loss tensor(0.0379)\n",
      "f1 0.7494819323123929\n",
      "Iteration 82: 0.12825356423854828\n",
      "val loss tensor(0.0379)\n",
      "f1 0.7494819323123929\n",
      "Iteration 83: 0.1281527727842331\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7494819323123929\n",
      "Iteration 84: 0.128058522939682\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7494819323123929\n",
      "Iteration 85: 0.12796804308891296\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7517414617010982\n",
      "Iteration 86: 0.12788237631320953\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7540401334829849\n",
      "Iteration 87: 0.12780192494392395\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7540401334829849\n",
      "Iteration 88: 0.12772433459758759\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7540401334829849\n",
      "Iteration 89: 0.1276494264602661\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7540401334829849\n",
      "Iteration 90: 0.12756890058517456\n",
      "val loss tensor(0.0378)\n",
      "f1 0.7540401334829849\n",
      "Iteration 91: 0.12748317420482635\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7540401334829849\n",
      "Iteration 92: 0.12738966941833496\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7540401334829849\n",
      "Iteration 93: 0.12729953229427338\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7540401334829849\n",
      "Iteration 94: 0.1272217333316803\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7540401334829849\n",
      "Iteration 95: 0.12715421617031097\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7540401334829849\n",
      "Iteration 96: 0.12709471583366394\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7540401334829849\n",
      "Iteration 97: 0.12703940272331238\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7517414617010982\n",
      "Iteration 98: 0.1269778311252594\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7517414617010982\n",
      "Iteration 99: 0.12691861391067505\n",
      "val loss tensor(0.0377)\n",
      "f1 0.7517414617010982\n"
     ]
    }
   ],
   "source": [
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 10000\n",
    "\n",
    "for iter in range(100):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, test_pred = evaluate(model, x_test, y_test, m_labels_test)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = test_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive 0.5 threshold best f1: 0.7540401334829849\n",
      "After tuning, the best f1 is 0.8993889254489047 w/ threshold 0.9962996299629964\n"
     ]
    }
   ],
   "source": [
    "# y_test and test_pred\n",
    "\n",
    "print(\"Naive 0.5 threshold best f1:\", best_val_f1)\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=10000):\n",
    "    y_test_preds = test_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for XOR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xorLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack):\n",
    "        super(xorLNN, self).__init__()\n",
    "        self.op_and1 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_and2 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_or = or_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = x[:,0].view(-1,1)\n",
    "        print(x0)\n",
    "        x1 = x[:,1].view(-1,1)\n",
    "        print(x1)\n",
    "        print(torch.cat((x0, negation(x1)), 1))\n",
    "        yhat = self.op_or(torch.cat((self.op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            self.op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.6349e-04],\n",
       "        [9.9932e-01],\n",
       "        [9.9967e-01],\n",
       "        [4.6349e-04]], grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xorLNN(0.8, 2, False)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.00041395798325538635\n",
      "Iteration 1: 0.0003413597878534347\n",
      "Iteration 2: 0.00027990160742774606\n",
      "Iteration 3: 0.00022847841319162399\n",
      "Iteration 4: 0.00018598556926008314\n",
      "Iteration 5: 0.00015122962940949947\n",
      "Iteration 6: 0.0001230025663971901\n",
      "Iteration 7: 0.00010024568473454565\n",
      "Iteration 8: 8.197502756956965e-05\n",
      "Iteration 9: 6.734087219228968e-05\n",
      "Iteration 10: 5.5627755500609055e-05\n",
      "Iteration 11: 4.625439760275185e-05\n",
      "Iteration 12: 3.872895103995688e-05\n",
      "Iteration 13: 3.267884312663227e-05\n",
      "Iteration 14: 2.7761290766648017e-05\n",
      "Iteration 15: 2.3767666789353825e-05\n",
      "Iteration 16: 2.051913361356128e-05\n",
      "Iteration 17: 1.7821967048803344e-05\n",
      "Iteration 18: 1.5646355677745305e-05\n",
      "Iteration 19: 1.3783679605694488e-05\n",
      "Iteration 20: 1.2263739336049184e-05\n",
      "Iteration 21: 1.0967321941279806e-05\n",
      "Iteration 22: 9.89442560239695e-06\n",
      "Iteration 23: 8.970543603936676e-06\n",
      "Iteration 24: 8.165872714016587e-06\n",
      "Iteration 25: 7.480413842131384e-06\n",
      "Iteration 26: 6.899264008097816e-06\n",
      "Iteration 27: 6.407521595974686e-06\n",
      "Iteration 28: 5.975385647616349e-06\n",
      "Iteration 29: 5.587952728092205e-06\n",
      "Iteration 30: 5.260125362838153e-06\n",
      "Iteration 31: 4.932297997584101e-06\n",
      "Iteration 32: 4.678976893046638e-06\n",
      "Iteration 33: 4.395853011374129e-06\n",
      "Iteration 34: 4.231939783494454e-06\n",
      "Iteration 35: 4.023322162538534e-06\n",
      "Iteration 36: 3.829606612271164e-06\n",
      "Iteration 37: 3.6805943182116607e-06\n",
      "Iteration 38: 3.561384346539853e-06\n",
      "Iteration 39: 3.4421748296153964e-06\n",
      "Iteration 40: 3.322964857943589e-06\n",
      "Iteration 41: 3.21865650221298e-06\n",
      "Iteration 42: 3.1292493076762185e-06\n",
      "Iteration 43: 3.024940724571934e-06\n",
      "Iteration 44: 2.920632368841325e-06\n",
      "Iteration 45: 2.831224946930888e-06\n",
      "Iteration 46: 2.786521463349345e-06\n",
      "Iteration 47: 2.7120154300064314e-06\n",
      "Iteration 48: 2.667311719051213e-06\n",
      "Iteration 49: 2.5928056857082993e-06\n",
      "Iteration 50: 2.563003363320604e-06\n",
      "Iteration 51: 2.4735961687838426e-06\n",
      "Iteration 52: 2.4437938463961473e-06\n",
      "Iteration 53: 2.3692878130532335e-06\n",
      "Iteration 54: 2.354386651859386e-06\n",
      "Iteration 55: 2.324584102098015e-06\n",
      "Iteration 56: 2.2500780687551014e-06\n",
      "Iteration 57: 2.2500780687551014e-06\n",
      "Iteration 58: 2.2351769075612538e-06\n",
      "Iteration 59: 2.145769485650817e-06\n",
      "Iteration 60: 2.1308683244569693e-06\n",
      "Iteration 61: 2.1308683244569693e-06\n",
      "Iteration 62: 2.056362518487731e-06\n",
      "Iteration 63: 2.0265601961000357e-06\n",
      "Iteration 64: 2.0265601961000357e-06\n",
      "Iteration 65: 2.0116588075325126e-06\n",
      "Iteration 66: 1.9371530015632743e-06\n",
      "Iteration 67: 1.9371530015632743e-06\n",
      "Iteration 68: 1.907350679175579e-06\n",
      "Iteration 69: 1.907350679175579e-06\n",
      "Iteration 70: 1.8924494042948936e-06\n",
      "Iteration 71: 1.8328446458326653e-06\n",
      "Iteration 72: 1.8179434846388176e-06\n",
      "Iteration 73: 1.80304232344497e-06\n",
      "Iteration 74: 1.7881411622511223e-06\n",
      "Iteration 75: 1.7881411622511223e-06\n",
      "Iteration 76: 1.7136352425950463e-06\n",
      "Iteration 77: 1.7136352425950463e-06\n",
      "Iteration 78: 1.7136352425950463e-06\n",
      "Iteration 79: 1.698733967714361e-06\n",
      "Iteration 80: 1.6838328065205133e-06\n",
      "Iteration 81: 1.6689316453266656e-06\n",
      "Iteration 82: 1.6093267731775995e-06\n",
      "Iteration 83: 1.6093267731775995e-06\n",
      "Iteration 84: 1.5944256119837519e-06\n",
      "Iteration 85: 1.5944256119837519e-06\n",
      "Iteration 86: 1.5944256119837519e-06\n",
      "Iteration 87: 1.5795244507899042e-06\n",
      "Iteration 88: 1.5646232895960566e-06\n",
      "Iteration 89: 1.4901173699399806e-06\n",
      "Iteration 90: 1.4901173699399806e-06\n",
      "Iteration 91: 1.4901173699399806e-06\n",
      "Iteration 92: 1.475216208746133e-06\n",
      "Iteration 93: 1.475216208746133e-06\n",
      "Iteration 94: 1.475216208746133e-06\n",
      "Iteration 95: 1.475216208746133e-06\n",
      "Iteration 96: 1.4454137726715999e-06\n",
      "Iteration 97: 1.3858090142093715e-06\n",
      "Iteration 98: 1.3858090142093715e-06\n",
      "Iteration 99: 1.3709078530155239e-06\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.0005070384358987212\n",
      "Iteration 1: 0.0004226093296892941\n",
      "Iteration 2: 0.00035011349245905876\n",
      "Iteration 3: 0.0002885941066779196\n",
      "Iteration 4: 0.0002370504371356219\n",
      "Iteration 5: 0.00019430331303738058\n",
      "Iteration 6: 0.00015917410200927407\n",
      "Iteration 7: 0.00013051435234956443\n",
      "Iteration 8: 0.0001072355080395937\n",
      "Iteration 9: 8.839827933115885e-05\n",
      "Iteration 10: 7.324236503336579e-05\n",
      "Iteration 11: 6.0992642829660326e-05\n",
      "Iteration 12: 5.106781463837251e-05\n",
      "Iteration 13: 4.3065447243861854e-05\n",
      "Iteration 14: 3.6598037695512176e-05\n",
      "Iteration 15: 3.130791810690425e-05\n",
      "Iteration 16: 2.6941725081996992e-05\n",
      "Iteration 17: 2.3410046196659096e-05\n",
      "Iteration 18: 2.048934402409941e-05\n",
      "Iteration 19: 1.8030596038443036e-05\n",
      "Iteration 20: 1.6003998098312877e-05\n",
      "Iteration 21: 1.4275432477006689e-05\n",
      "Iteration 22: 1.2785292710759677e-05\n",
      "Iteration 23: 1.1593181625357829e-05\n",
      "Iteration 24: 1.0550087608862668e-05\n",
      "Iteration 25: 9.641104952606838e-06\n",
      "Iteration 26: 8.851335223880596e-06\n",
      "Iteration 27: 8.18077660369454e-06\n",
      "Iteration 28: 7.59962586016627e-06\n",
      "Iteration 29: 7.107882993295789e-06\n",
      "Iteration 30: 6.660844519501552e-06\n",
      "Iteration 31: 6.243609277589712e-06\n",
      "Iteration 32: 5.900879841647111e-06\n",
      "Iteration 33: 5.573052021645708e-06\n",
      "Iteration 34: 5.3048297559143975e-06\n",
      "Iteration 35: 5.08131097376463e-06\n",
      "Iteration 36: 4.842891030421015e-06\n",
      "Iteration 37: 4.634273409465095e-06\n",
      "Iteration 38: 4.440557404450374e-06\n",
      "Iteration 39: 4.291544883017195e-06\n",
      "Iteration 40: 4.097828878002474e-06\n",
      "Iteration 41: 3.978619133704342e-06\n",
      "Iteration 42: 3.859408934658859e-06\n",
      "Iteration 43: 3.7551008063019253e-06\n",
      "Iteration 44: 3.635891062003793e-06\n",
      "Iteration 45: 3.5315824788995087e-06\n",
      "Iteration 46: 3.4272738957952242e-06\n",
      "Iteration 47: 3.382570184840006e-06\n",
      "Iteration 48: 3.2931629903032444e-06\n",
      "Iteration 49: 3.2037555683928076e-06\n",
      "Iteration 50: 3.1739532460051123e-06\n",
      "Iteration 51: 3.0845458240946755e-06\n",
      "Iteration 52: 2.995138629557914e-06\n",
      "Iteration 53: 2.9653360797965433e-06\n",
      "Iteration 54: 2.890830273827305e-06\n",
      "Iteration 55: 2.8461265628720867e-06\n",
      "Iteration 56: 2.771620529529173e-06\n",
      "Iteration 57: 2.75671914096165e-06\n",
      "Iteration 58: 2.741817979767802e-06\n",
      "Iteration 59: 2.652410785231041e-06\n",
      "Iteration 60: 2.637509624037193e-06\n",
      "Iteration 61: 2.563003363320604e-06\n",
      "Iteration 62: 2.5481022021267563e-06\n",
      "Iteration 63: 2.518299879739061e-06\n",
      "Iteration 64: 2.4437938463961473e-06\n",
      "Iteration 65: 2.4288926852022996e-06\n",
      "Iteration 66: 2.4288926852022996e-06\n",
      "Iteration 67: 2.413991524008452e-06\n",
      "Iteration 68: 2.3245843294716906e-06\n",
      "Iteration 69: 2.3096829409041675e-06\n",
      "Iteration 70: 2.3096829409041675e-06\n",
      "Iteration 71: 2.29478177971032e-06\n",
      "Iteration 72: 2.220275746367406e-06\n",
      "Iteration 73: 2.220275746367406e-06\n",
      "Iteration 74: 2.2053745851735584e-06\n",
      "Iteration 75: 2.1904734239797108e-06\n",
      "Iteration 76: 2.1159676180104725e-06\n",
      "Iteration 77: 2.1010662294429494e-06\n",
      "Iteration 78: 2.1010662294429494e-06\n",
      "Iteration 79: 2.0861648408754263e-06\n",
      "Iteration 80: 2.0265601961000357e-06\n",
      "Iteration 81: 2.011659034906188e-06\n",
      "Iteration 82: 1.9967578737123404e-06\n",
      "Iteration 83: 1.9818567125184927e-06\n",
      "Iteration 84: 1.9818567125184927e-06\n",
      "Iteration 85: 1.966955551324645e-06\n",
      "Iteration 86: 1.9073504518019035e-06\n",
      "Iteration 87: 1.8924494042948936e-06\n",
      "Iteration 88: 1.8924494042948936e-06\n",
      "Iteration 89: 1.877548243101046e-06\n",
      "Iteration 90: 1.877548243101046e-06\n",
      "Iteration 91: 1.7881411622511223e-06\n",
      "Iteration 92: 1.7881411622511223e-06\n",
      "Iteration 93: 1.7881411622511223e-06\n",
      "Iteration 94: 1.7732397736835992e-06\n",
      "Iteration 95: 1.7732397736835992e-06\n",
      "Iteration 96: 1.7583386124897515e-06\n",
      "Iteration 97: 1.698733967714361e-06\n",
      "Iteration 98: 1.698733967714361e-06\n",
      "Iteration 99: 1.6838328065205133e-06\n",
      "------- Checking outputs (left) vs ground truth (right): -----\n",
      "tensor([[1.9073e-06, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.9073e-06, 0.0000e+00]])\n",
      "--------------- LNN Parameters (post-training) ---------------\n",
      "OR (beta, argument weights): 9.477 [19.059 17.79 ]\n",
      "AND1 (beta, argument weights): 7.542 [14.488 15.328]\n",
      "AND2 (beta, argument weights): 7.761 [14.913 15.211]\n"
     ]
    }
   ],
   "source": [
    "#this is a hyperparameter\n",
    "alpha = 0.8\n",
    "\n",
    "op_and1 = and_lukasiewicz(alpha, 2, False)\n",
    "op_and2 = and_lukasiewicz(alpha, 2, False)\n",
    "op_or = or_lukasiewicz(alpha, 2, False)\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam([{'params': op_or.parameters()}, \\\n",
    "                        {'params': op_and1.parameters()}, \\\n",
    "                        {'params': op_and2.parameters()}], lr=0.1)\n",
    "\n",
    "for iter in range(100):\n",
    "    op_or.train()\n",
    "    op_and1.train()\n",
    "    op_and2.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = x[:,0].view(-1,1)\n",
    "    x1 = x[:,1].view(-1,1)\n",
    "    yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#check to see output of xor post-training\n",
    "x0 = x[:,0].view(-1,1)\n",
    "x1 = x[:,1].view(-1,1)\n",
    "yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                        op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "check_values = torch.cat((yhat, y), 1)\n",
    "print(\"------- Checking outputs (left) vs ground truth (right): -----\")\n",
    "print(check_values.detach())\n",
    "\n",
    "#LNN parameters: post-training (we have 3 sets of beta, argument weights)\n",
    "print(\"--------------- LNN Parameters (post-training) ---------------\")\n",
    "beta_or, argument_wts_or = op_or.AND.cdd()\n",
    "beta_and1, argument_wts_and1 = op_and1.cdd()\n",
    "beta_and2, argument_wts_and2 = op_and2.cdd()\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"OR (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_or.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_or.detach().numpy()))\n",
    "print(\"AND1 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and1.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and1.detach().numpy()))\n",
    "print(\"AND2 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and2.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arity should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PureNameLNN(nn.Module):\n",
    "#     def __init__(self, alpha, sim_arity=4, rule_arity=2, slack=None):\n",
    "#         super(PureNameLNN, self).__init__()\n",
    "#         self.threshold = 0.5\n",
    "        \n",
    "#         self.sim_disjunction_or = or_lukasiewicz(alpha, sim_arity, slack)\n",
    "    \n",
    "#     def forward(self, x, mention_labels=None):\n",
    "#         \"\"\"\n",
    "#             x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "#                normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "#         \"\"\"\n",
    "#         yhat = None\n",
    "        \n",
    "#         # RULE 1: lookup predicate\n",
    "#         lookup_features = x[:,5]\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "#         # RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb))\n",
    "#         feature_list = []\n",
    "#         # rule 2 (1) mention==label\n",
    "#         mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "#         labels = np.array([m[1].lower() for m in mention_labels])\n",
    "#         exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float()\n",
    "#         feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "#         # rule 2 (2)-(4) Jaro(m, lb) AND Jacc(m, lb) AND Lev(m, lb))\n",
    "#         sim_features = x[:, 0:3]\n",
    "#         print(sim_features)\n",
    "\n",
    "#         return yhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
