{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f845827ddf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/meta_rule/')\n",
    "\n",
    "from lnn_operators import and_lukasiewicz, or_lukasiewicz, negation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5158, 0.0000, 0.1579, 0.4390, 1.0000, 1.0000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4980, 0.8569, 0.4000],\n",
      "        [0.4815, 0.0000, 0.0000, 0.4292, 0.8430, 0.5000],\n",
      "        ...,\n",
      "        [0.9176, 0.6667, 0.5882, 0.9091, 0.1000, 1.0000],\n",
      "        [0.6235, 0.4000, 0.3704, 0.8776, 0.1000, 0.0000],\n",
      "        [0.6449, 0.4000, 0.4348, 0.8339, 0.1000, 0.0000]]) torch.Size([31553, 6])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.]) torch.Size([31553])\n",
      "['GMT;Greenwich Mean Time' 'GMT;UTC+02:00' 'GMT;UTC+08:00' ...\n",
      " 'John Adams;John Adams School' 'John Adams;USS John Adams (SSBN-620) 1'\n",
      " 'John Adams;USS John Adams (1799) 1']\n"
     ]
    }
   ],
   "source": [
    "# train and val\n",
    "\n",
    "df_train_val = pd.read_csv(\"data/train.csv\")\n",
    "# df_train_val = df_train_val.loc[22:25]\n",
    "\n",
    "features_train_val = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train_val.Features.values])\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "X_train_val = torch.from_numpy(features_train_val).float()\n",
    "print(X_train_val, X_train_val.shape)\n",
    "#the target values for each row in the truth table (xor)\n",
    "Y_train_val = torch.from_numpy(df_train_val.Label.values).float()\n",
    "print(Y_train_val, Y_train_val.shape)\n",
    "# mention_labels (cannot convert string explicitly)\n",
    "mention_labels_train_val = df_train_val.Mention_label.values\n",
    "print(mention_labels_train_val)\n",
    "\n",
    "x_train, x_val, y_train, y_val, m_labels_train, m_labels_val = \\\n",
    "    train_test_split(X_train_val, Y_train_val, mention_labels_train_val, test_size=0.2,train_size=0.8, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5962, 0.0000, 0.1538, 0.6732, 1.0000, 1.0000],\n",
      "        [0.4259, 0.0000, 0.0278, 0.4125, 0.6727, 1.0000],\n",
      "        [0.5722, 0.0000, 0.0741, 0.6346, 0.6726, 1.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5000, 0.5556, 0.7310, 0.1000, 0.0000],\n",
      "        [0.4256, 0.3333, 0.3846, 0.5935, 0.1000, 0.0000],\n",
      "        [0.6444, 0.3333, 0.3333, 0.5635, 0.1000, 1.0000]]) torch.Size([19048, 6])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.]) torch.Size([19048])\n",
      "['U.S.;United States' 'U.S.;National Register of Historic Places'\n",
      " 'U.S.;United States Census Bureau' ... 'Pluto;HMS Pluto'\n",
      " 'Pluto;Terry Pluto 1' 'Pluto;The Pluto Files']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "features_test = np.array([np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_test.Features.values])\n",
    "\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "print(x_test, x_test.shape)\n",
    "y_test = torch.from_numpy(df_test.Label.values).float()\n",
    "print(y_test, y_test.shape)\n",
    "m_labels_test = df_test.Mention_label.values\n",
    "print(m_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([25242, 6])\n",
      "val torch.Size([19048, 6])\n"
     ]
    }
   ],
   "source": [
    "print('train', x_train.shape)\n",
    "print('val', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PureNameLNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4205e-08],\n",
      "        [7.0606e-05],\n",
      "        [8.5324e-08],\n",
      "        ...,\n",
      "        [8.8844e-04],\n",
      "        [6.9631e-05],\n",
      "        [1.2467e-04]], grad_fn=<SWhereBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from RuleLNN_nway import *\n",
    "\n",
    "# Sanity Check\n",
    "model = PureNameLNN(0.9, 2, False)\n",
    "print(model(x_train, m_labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7677e-09],\n",
      "        [1.1838e-05],\n",
      "        [1.5784e-08],\n",
      "        ...,\n",
      "        [3.0863e-04],\n",
      "        [8.4752e-07],\n",
      "        [2.4594e-05]], grad_fn=<SWhereBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = PureNameLNN(0.9, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, val_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.06329169124364853\n",
      "val loss tensor(0.0636)\n",
      "f1 w/ 0.5 threshold 0.737399375850204\n",
      "Iteration 1: 0.05505715310573578\n",
      "val loss tensor(0.0569)\n",
      "f1 w/ 0.5 threshold 0.7567702980472765\n",
      "Iteration 2: 0.048817068338394165\n",
      "val loss tensor(0.0534)\n",
      "f1 w/ 0.5 threshold 0.7665207560125251\n",
      "Iteration 3: 0.0443258099257946\n",
      "val loss tensor(0.0480)\n",
      "f1 w/ 0.5 threshold 0.7681632327104296\n",
      "Iteration 4: 0.03993341699242592\n",
      "val loss tensor(0.0467)\n",
      "f1 w/ 0.5 threshold 0.7737535768060018\n",
      "Iteration 5: 0.038260553032159805\n",
      "val loss tensor(0.0447)\n",
      "f1 w/ 0.5 threshold 0.7749194994701232\n",
      "Iteration 6: 0.036917369812726974\n",
      "val loss tensor(0.0443)\n",
      "f1 w/ 0.5 threshold 0.7671011854647755\n",
      "Iteration 7: 0.03630094230175018\n",
      "val loss tensor(0.0439)\n",
      "f1 w/ 0.5 threshold 0.7730893318635716\n",
      "Iteration 8: 0.035539813339710236\n",
      "val loss tensor(0.0435)\n",
      "f1 w/ 0.5 threshold 0.782575032275846\n",
      "Iteration 9: 0.03473767638206482\n",
      "val loss tensor(0.0426)\n",
      "f1 w/ 0.5 threshold 0.7858804542682787\n",
      "Iteration 10: 0.03422999382019043\n",
      "val loss tensor(0.0419)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 11: 0.033729150891304016\n",
      "val loss tensor(0.0417)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 12: 0.033188268542289734\n",
      "val loss tensor(0.0416)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 13: 0.032633643597364426\n",
      "val loss tensor(0.0412)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 14: 0.03224712982773781\n",
      "val loss tensor(0.0409)\n",
      "f1 w/ 0.5 threshold 0.803602131715097\n",
      "Iteration 15: 0.031707603484392166\n",
      "val loss tensor(0.0405)\n",
      "f1 w/ 0.5 threshold 0.803602131715097\n",
      "Iteration 16: 0.031120358034968376\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.803602131715097\n",
      "Iteration 17: 0.03064713440835476\n",
      "val loss tensor(0.0393)\n",
      "f1 w/ 0.5 threshold 0.803602131715097\n",
      "Iteration 18: 0.03029279038310051\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.803602131715097\n",
      "Iteration 19: 0.030006391927599907\n",
      "val loss tensor(0.0382)\n",
      "f1 w/ 0.5 threshold 0.8074060292118395\n",
      "Iteration 20: 0.029740935191512108\n",
      "val loss tensor(0.0377)\n",
      "f1 w/ 0.5 threshold 0.8074060292118395\n",
      "Iteration 21: 0.02948511578142643\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.8074060292118395\n",
      "Iteration 22: 0.029198940843343735\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.8074060292118395\n",
      "Iteration 23: 0.028916578739881516\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.8074060292118395\n",
      "Iteration 24: 0.028537016361951828\n",
      "val loss tensor(0.0356)\n",
      "f1 w/ 0.5 threshold 0.8074060292118395\n",
      "Iteration 25: 0.028256172314286232\n",
      "val loss tensor(0.0348)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 26: 0.02798263169825077\n",
      "val loss tensor(0.0343)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 27: 0.027732649818062782\n",
      "val loss tensor(0.0339)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 28: 0.02749793790280819\n",
      "val loss tensor(0.0335)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 29: 0.027277758345007896\n",
      "val loss tensor(0.0332)\n",
      "f1 w/ 0.5 threshold 0.8152996729226146\n",
      "Iteration 30: 0.02706439420580864\n",
      "val loss tensor(0.0329)\n",
      "f1 w/ 0.5 threshold 0.8152996729226146\n",
      "Iteration 31: 0.02682076394557953\n",
      "val loss tensor(0.0326)\n",
      "f1 w/ 0.5 threshold 0.8152996729226146\n",
      "Iteration 32: 0.026557274162769318\n",
      "val loss tensor(0.0323)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 33: 0.026300903409719467\n",
      "val loss tensor(0.0319)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 34: 0.026061559095978737\n",
      "val loss tensor(0.0316)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 35: 0.025837795808911324\n",
      "val loss tensor(0.0313)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 36: 0.025629792362451553\n",
      "val loss tensor(0.0310)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 37: 0.025433670729398727\n",
      "val loss tensor(0.0308)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 38: 0.025248749181628227\n",
      "val loss tensor(0.0306)\n",
      "f1 w/ 0.5 threshold 0.8113040184978473\n",
      "Iteration 39: 0.025079142302274704\n",
      "val loss tensor(0.0304)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 40: 0.024913957342505455\n",
      "val loss tensor(0.0302)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 41: 0.02472076378762722\n",
      "val loss tensor(0.0301)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 42: 0.02456565573811531\n",
      "val loss tensor(0.0299)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 43: 0.02442934736609459\n",
      "val loss tensor(0.0296)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 44: 0.024308348074555397\n",
      "val loss tensor(0.0294)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 45: 0.024198833853006363\n",
      "val loss tensor(0.0292)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 46: 0.02409939281642437\n",
      "val loss tensor(0.0291)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 47: 0.024009525775909424\n",
      "val loss tensor(0.0289)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 48: 0.023927701637148857\n",
      "val loss tensor(0.0288)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 49: 0.023854119703173637\n",
      "val loss tensor(0.0286)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 50: 0.02378762699663639\n",
      "val loss tensor(0.0285)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 51: 0.023727426305413246\n",
      "val loss tensor(0.0284)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 52: 0.023673707619309425\n",
      "val loss tensor(0.0283)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 53: 0.023625537753105164\n",
      "val loss tensor(0.0281)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 54: 0.023582514375448227\n",
      "val loss tensor(0.0279)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 55: 0.02354210801422596\n",
      "val loss tensor(0.0278)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 56: 0.0234982892870903\n",
      "val loss tensor(0.0277)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 57: 0.02345113269984722\n",
      "val loss tensor(0.0276)\n",
      "f1 w/ 0.5 threshold 0.7858804542682787\n",
      "Iteration 58: 0.0234050452709198\n",
      "val loss tensor(0.0275)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 59: 0.023360129445791245\n",
      "val loss tensor(0.0274)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 60: 0.023316429927945137\n",
      "val loss tensor(0.0274)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 61: 0.02327408827841282\n",
      "val loss tensor(0.0274)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 62: 0.023233113810420036\n",
      "val loss tensor(0.0273)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 63: 0.023193733766674995\n",
      "val loss tensor(0.0273)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 64: 0.02315608039498329\n",
      "val loss tensor(0.0273)\n",
      "f1 w/ 0.5 threshold 0.7892618097089684\n",
      "Iteration 65: 0.023120256140828133\n",
      "val loss tensor(0.0272)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 66: 0.023077694699168205\n",
      "val loss tensor(0.0272)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 67: 0.023035842925310135\n",
      "val loss tensor(0.0272)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 68: 0.02299710549414158\n",
      "val loss tensor(0.0272)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 69: 0.022961432114243507\n",
      "val loss tensor(0.0272)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 70: 0.022928711026906967\n",
      "val loss tensor(0.0272)\n",
      "f1 w/ 0.5 threshold 0.7927217788287844\n",
      "Iteration 71: 0.02289910614490509\n",
      "val loss tensor(0.0271)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 72: 0.02287251316010952\n",
      "val loss tensor(0.0271)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 73: 0.022848257794976234\n",
      "val loss tensor(0.0271)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 74: 0.022825991734862328\n",
      "val loss tensor(0.0271)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 75: 0.022805098444223404\n",
      "val loss tensor(0.0270)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 76: 0.02278563193976879\n",
      "val loss tensor(0.0270)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 77: 0.022767307236790657\n",
      "val loss tensor(0.0270)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 78: 0.022749725729227066\n",
      "val loss tensor(0.0270)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 79: 0.02273283526301384\n",
      "val loss tensor(0.0270)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 80: 0.022716514766216278\n",
      "val loss tensor(0.0269)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 81: 0.022700700908899307\n",
      "val loss tensor(0.0269)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 82: 0.022685296833515167\n",
      "val loss tensor(0.0269)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 83: 0.022670287638902664\n",
      "val loss tensor(0.0269)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 84: 0.02265571616590023\n",
      "val loss tensor(0.0269)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 85: 0.022641506046056747\n",
      "val loss tensor(0.0268)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86: 0.02262766845524311\n",
      "val loss tensor(0.0268)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 87: 0.02261422947049141\n",
      "val loss tensor(0.0268)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 88: 0.022601181641221046\n",
      "val loss tensor(0.0268)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 89: 0.022588621824979782\n",
      "val loss tensor(0.0267)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 90: 0.022576697170734406\n",
      "val loss tensor(0.0267)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 91: 0.02256515994668007\n",
      "val loss tensor(0.0267)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 92: 0.022554049268364906\n",
      "val loss tensor(0.0267)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 93: 0.02254355140030384\n",
      "val loss tensor(0.0267)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 94: 0.02253328636288643\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 95: 0.02252349816262722\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 96: 0.022514205425977707\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 97: 0.02250523678958416\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 98: 0.02249654196202755\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 99: 0.02248826064169407\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 100: 0.022480342537164688\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 101: 0.022472504526376724\n",
      "val loss tensor(0.0266)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 102: 0.02246474288403988\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 103: 0.022457124665379524\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 104: 0.022449584677815437\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 105: 0.022442106157541275\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 106: 0.022434810176491737\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 107: 0.022427773103117943\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 108: 0.022420989349484444\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 109: 0.0224144347012043\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 110: 0.02240818925201893\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 111: 0.02240189164876938\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 112: 0.022395798936486244\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 113: 0.022389911115169525\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 114: 0.02238413132727146\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 115: 0.022378459572792053\n",
      "val loss tensor(0.0265)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 116: 0.022372905164957047\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 117: 0.022367505356669426\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 118: 0.022362135350704193\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 119: 0.022356951609253883\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 120: 0.022351853549480438\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 121: 0.022346792742609978\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 122: 0.022341886535286903\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 123: 0.022337080910801888\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 124: 0.022332321852445602\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 125: 0.022327665239572525\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 126: 0.02232309617102146\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 127: 0.022318748757243156\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 128: 0.022314297035336494\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 129: 0.022310012951493263\n",
      "val loss tensor(0.0264)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 130: 0.02230582945048809\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 131: 0.02230169251561165\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 132: 0.02229762263596058\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 133: 0.022293657064437866\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 134: 0.022289764136075974\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 135: 0.022285902872681618\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 136: 0.02228211984038353\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 137: 0.02227841690182686\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 138: 0.022274723276495934\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 139: 0.022271106019616127\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 140: 0.022267509251832962\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 141: 0.022264031693339348\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 142: 0.022260526195168495\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 143: 0.022257143631577492\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 144: 0.022253816947340965\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 145: 0.02225053310394287\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 146: 0.022247329354286194\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 147: 0.02224414423108101\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 148: 0.02224104478955269\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 149: 0.02223793976008892\n",
      "val loss tensor(0.0263)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 150: 0.022234944626688957\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 151: 0.022231951355934143\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 152: 0.022229021415114403\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 153: 0.022226089611649513\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 154: 0.022223230451345444\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 155: 0.022220412269234657\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 156: 0.022217610850930214\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 157: 0.022214865311980247\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 158: 0.02221212349832058\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 159: 0.022209450602531433\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 160: 0.02220681495964527\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 161: 0.022204196080565453\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 162: 0.022201646119356155\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 163: 0.02219914086163044\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 164: 0.022196633741259575\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 165: 0.022194143384695053\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 166: 0.022191695868968964\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 167: 0.022189371287822723\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 168: 0.022186946123838425\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 169: 0.022184640169143677\n",
      "val loss tensor(0.0262)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 170: 0.02218233235180378\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 171: 0.022180013358592987\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 172: 0.022177761420607567\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 173: 0.02217555232346058\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 174: 0.022173341363668442\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 175: 0.022171147167682648\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 176: 0.022169072180986404\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 177: 0.022166887298226357\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 178: 0.02216479554772377\n",
      "val loss tensor(0.0262)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 179: 0.022162754088640213\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 180: 0.022160714492201805\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 181: 0.022158680483698845\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7962631694885645\n",
      "Iteration 182: 0.02215670235455036\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 183: 0.022154707461595535\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 184: 0.02215276099741459\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 185: 0.022150911390781403\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 186: 0.022148901596665382\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 187: 0.022147059440612793\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 188: 0.022145187482237816\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 189: 0.022143332287669182\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 190: 0.022141486406326294\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 191: 0.022139696404337883\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 192: 0.022137904539704323\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 193: 0.02213616855442524\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 194: 0.02213437668979168\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 195: 0.022132696583867073\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 196: 0.022130992263555527\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 197: 0.02212931588292122\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 198: 0.022127613425254822\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n",
      "Iteration 199: 0.0221259668469429\n",
      "val loss tensor(0.0261)\n",
      "f1 w/ 0.5 threshold 0.7998889248676659\n"
     ]
    }
   ],
   "source": [
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 1000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = val_pred\n",
    "        torch.save(model.state_dict(), \"best_PureNameLNN.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- The best f1 is 0.7998889248676659 w/ naive threshold 0.5\n",
      "Val -- After tuning, the best f1 is 0.8415493457427705 w/ threshold 0.993993993993994\n"
     ]
    }
   ],
   "source": [
    "# tune on val set\n",
    "\n",
    "print(\"Val -- The best f1 is {} w/ naive threshold 0.5\".format(best_val_f1))\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=1000):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -- f1 is 0.8843884222621656 w/ threshold 0.993993993993994\n"
     ]
    }
   ],
   "source": [
    "bestModel = PureNameLNN(0.9, 2, False)\n",
    "bestModel.load_state_dict(torch.load(\"best_PureNameLNN.pt\"))\n",
    "bestModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = bestModel(x_test, m_labels_test)\n",
    "    test_pred = test_pred >= best_tuned_threshold\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='macro')\n",
    "    print(\"Test -- f1 is {} w/ threshold {}\".format(f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "rows = []\n",
    "question_rows_map = defaultdict(list)\n",
    "\n",
    "for i, (pred, true_label) in enumerate(zip(test_pred, y_test)):\n",
    "    pred = pred.data.tolist()[0]\n",
    "    true_label = true_label.tolist()\n",
    "    if pred:\n",
    "        row = df_test.iloc[i]\n",
    "        men_entity_label = '_'.join(m_labels_test[i].split(';')[-1].split())\n",
    "        question_rows_map[row.Question].append(('http://dbpedia.org/resource/{}'.format(men_entity_label), 1.0))\n",
    "#         print(row.Question, question_rows_map[row.Question])\n",
    "\n",
    "for key, value in question_rows_map.items():\n",
    "    rows.append([key, [value]])\n",
    "\n",
    "df_output = pd.DataFrame(rows)\n",
    "df_output.head()\n",
    "df_output.to_csv(\"output/purename_nway_alpha09.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(8.6251, grad_fn=<SelectBackward>), tensor([19.7417, 10.8246], grad_fn=<SliceBackward>))\n",
      "(tensor(8.4585, grad_fn=<SelectBackward>), tensor([19.1825, 11.0743], grad_fn=<SliceBackward>))\n",
      "(tensor(7.0526, grad_fn=<SelectBackward>), tensor([16.0262,  8.7171], grad_fn=<SliceBackward>))\n",
      "(tensor(4.1313, grad_fn=<SelectBackward>), tensor([7.7373, 6.6682], grad_fn=<SliceBackward>))\n",
      "=========predicate_and=========\n",
      "and_lukasiewicz(\n",
      "  (cdd): cdd_lnn()\n",
      ") (tensor(2.8722, grad_fn=<SelectBackward>), tensor([6.9262, 3.3761], grad_fn=<SliceBackward>))\n"
     ]
    }
   ],
   "source": [
    "for name, mod in bestModel.named_children():\n",
    "    print(\"========={}=========\".format(name))\n",
    "    if type(mod) == nn.ModuleList:\n",
    "        for each_op in mod:\n",
    "            print(each_op.AND.cdd())\n",
    "    else:\n",
    "        print(mod, mod.cdd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (post-training): 2.8722004890441895\n",
      "argument weights (post-training): tensor([6.9262, 3.3761])\n",
      "beta (post-training): 8.625083923339844\n",
      "argument weights (post-training): tensor([19.7417, 10.8246])\n",
      "beta (post-training): 8.45850658416748\n",
      "argument weights (post-training): tensor([19.1825, 11.0743])\n",
      "beta (post-training): 7.052649021148682\n",
      "argument weights (post-training): tensor([16.0262,  8.7171])\n",
      "beta (post-training): 4.1312785148620605\n",
      "argument weights (post-training): tensor([7.7373, 6.6682])\n"
     ]
    }
   ],
   "source": [
    "# beta, argument_wts = bestModel.predicate_and.cdd()\n",
    "# print(\"beta (post-training): \" + str(beta.item()))\n",
    "# print(\"argument weights (post-training): \" + str(argument_wts.detach()))\n",
    "\n",
    "# for each_op in bestModel.sim_disjunction_or_ops:\n",
    "#     #lets check the LNN conjunction parameters post-training\n",
    "#     #do these look different from the pre-training settings?\n",
    "#     beta, argument_wts = each_op.AND.cdd()\n",
    "#     print(\"beta (post-training): \" + str(beta.item()))\n",
    "#     print(\"argument weights (post-training): \" + str(argument_wts.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ContextLNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9200e-04],\n",
      "        [1.5054e-07],\n",
      "        [9.9943e-01],\n",
      "        ...,\n",
      "        [2.6379e-04],\n",
      "        [1.0349e-05],\n",
      "        [2.5376e-04]], grad_fn=<SWhereBackward>)\n",
      "Iteration 0: 0.2859273850917816\n",
      "val loss tensor(0.2682)\n",
      "f1 w/ 0.5 threshold 0.5848549075800495\n",
      "Iteration 1: 0.2410481721162796\n",
      "val loss tensor(0.2264)\n",
      "f1 w/ 0.5 threshold 0.5960588363785881\n",
      "Iteration 2: 0.20673899352550507\n",
      "val loss tensor(0.1875)\n",
      "f1 w/ 0.5 threshold 0.6139278208619465\n",
      "Iteration 3: 0.1752336174249649\n",
      "val loss tensor(0.1554)\n",
      "f1 w/ 0.5 threshold 0.6279422651402675\n",
      "Iteration 4: 0.1513768881559372\n",
      "val loss tensor(0.1329)\n",
      "f1 w/ 0.5 threshold 0.6470300283758199\n",
      "Iteration 5: 0.1327628642320633\n",
      "val loss tensor(0.1150)\n",
      "f1 w/ 0.5 threshold 0.6585167134759234\n",
      "Iteration 6: 0.11826548725366592\n",
      "val loss tensor(0.1046)\n",
      "f1 w/ 0.5 threshold 0.6729966604011413\n",
      "Iteration 7: 0.10744988918304443\n",
      "val loss tensor(0.0941)\n",
      "f1 w/ 0.5 threshold 0.6797538720201313\n",
      "Iteration 8: 0.09956284612417221\n",
      "val loss tensor(0.0883)\n",
      "f1 w/ 0.5 threshold 0.6855150368395402\n",
      "Iteration 9: 0.09405536949634552\n",
      "val loss tensor(0.0819)\n",
      "f1 w/ 0.5 threshold 0.6998018009294376\n",
      "Iteration 10: 0.08972397446632385\n",
      "val loss tensor(0.0748)\n",
      "f1 w/ 0.5 threshold 0.70502583002583\n",
      "Iteration 11: 0.08479087799787521\n",
      "val loss tensor(0.0687)\n",
      "f1 w/ 0.5 threshold 0.7123999166107481\n",
      "Iteration 12: 0.07821345329284668\n",
      "val loss tensor(0.0626)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 13: 0.07433254271745682\n",
      "val loss tensor(0.0605)\n",
      "f1 w/ 0.5 threshold 0.737836964884521\n",
      "Iteration 14: 0.07177156209945679\n",
      "val loss tensor(0.0586)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 15: 0.06951772421598434\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 16: 0.0673786923289299\n",
      "val loss tensor(0.0563)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 17: 0.06599988788366318\n",
      "val loss tensor(0.0553)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 18: 0.06360871344804764\n",
      "val loss tensor(0.0546)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 19: 0.06222320720553398\n",
      "val loss tensor(0.0540)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 20: 0.0609227754175663\n",
      "val loss tensor(0.0536)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 21: 0.059561342000961304\n",
      "val loss tensor(0.0532)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 22: 0.05859719589352608\n",
      "val loss tensor(0.0529)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 23: 0.05768325924873352\n",
      "val loss tensor(0.0525)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 24: 0.0565960668027401\n",
      "val loss tensor(0.0522)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 25: 0.056002408266067505\n",
      "val loss tensor(0.0517)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 26: 0.05505039915442467\n",
      "val loss tensor(0.0508)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 27: 0.054295796900987625\n",
      "val loss tensor(0.0504)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 28: 0.05367742478847504\n",
      "val loss tensor(0.0501)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 29: 0.05320493504405022\n",
      "val loss tensor(0.0498)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 30: 0.052634648978710175\n",
      "val loss tensor(0.0496)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 31: 0.05208703130483627\n",
      "val loss tensor(0.0493)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 32: 0.05166373401880264\n",
      "val loss tensor(0.0490)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 33: 0.0513063445687294\n",
      "val loss tensor(0.0487)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 34: 0.05101126432418823\n",
      "val loss tensor(0.0486)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 35: 0.05073432996869087\n",
      "val loss tensor(0.0484)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 36: 0.050489455461502075\n",
      "val loss tensor(0.0483)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 37: 0.05017496645450592\n",
      "val loss tensor(0.0481)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 38: 0.04991751164197922\n",
      "val loss tensor(0.0477)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 39: 0.049715667963027954\n",
      "val loss tensor(0.0474)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 40: 0.049543701112270355\n",
      "val loss tensor(0.0472)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 41: 0.049394797533750534\n",
      "val loss tensor(0.0471)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 42: 0.04923468083143234\n",
      "val loss tensor(0.0470)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 43: 0.049067750573158264\n",
      "val loss tensor(0.0469)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 44: 0.04891333729028702\n",
      "val loss tensor(0.0466)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 45: 0.04860537871718407\n",
      "val loss tensor(0.0463)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 46: 0.04796835780143738\n",
      "val loss tensor(0.0462)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 47: 0.047833457589149475\n",
      "val loss tensor(0.0460)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 48: 0.04767162352800369\n",
      "val loss tensor(0.0458)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 49: 0.047547291964292526\n",
      "val loss tensor(0.0455)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 50: 0.047445669770240784\n",
      "val loss tensor(0.0454)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 51: 0.04711690917611122\n",
      "val loss tensor(0.0448)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 52: 0.047032974660396576\n",
      "val loss tensor(0.0446)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 53: 0.04695623368024826\n",
      "val loss tensor(0.0443)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 54: 0.04679656773805618\n",
      "val loss tensor(0.0441)\n",
      "f1 w/ 0.5 threshold 0.7667906230964376\n",
      "Iteration 55: 0.04669475927948952\n",
      "val loss tensor(0.0440)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 56: 0.046544693410396576\n",
      "val loss tensor(0.0437)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 57: 0.046396803110837936\n",
      "val loss tensor(0.0434)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 58: 0.04628228396177292\n",
      "val loss tensor(0.0431)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 59: 0.04618437588214874\n",
      "val loss tensor(0.0429)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 60: 0.04609444737434387\n",
      "val loss tensor(0.0427)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 61: 0.046013571321964264\n",
      "val loss tensor(0.0426)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 62: 0.045939259231090546\n",
      "val loss tensor(0.0425)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 63: 0.045811302959918976\n",
      "val loss tensor(0.0424)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 64: 0.04570474848151207\n",
      "val loss tensor(0.0423)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 65: 0.045614879578351974\n",
      "val loss tensor(0.0422)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 66: 0.04554110765457153\n",
      "val loss tensor(0.0421)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 67: 0.04547547549009323\n",
      "val loss tensor(0.0420)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 68: 0.04541512578725815\n",
      "val loss tensor(0.0420)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 69: 0.04535946995019913\n",
      "val loss tensor(0.0419)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 70: 0.04527847096323967\n",
      "val loss tensor(0.0418)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 71: 0.04513586685061455\n",
      "val loss tensor(0.0418)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 72: 0.04502824693918228\n",
      "val loss tensor(0.0417)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 73: 0.04495023563504219\n",
      "val loss tensor(0.0416)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 74: 0.044884614646434784\n",
      "val loss tensor(0.0416)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 75: 0.044823843985795975\n",
      "val loss tensor(0.0415)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 76: 0.04476819932460785\n",
      "val loss tensor(0.0415)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 77: 0.04471709951758385\n",
      "val loss tensor(0.0414)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 78: 0.044670116156339645\n",
      "val loss tensor(0.0414)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 79: 0.04462619125843048\n",
      "val loss tensor(0.0413)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 80: 0.04458530247211456\n",
      "val loss tensor(0.0413)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 81: 0.04454651474952698\n",
      "val loss tensor(0.0411)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 82: 0.044510629028081894\n",
      "val loss tensor(0.0408)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 83: 0.04447885975241661\n",
      "val loss tensor(0.0407)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 84: 0.04444865137338638\n",
      "val loss tensor(0.0406)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 85: 0.04440293461084366\n",
      "val loss tensor(0.0406)\n",
      "f1 w/ 0.5 threshold 0.7697598655902963\n",
      "Iteration 86: 0.044356729835271835\n",
      "val loss tensor(0.0405)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87: 0.04431500658392906\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 88: 0.04427635297179222\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 89: 0.04423997551202774\n",
      "val loss tensor(0.0404)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 90: 0.04420547932386398\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 91: 0.044172924011945724\n",
      "val loss tensor(0.0403)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 92: 0.04414163529872894\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7727933173225794\n",
      "Iteration 93: 0.044053349643945694\n",
      "val loss tensor(0.0402)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 94: 0.043939944356679916\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 95: 0.04386923462152481\n",
      "val loss tensor(0.0401)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 96: 0.04381221905350685\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 97: 0.04376303777098656\n",
      "val loss tensor(0.0400)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 98: 0.04368624836206436\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 99: 0.0436105914413929\n",
      "val loss tensor(0.0399)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 100: 0.0434652678668499\n",
      "val loss tensor(0.0398)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 101: 0.04335663467645645\n",
      "val loss tensor(0.0394)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 102: 0.04319017753005028\n",
      "val loss tensor(0.0392)\n",
      "f1 w/ 0.5 threshold 0.7758931189010344\n",
      "Iteration 103: 0.04307891055941582\n",
      "val loss tensor(0.0391)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 104: 0.042995989322662354\n",
      "val loss tensor(0.0390)\n",
      "f1 w/ 0.5 threshold 0.7790615071403844\n",
      "Iteration 105: 0.042910780757665634\n",
      "val loss tensor(0.0390)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 106: 0.042817696928977966\n",
      "val loss tensor(0.0389)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 107: 0.04273880645632744\n",
      "val loss tensor(0.0388)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 108: 0.0426710806787014\n",
      "val loss tensor(0.0388)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 109: 0.042611151933670044\n",
      "val loss tensor(0.0388)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 110: 0.04253305867314339\n",
      "val loss tensor(0.0387)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 111: 0.04244228079915047\n",
      "val loss tensor(0.0387)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 112: 0.04236641898751259\n",
      "val loss tensor(0.0387)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 113: 0.04229912534356117\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 114: 0.04223693162202835\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 115: 0.04217987507581711\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7823008205286328\n",
      "Iteration 116: 0.04212726652622223\n",
      "val loss tensor(0.0386)\n",
      "f1 w/ 0.5 threshold 0.7856135050703569\n",
      "Iteration 117: 0.04205819591879845\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 118: 0.041994500905275345\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 119: 0.04193778336048126\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 120: 0.04188627749681473\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 121: 0.04183843731880188\n",
      "val loss tensor(0.0385)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 122: 0.04179354012012482\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 123: 0.04175136238336563\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 124: 0.041711993515491486\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 125: 0.041674450039863586\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 126: 0.04163876175880432\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 127: 0.04160492494702339\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 128: 0.041500069200992584\n",
      "val loss tensor(0.0384)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 129: 0.041263189166784286\n",
      "val loss tensor(0.0383)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 130: 0.041042622178792953\n",
      "val loss tensor(0.0383)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 131: 0.04091261699795723\n",
      "val loss tensor(0.0383)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 132: 0.04081980884075165\n",
      "val loss tensor(0.0382)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 133: 0.04074649512767792\n",
      "val loss tensor(0.0382)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 134: 0.04068475216627121\n",
      "val loss tensor(0.0381)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 135: 0.040630701929330826\n",
      "val loss tensor(0.0381)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 136: 0.04058527201414108\n",
      "val loss tensor(0.0381)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 137: 0.040549539029598236\n",
      "val loss tensor(0.0376)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 138: 0.040504664182662964\n",
      "val loss tensor(0.0375)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 139: 0.04046022146940231\n",
      "val loss tensor(0.0374)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 140: 0.040420640259981155\n",
      "val loss tensor(0.0374)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 141: 0.04038676247000694\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 142: 0.0403556264936924\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 143: 0.040326155722141266\n",
      "val loss tensor(0.0373)\n",
      "f1 w/ 0.5 threshold 0.7890021205376702\n",
      "Iteration 144: 0.04029879719018936\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 145: 0.04027266800403595\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 146: 0.04024891182780266\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 147: 0.04022563248872757\n",
      "val loss tensor(0.0372)\n",
      "f1 w/ 0.5 threshold 0.7924693471624309\n",
      "Iteration 148: 0.04020383208990097\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 149: 0.040183085948228836\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 150: 0.0401633158326149\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 151: 0.04014444351196289\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 152: 0.04012640193104744\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 153: 0.04010897874832153\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 154: 0.040092285722494125\n",
      "val loss tensor(0.0371)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 155: 0.040076058357954025\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 156: 0.04006019979715347\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 157: 0.04004453867673874\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 158: 0.04002942889928818\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 159: 0.04001491516828537\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 160: 0.04000096395611763\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 161: 0.03998759388923645\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 162: 0.03997430205345154\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 163: 0.03996171057224274\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 164: 0.03994910418987274\n",
      "val loss tensor(0.0370)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 165: 0.03993723541498184\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 166: 0.039925575256347656\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 167: 0.03991397097706795\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 168: 0.03990329056978226\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 169: 0.039892349392175674\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 170: 0.03988182544708252\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 171: 0.03987161070108414\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 172: 0.03985513746738434\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 173: 0.03983491659164429\n",
      "val loss tensor(0.0369)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 174: 0.03981638699769974\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 175: 0.03979814052581787\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 176: 0.03978121280670166\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 177: 0.03976478427648544\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 178: 0.039749402552843094\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 179: 0.03973430022597313\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 180: 0.03972012177109718\n",
      "val loss tensor(0.0368)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 181: 0.03970617800951004\n",
      "val loss tensor(0.0366)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 182: 0.039692845195531845\n",
      "val loss tensor(0.0366)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 183: 0.03968033939599991\n",
      "val loss tensor(0.0365)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 184: 0.03966781497001648\n",
      "val loss tensor(0.0364)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 185: 0.03964666277170181\n",
      "val loss tensor(0.0364)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 186: 0.0396248921751976\n",
      "val loss tensor(0.0364)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 187: 0.039604369550943375\n",
      "val loss tensor(0.0363)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 188: 0.03958461061120033\n",
      "val loss tensor(0.0363)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 189: 0.03956630080938339\n",
      "val loss tensor(0.0363)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 190: 0.039548978209495544\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 191: 0.03953227400779724\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 192: 0.039516348391771317\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 193: 0.03950135037302971\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 194: 0.03948691859841347\n",
      "val loss tensor(0.0362)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 195: 0.039473388344049454\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 196: 0.039460111409425735\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 197: 0.03944764286279678\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 198: 0.03943556547164917\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n",
      "Iteration 199: 0.039423707872629166\n",
      "val loss tensor(0.0361)\n",
      "f1 w/ 0.5 threshold 0.7960179928064651\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = ContextLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, val_pred\n",
    "\n",
    "\n",
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 10000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = val_pred\n",
    "        torch.save(model.state_dict(), \"best_ContextLNN.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- The best f1 is 0.7998889248676659 w/ naive threshold 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/lnn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- After tuning, the best f1 is 0.8415493457427705 w/ threshold 0.993993993993994\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-423ef74a1fc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbestModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContextLNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_ContextLNN.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/enhanced_amr/lnn/examples/entity/RuleLNN_nway.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, arity, slack)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# self.sim_disjunction_or_ops = nn.ModuleList([or_lukasiewicz(alpha, arity, slack) for i in range(3)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# self.predicate_and_ops = nn.ModuleList([and_lukasiewicz(alpha, arity, slack) for i in range(2)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_disjunction_or\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mor_lukasiewicz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicate_and\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mand_lukasiewicz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/enhanced_amr/lnn/src/meta_rule/lnn_operators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, arity, with_slack)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mand_lukasiewicz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_slack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/enhanced_amr/lnn/src/meta_rule/lnn_operators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, arity, with_slack)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_slack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwith_slack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdd_lnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_slack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#differentiable clamping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/enhanced_amr/lnn/src/meta_rule/cdd_interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, arity, with_slack)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_constraints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwith_slack\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_constraints_with_slacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mis_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrays_and_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_v_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/enhanced_amr/lnn/src/meta_rule/cdd_interface.py\u001b[0m in \u001b[0;36mget_v_representation\u001b[0;34m(self, A, b)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m#constructs the linear inequality constraints. Non-slack version.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# tune on val set\n",
    "\n",
    "print(\"Val -- The best f1 is {} w/ naive threshold 0.5\".format(best_val_f1))\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=1000):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))\n",
    "\n",
    "\n",
    "bestModel = ContextLNN(0.9, 2, False)\n",
    "bestModel.load_state_dict(torch.load(\"best_ContextLNN.pt\"))\n",
    "bestModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = bestModel(x_test, m_labels_test)\n",
    "    test_pred = test_pred >= best_tuned_threshold\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='macro')\n",
    "    print(\"Test -- f1 is {} w/ threshold {}\".format(f1, best_tuned_threshold))\n",
    "    print(\"prec, recall, f1\", prec, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(5.8912, grad_fn=<SelectBackward>), tensor([12.1745,  8.3585], grad_fn=<SliceBackward>))\n",
      "(tensor(8.8607, grad_fn=<SelectBackward>), tensor([19.9964, 11.0276], grad_fn=<SliceBackward>))\n",
      "(tensor(7.7926, grad_fn=<SelectBackward>), tensor([17.1775,  9.7577], grad_fn=<SliceBackward>))\n",
      "(tensor(4.7304, grad_fn=<SelectBackward>), tensor([8.7144, 7.7403], grad_fn=<SliceBackward>))\n",
      "=========predicate_and_ops=========\n",
      "(tensor(5.3318, grad_fn=<SelectBackward>), tensor([15.9246,  6.5601], grad_fn=<SliceBackward>))\n",
      "(tensor(1.8147, grad_fn=<SelectBackward>), tensor([2.9074, 2.0872], grad_fn=<SliceBackward>))\n"
     ]
    }
   ],
   "source": [
    "for name, mod in bestModel.named_children():\n",
    "    print(\"========={}=========\".format(name))\n",
    "    if name == 'sim_disjunction_or_ops':\n",
    "        for each_op in mod:\n",
    "            print(each_op.AND.cdd())\n",
    "    elif name == 'predicate_and_ops':\n",
    "        for each_op in mod:\n",
    "            print(each_op.cdd())\n",
    "    else:\n",
    "        print(mod, mod.cdd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.6863e-03],\n",
      "        [3.4976e-04],\n",
      "        [1.0000e+00],\n",
      "        ...,\n",
      "        [3.7682e-04],\n",
      "        [3.5048e-04],\n",
      "        [3.5691e-04]], grad_fn=<RsubBackward1>)\n",
      "Iteration 0: 0.9573822617530823\n",
      "val loss tensor(0.8644)\n",
      "f1 w/ 0.5 threshold 0.5389048163593573\n",
      "Iteration 1: 0.7983121871948242\n",
      "val loss tensor(0.7087)\n",
      "f1 w/ 0.5 threshold 0.5500277705571773\n",
      "Iteration 2: 0.6616075038909912\n",
      "val loss tensor(0.5831)\n",
      "f1 w/ 0.5 threshold 0.5611543117468767\n",
      "Iteration 3: 0.5385414958000183\n",
      "val loss tensor(0.4938)\n",
      "f1 w/ 0.5 threshold 0.5713164135102224\n",
      "Iteration 4: 0.45025917887687683\n",
      "val loss tensor(0.4145)\n",
      "f1 w/ 0.5 threshold 0.5832352587535825\n",
      "Iteration 5: 0.3785131275653839\n",
      "val loss tensor(0.3597)\n",
      "f1 w/ 0.5 threshold 0.5946052934809039\n",
      "Iteration 6: 0.3310108780860901\n",
      "val loss tensor(0.3109)\n",
      "f1 w/ 0.5 threshold 0.6049645533978529\n",
      "Iteration 7: 0.2867151200771332\n",
      "val loss tensor(0.2659)\n",
      "f1 w/ 0.5 threshold 0.6171883180043028\n",
      "Iteration 8: 0.25164327025413513\n",
      "val loss tensor(0.2293)\n",
      "f1 w/ 0.5 threshold 0.6294995328918541\n",
      "Iteration 9: 0.220547154545784\n",
      "val loss tensor(0.1972)\n",
      "f1 w/ 0.5 threshold 0.6460581903307537\n",
      "Iteration 10: 0.19456565380096436\n",
      "val loss tensor(0.1765)\n",
      "f1 w/ 0.5 threshold 0.656312603798053\n",
      "Iteration 11: 0.17733366787433624\n",
      "val loss tensor(0.1556)\n",
      "f1 w/ 0.5 threshold 0.6704213487052771\n",
      "Iteration 12: 0.16357427835464478\n",
      "val loss tensor(0.1433)\n",
      "f1 w/ 0.5 threshold 0.6797538720201313\n",
      "Iteration 13: 0.15118233859539032\n",
      "val loss tensor(0.1338)\n",
      "f1 w/ 0.5 threshold 0.6825929022563186\n",
      "Iteration 14: 0.14243221282958984\n",
      "val loss tensor(0.1251)\n",
      "f1 w/ 0.5 threshold 0.6932097317622531\n",
      "Iteration 15: 0.1357273906469345\n",
      "val loss tensor(0.1165)\n",
      "f1 w/ 0.5 threshold 0.70502583002583\n",
      "Iteration 16: 0.12959443032741547\n",
      "val loss tensor(0.1104)\n",
      "f1 w/ 0.5 threshold 0.7086520107101221\n",
      "Iteration 17: 0.12384503334760666\n",
      "val loss tensor(0.1069)\n",
      "f1 w/ 0.5 threshold 0.7086520107101221\n",
      "Iteration 18: 0.11996263265609741\n",
      "val loss tensor(0.1017)\n",
      "f1 w/ 0.5 threshold 0.7105103577470134\n",
      "Iteration 19: 0.11568249762058258\n",
      "val loss tensor(0.0974)\n",
      "f1 w/ 0.5 threshold 0.7202869958181574\n",
      "Iteration 20: 0.11134393513202667\n",
      "val loss tensor(0.0955)\n",
      "f1 w/ 0.5 threshold 0.7223454216183892\n",
      "Iteration 21: 0.10840903222560883\n",
      "val loss tensor(0.0932)\n",
      "f1 w/ 0.5 threshold 0.7223454216183892\n",
      "Iteration 22: 0.10485221445560455\n",
      "val loss tensor(0.0908)\n",
      "f1 w/ 0.5 threshold 0.7265729402400928\n",
      "Iteration 23: 0.10254904627799988\n",
      "val loss tensor(0.0893)\n",
      "f1 w/ 0.5 threshold 0.7265729402400928\n",
      "Iteration 24: 0.10086236149072647\n",
      "val loss tensor(0.0874)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 25: 0.09865526109933853\n",
      "val loss tensor(0.0862)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 26: 0.09689716249704361\n",
      "val loss tensor(0.0847)\n",
      "f1 w/ 0.5 threshold 0.728744082954013\n",
      "Iteration 27: 0.09454996138811111\n",
      "val loss tensor(0.0832)\n",
      "f1 w/ 0.5 threshold 0.730954911724363\n",
      "Iteration 28: 0.09267484396696091\n",
      "val loss tensor(0.0807)\n",
      "f1 w/ 0.5 threshold 0.7332065499975071\n",
      "Iteration 29: 0.09102161973714828\n",
      "val loss tensor(0.0799)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 30: 0.08946888148784637\n",
      "val loss tensor(0.0787)\n",
      "f1 w/ 0.5 threshold 0.7355001640176689\n",
      "Iteration 31: 0.08827196061611176\n",
      "val loss tensor(0.0779)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 32: 0.08675168454647064\n",
      "val loss tensor(0.0774)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 33: 0.0847276821732521\n",
      "val loss tensor(0.0769)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 34: 0.08372052758932114\n",
      "val loss tensor(0.0760)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 35: 0.08295141905546188\n",
      "val loss tensor(0.0754)\n",
      "f1 w/ 0.5 threshold 0.7402182107306332\n",
      "Iteration 36: 0.08228398114442825\n",
      "val loss tensor(0.0751)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 37: 0.0816381573677063\n",
      "val loss tensor(0.0747)\n",
      "f1 w/ 0.5 threshold 0.742645209027007\n",
      "Iteration 38: 0.08116227388381958\n",
      "val loss tensor(0.0744)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 39: 0.08070433139801025\n",
      "val loss tensor(0.0742)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 40: 0.07984835654497147\n",
      "val loss tensor(0.0739)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 41: 0.0793939083814621\n",
      "val loss tensor(0.0736)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 42: 0.07900255918502808\n",
      "val loss tensor(0.0734)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 43: 0.07865896075963974\n",
      "val loss tensor(0.0732)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 44: 0.07833129167556763\n",
      "val loss tensor(0.0730)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 45: 0.07802063226699829\n",
      "val loss tensor(0.0728)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 46: 0.07764691859483719\n",
      "val loss tensor(0.0724)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 47: 0.07718026638031006\n",
      "val loss tensor(0.0716)\n",
      "f1 w/ 0.5 threshold 0.7451193190255729\n",
      "Iteration 48: 0.07664802670478821\n",
      "val loss tensor(0.0712)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 49: 0.0762629359960556\n",
      "val loss tensor(0.0710)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 50: 0.07594947516918182\n",
      "val loss tensor(0.0708)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 51: 0.0755188912153244\n",
      "val loss tensor(0.0706)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 52: 0.07502666860818863\n",
      "val loss tensor(0.0705)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 53: 0.07459666579961777\n",
      "val loss tensor(0.0703)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 54: 0.07389184832572937\n",
      "val loss tensor(0.0701)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 55: 0.07346701622009277\n",
      "val loss tensor(0.0700)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 56: 0.07292782515287399\n",
      "val loss tensor(0.0698)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 57: 0.07259827107191086\n",
      "val loss tensor(0.0697)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 58: 0.07232951372861862\n",
      "val loss tensor(0.0696)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 59: 0.07205768674612045\n",
      "val loss tensor(0.0694)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 60: 0.07182914763689041\n",
      "val loss tensor(0.0693)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 61: 0.07158388197422028\n",
      "val loss tensor(0.0692)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 62: 0.0713578313589096\n",
      "val loss tensor(0.0690)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 63: 0.07100388407707214\n",
      "val loss tensor(0.0685)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 64: 0.07003523409366608\n",
      "val loss tensor(0.0681)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 65: 0.06932520121335983\n",
      "val loss tensor(0.0677)\n",
      "f1 w/ 0.5 threshold 0.7476419543482362\n",
      "Iteration 66: 0.06890159100294113\n",
      "val loss tensor(0.0675)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 67: 0.06862232089042664\n",
      "val loss tensor(0.0673)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 68: 0.06845177710056305\n",
      "val loss tensor(0.0672)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 69: 0.06822183728218079\n",
      "val loss tensor(0.0670)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 70: 0.06799834966659546\n",
      "val loss tensor(0.0668)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 71: 0.06773873418569565\n",
      "val loss tensor(0.0666)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 72: 0.06756342947483063\n",
      "val loss tensor(0.0665)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 73: 0.06741606444120407\n",
      "val loss tensor(0.0663)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 74: 0.06728531420230865\n",
      "val loss tensor(0.0662)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 75: 0.06716883927583694\n",
      "val loss tensor(0.0661)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 76: 0.06695881485939026\n",
      "val loss tensor(0.0659)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 77: 0.06670712679624557\n",
      "val loss tensor(0.0658)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 78: 0.06643928587436676\n",
      "val loss tensor(0.0657)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 79: 0.06626658141613007\n",
      "val loss tensor(0.0656)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 80: 0.06612136960029602\n",
      "val loss tensor(0.0655)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 81: 0.0659990906715393\n",
      "val loss tensor(0.0654)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 82: 0.06589484959840775\n",
      "val loss tensor(0.0651)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 83: 0.06571509689092636\n",
      "val loss tensor(0.0648)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 84: 0.06558798998594284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0646)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 85: 0.06548275798559189\n",
      "val loss tensor(0.0645)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 86: 0.0653853565454483\n",
      "val loss tensor(0.0644)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 87: 0.06529440730810165\n",
      "val loss tensor(0.0643)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 88: 0.06520730257034302\n",
      "val loss tensor(0.0642)\n",
      "f1 w/ 0.5 threshold 0.7502145857328353\n",
      "Iteration 89: 0.06512439996004105\n",
      "val loss tensor(0.0641)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 90: 0.06504955887794495\n",
      "val loss tensor(0.0641)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 91: 0.0649760365486145\n",
      "val loss tensor(0.0640)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 92: 0.06483722478151321\n",
      "val loss tensor(0.0639)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 93: 0.06472838670015335\n",
      "val loss tensor(0.0639)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 94: 0.06463892012834549\n",
      "val loss tensor(0.0638)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 95: 0.06455688923597336\n",
      "val loss tensor(0.0638)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 96: 0.06448031961917877\n",
      "val loss tensor(0.0637)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 97: 0.06440705806016922\n",
      "val loss tensor(0.0637)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 98: 0.0643375962972641\n",
      "val loss tensor(0.0636)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 99: 0.06427059322595596\n",
      "val loss tensor(0.0636)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 100: 0.06420588493347168\n",
      "val loss tensor(0.0635)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 101: 0.064143106341362\n",
      "val loss tensor(0.0635)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 102: 0.06402061134576797\n",
      "val loss tensor(0.0634)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 103: 0.0639280378818512\n",
      "val loss tensor(0.0634)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 104: 0.06385088711977005\n",
      "val loss tensor(0.0633)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 105: 0.06378060579299927\n",
      "val loss tensor(0.0633)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 106: 0.06371445953845978\n",
      "val loss tensor(0.0632)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 107: 0.06358391791582108\n",
      "val loss tensor(0.0632)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 108: 0.06343980133533478\n",
      "val loss tensor(0.0631)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 109: 0.06330463290214539\n",
      "val loss tensor(0.0630)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 110: 0.06320517510175705\n",
      "val loss tensor(0.0630)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 111: 0.06312074512243271\n",
      "val loss tensor(0.0629)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 112: 0.063046894967556\n",
      "val loss tensor(0.0629)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 113: 0.06297903507947922\n",
      "val loss tensor(0.0628)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 114: 0.06291625648736954\n",
      "val loss tensor(0.0628)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 115: 0.06285570561885834\n",
      "val loss tensor(0.0627)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 116: 0.06280048191547394\n",
      "val loss tensor(0.0627)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 117: 0.06274885684251785\n",
      "val loss tensor(0.0626)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 118: 0.06270213425159454\n",
      "val loss tensor(0.0626)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 119: 0.0626557394862175\n",
      "val loss tensor(0.0625)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 120: 0.06261108070611954\n",
      "val loss tensor(0.0625)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 121: 0.0625675618648529\n",
      "val loss tensor(0.0624)\n",
      "f1 w/ 0.5 threshold 0.7528387439472204\n",
      "Iteration 122: 0.06252395361661911\n",
      "val loss tensor(0.0624)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 123: 0.06248168647289276\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 124: 0.06244070827960968\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 125: 0.062399618327617645\n",
      "val loss tensor(0.0623)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 126: 0.06235966086387634\n",
      "val loss tensor(0.0622)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 127: 0.06232014670968056\n",
      "val loss tensor(0.0622)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 128: 0.062281761318445206\n",
      "val loss tensor(0.0621)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 129: 0.06224387139081955\n",
      "val loss tensor(0.0621)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 130: 0.062206242233514786\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 131: 0.06216920167207718\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 132: 0.06213327497243881\n",
      "val loss tensor(0.0620)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 133: 0.06209752708673477\n",
      "val loss tensor(0.0619)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 134: 0.06206225976347923\n",
      "val loss tensor(0.0619)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 135: 0.062021106481552124\n",
      "val loss tensor(0.0618)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 136: 0.06191370263695717\n",
      "val loss tensor(0.0618)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 137: 0.06184849515557289\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 138: 0.06179441884160042\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 139: 0.061745792627334595\n",
      "val loss tensor(0.0617)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 140: 0.06170079484581947\n",
      "val loss tensor(0.0616)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 141: 0.06165821850299835\n",
      "val loss tensor(0.0616)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 142: 0.06161757931113243\n",
      "val loss tensor(0.0615)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 143: 0.06157737970352173\n",
      "val loss tensor(0.0615)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 144: 0.061534181237220764\n",
      "val loss tensor(0.0614)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 145: 0.061480652540922165\n",
      "val loss tensor(0.0614)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 146: 0.06142852082848549\n",
      "val loss tensor(0.0614)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 147: 0.06137336045503616\n",
      "val loss tensor(0.0612)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 148: 0.06129705533385277\n",
      "val loss tensor(0.0610)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 149: 0.06113814562559128\n",
      "val loss tensor(0.0608)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 150: 0.061001256108284\n",
      "val loss tensor(0.0606)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 151: 0.060761064291000366\n",
      "val loss tensor(0.0604)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 152: 0.06063036993145943\n",
      "val loss tensor(0.0596)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 153: 0.060562700033187866\n",
      "val loss tensor(0.0595)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 154: 0.06050149351358414\n",
      "val loss tensor(0.0594)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 155: 0.0604444183409214\n",
      "val loss tensor(0.0593)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 156: 0.06039157882332802\n",
      "val loss tensor(0.0592)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 157: 0.06034117937088013\n",
      "val loss tensor(0.0592)\n",
      "f1 w/ 0.5 threshold 0.7555160228835844\n",
      "Iteration 158: 0.06029430404305458\n",
      "val loss tensor(0.0591)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 159: 0.060198571532964706\n",
      "val loss tensor(0.0590)\n",
      "f1 w/ 0.5 threshold 0.7582480828461877\n",
      "Iteration 160: 0.06009576842188835\n",
      "val loss tensor(0.0589)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 161: 0.060009051114320755\n",
      "val loss tensor(0.0589)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 162: 0.05992720276117325\n",
      "val loss tensor(0.0588)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 163: 0.05985967442393303\n",
      "val loss tensor(0.0585)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 164: 0.0598125122487545\n",
      "val loss tensor(0.0583)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 165: 0.059768196195364\n",
      "val loss tensor(0.0582)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 166: 0.05972554162144661\n",
      "val loss tensor(0.0581)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 167: 0.05968529358506203\n",
      "val loss tensor(0.0581)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 168: 0.05964692682027817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(0.0580)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 169: 0.059609394520521164\n",
      "val loss tensor(0.0579)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 170: 0.05957398936152458\n",
      "val loss tensor(0.0579)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 171: 0.0595400333404541\n",
      "val loss tensor(0.0578)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 172: 0.059509534388780594\n",
      "val loss tensor(0.0578)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 173: 0.059479229152202606\n",
      "val loss tensor(0.0577)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 174: 0.05945027247071266\n",
      "val loss tensor(0.0577)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 175: 0.0594213642179966\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 176: 0.05939330905675888\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 177: 0.05936622619628906\n",
      "val loss tensor(0.0576)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 178: 0.05933916196227074\n",
      "val loss tensor(0.0575)\n",
      "f1 w/ 0.5 threshold 0.7610366540467318\n",
      "Iteration 179: 0.05931293964385986\n",
      "val loss tensor(0.0575)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 180: 0.05928676202893257\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 181: 0.05926106497645378\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 182: 0.05923580005764961\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 183: 0.05920995771884918\n",
      "val loss tensor(0.0574)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 184: 0.0591849610209465\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 185: 0.05916082113981247\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 186: 0.05913598835468292\n",
      "val loss tensor(0.0573)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 187: 0.05911250784993172\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 188: 0.05908971279859543\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 189: 0.05906708911061287\n",
      "val loss tensor(0.0572)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 190: 0.059044282883405685\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 191: 0.059022217988967896\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 192: 0.059000566601753235\n",
      "val loss tensor(0.0571)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 193: 0.05897759646177292\n",
      "val loss tensor(0.0570)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 194: 0.05894763767719269\n",
      "val loss tensor(0.0570)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 195: 0.058852002024650574\n",
      "val loss tensor(0.0570)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 196: 0.05866466462612152\n",
      "val loss tensor(0.0569)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 197: 0.058525096625089645\n",
      "val loss tensor(0.0565)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 198: 0.058421872556209564\n",
      "val loss tensor(0.0564)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n",
      "Iteration 199: 0.058340393006801605\n",
      "val loss tensor(0.0563)\n",
      "f1 w/ 0.5 threshold 0.7638835403228394\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model = ComplexRuleLNN(0.8, 2, False)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "def evaluate(eval_model, x_eval, y_eval, m_labels_eval):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = eval_model(x_eval, m_labels_eval)\n",
    "        loss = loss_fn(val_pred, y_eval)\n",
    "        val_pred_ = val_pred > 0.5\n",
    "        print(\"val loss\", loss)\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(y_eval, val_pred_, average='macro')\n",
    "        print(\"f1 w/ 0.5 threshold\", f1)\n",
    "    return loss, f1, val_pred\n",
    "\n",
    "\n",
    "best_pred = None\n",
    "best_val_f1, best_val_loss = 0, 10000\n",
    "\n",
    "for iter in range(200):\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x_train, m_labels_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_f1, val_pred = evaluate(model, x_val, y_val, m_labels_val)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_f1 = val_f1\n",
    "        best_pred = val_pred\n",
    "        torch.save(model.state_dict(), \"best_complex.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val -- The best f1 is 0.7638835403228394 w/ naive threshold 0.5\n",
      "Val -- After tuning, the best f1 is 0.7856135050703569 w/ threshold 0.992992992992993\n",
      "Test -- f1 is 0.8164090967900519 w/ threshold 0.992992992992993\n"
     ]
    }
   ],
   "source": [
    "# tune on val set\n",
    "\n",
    "print(\"Val -- The best f1 is {} w/ naive threshold 0.5\".format(best_val_f1))\n",
    "\n",
    "best_tuned_threshold = 0.5\n",
    "best_tuned_f1 = best_val_f1\n",
    "\n",
    "for threshold_ in np.linspace(0.0, 1.0, num=1000):\n",
    "    y_val_preds = best_pred >= threshold_\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_preds, average='macro')\n",
    "    if f1 > best_tuned_f1:\n",
    "        best_tuned_threshold = threshold_\n",
    "        best_tuned_f1 = f1\n",
    "print(\"Val -- After tuning, the best f1 is {} w/ threshold {}\".format(best_tuned_f1, best_tuned_threshold))\n",
    "\n",
    "\n",
    "bestModel = ComplexRuleLNN(0.8, 2, False)\n",
    "bestModel.load_state_dict(torch.load(\"best_complex.pt\"))\n",
    "bestModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = bestModel(x_test, m_labels_test)\n",
    "    test_pred = test_pred >= best_tuned_threshold\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='macro')\n",
    "    print(\"Test -- f1 is {} w/ threshold {}\".format(f1, best_tuned_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(5.1019, grad_fn=<SelectBackward>), tensor([10.4697,  8.0609], grad_fn=<SliceBackward>))\n",
      "(tensor(5.7117, grad_fn=<SelectBackward>), tensor([ 8.2640, 11.7477], grad_fn=<SliceBackward>))\n",
      "(tensor(6.4294, grad_fn=<SelectBackward>), tensor([12.9908,  8.3084], grad_fn=<SliceBackward>))\n",
      "(tensor(3.8584, grad_fn=<SelectBackward>), tensor([6.8998, 6.3426], grad_fn=<SliceBackward>))\n",
      "=========predicate_and=========\n",
      "predicate_and (tensor(4.6245, grad_fn=<SelectBackward>), tensor([13.1790,  5.6998], grad_fn=<SliceBackward>))\n",
      "=========sim_disjunction_or_ops=========\n",
      "(tensor(6.3645, grad_fn=<SelectBackward>), tensor([13.3272,  9.0812], grad_fn=<SliceBackward>))\n",
      "(tensor(6.9162, grad_fn=<SelectBackward>), tensor([14.1029,  9.5137], grad_fn=<SliceBackward>))\n",
      "(tensor(6.0238, grad_fn=<SelectBackward>), tensor([13.0749,  7.8272], grad_fn=<SliceBackward>))\n",
      "(tensor(4.1672, grad_fn=<SelectBackward>), tensor([6.7004, 7.7094], grad_fn=<SliceBackward>))\n",
      "=========predicate_and_ops=========\n",
      "(tensor(4.4623, grad_fn=<SelectBackward>), tensor([12.3697,  5.6580], grad_fn=<SliceBackward>))\n",
      "(tensor(4.3978, grad_fn=<SelectBackward>), tensor([12.1416,  5.4737], grad_fn=<SliceBackward>))\n",
      "=========0=========\n",
      "0 (tensor(1.5519, grad_fn=<SelectBackward>), tensor([1.7932, 1.7873], grad_fn=<SliceBackward>))\n",
      "=========1=========\n",
      "1 (tensor(4.0893, grad_fn=<SelectBackward>), tensor([6.6543, 6.9582], grad_fn=<SliceBackward>))\n"
     ]
    }
   ],
   "source": [
    "for name1, mod1 in bestModel.named_children():\n",
    "    for name, mod in mod1.named_children():\n",
    "        print(\"========={}=========\".format(name))\n",
    "        if name == 'sim_disjunction_or_ops':\n",
    "            for each_op in mod:\n",
    "                print(each_op.AND.cdd())\n",
    "        elif name == 'predicate_and_ops':\n",
    "            for each_op in mod:\n",
    "                print(each_op.cdd())\n",
    "        elif 'and' in name:\n",
    "            print(name, mod.cdd())\n",
    "        else:\n",
    "            print(name, mod.AND.cdd())\n",
    "#     if 'and' in name1:\n",
    "#         print(name1, mod1.cdd())\n",
    "#     elif 'or' in name1:\n",
    "#         print(name1, mod1.AND.cdd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pureNameRule.sim_disjunction_or_ops.0.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.6911]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.0.AND.cdd.mu Parameter containing:\n",
      "tensor([[1.5432, 0.1545, 0.8296]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.1.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.4595]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.1.AND.cdd.mu Parameter containing:\n",
      "tensor([[-0.3108,  1.7886,  1.6416]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.2.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.6444]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.2.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 1.9484, -1.4607,  2.6722]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.3.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.1029]], requires_grad=True)\n",
      "pureNameRule.sim_disjunction_or_ops.3.AND.cdd.mu Parameter containing:\n",
      "tensor([[0.4294, 0.0292, 0.2391]], requires_grad=True)\n",
      "pureNameRule.predicate_and.cdd.gamma Parameter containing:\n",
      "tensor([[0.0540]], requires_grad=True)\n",
      "pureNameRule.predicate_and.cdd.mu Parameter containing:\n",
      "tensor([[ 3.0113, -2.6593, -2.2786]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.0.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.5774]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.0.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 2.1371, -0.3097,  2.0438]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.1.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.3351]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.1.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 2.1755, -0.5724,  2.7221]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.2.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.7987]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.2.AND.cdd.mu Parameter containing:\n",
      "tensor([[ 2.2144, -1.4073,  1.9542]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.3.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.1124]], requires_grad=True)\n",
      "contextRule.sim_disjunction_or_ops.3.AND.cdd.mu Parameter containing:\n",
      "tensor([[0.0068, 0.6955, 0.4942]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.0.cdd.gamma Parameter containing:\n",
      "tensor([[0.6384]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.0.cdd.mu Parameter containing:\n",
      "tensor([[ 2.7550, -1.9580, -2.1179]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.1.cdd.gamma Parameter containing:\n",
      "tensor([[0.9517]], requires_grad=True)\n",
      "contextRule.predicate_and_ops.1.cdd.mu Parameter containing:\n",
      "tensor([[ 2.6922, -2.3561, -1.8245]], requires_grad=True)\n",
      "rule_or_ops.0.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.3596]], requires_grad=True)\n",
      "rule_or_ops.0.AND.cdd.mu Parameter containing:\n",
      "tensor([[-3.1657, -3.2260, -2.5996]], requires_grad=True)\n",
      "rule_or_ops.1.AND.cdd.gamma Parameter containing:\n",
      "tensor([[0.7368]], requires_grad=True)\n",
      "rule_or_ops.1.AND.cdd.mu Parameter containing:\n",
      "tensor([[0.0473, 0.2724, 0.7452]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in bestModel.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (post-training): 4.196691513061523\n",
      "argument weights (post-training): tensor([7.9233, 7.2595])\n",
      "beta (post-training): 3.902745246887207\n",
      "argument weights (post-training): tensor([6.9828, 6.3890])\n",
      "beta (post-training): 4.169642925262451\n",
      "argument weights (post-training): tensor([7.6434, 7.0587])\n",
      "beta (post-training): 4.3188300132751465\n",
      "argument weights (post-training): tensor([7.2704, 8.2539])\n",
      "beta (post-training): 4.196691513061523\n",
      "argument weights (post-training): tensor([7.9233, 7.2595])\n",
      "beta (post-training): 3.902745246887207\n",
      "argument weights (post-training): tensor([6.9828, 6.3890])\n",
      "beta (post-training): 4.169642925262451\n",
      "argument weights (post-training): tensor([7.6434, 7.0587])\n",
      "beta (post-training): 4.3188300132751465\n",
      "argument weights (post-training): tensor([7.2704, 8.2539])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for each_op in bestModel.sim_disjunction_or_ops:\n",
    "    #lets check the LNN conjunction parameters post-training\n",
    "    #do these look different from the pre-training settings?\n",
    "    beta, argument_wts = each_op.AND.cdd()\n",
    "    print(\"beta (post-training): \" + str(beta.item()))\n",
    "    print(\"argument weights (post-training): \" + str(argument_wts.detach()))\n",
    "\n",
    "for each_op in bestModel.sim_disjunction_or_ops:\n",
    "    #lets check the LNN conjunction parameters post-training\n",
    "    #do these look different from the pre-training settings?\n",
    "    beta, argument_wts = each_op.AND.cdd()\n",
    "    print(\"beta (post-training): \" + str(beta.item()))\n",
    "    print(\"argument weights (post-training): \" + str(argument_wts.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for XOR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xorLNN(nn.Module):\n",
    "    def __init__(self, alpha, arity, slack):\n",
    "        super(xorLNN, self).__init__()\n",
    "        self.op_and1 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_and2 = and_lukasiewicz(alpha, arity, slack)\n",
    "        self.op_or = or_lukasiewicz(alpha, arity, slack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = x[:,0].view(-1,1)\n",
    "        print(x0)\n",
    "        x1 = x[:,1].view(-1,1)\n",
    "        print(x1)\n",
    "        print(torch.cat((x0, negation(x1)), 1))\n",
    "        yhat = self.op_or(torch.cat((self.op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            self.op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.6349e-04],\n",
       "        [9.9932e-01],\n",
       "        [9.9967e-01],\n",
       "        [4.6349e-04]], grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xorLNN(0.8, 2, False)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.00041395798325538635\n",
      "Iteration 1: 0.0003413597878534347\n",
      "Iteration 2: 0.00027990160742774606\n",
      "Iteration 3: 0.00022847841319162399\n",
      "Iteration 4: 0.00018598556926008314\n",
      "Iteration 5: 0.00015122962940949947\n",
      "Iteration 6: 0.0001230025663971901\n",
      "Iteration 7: 0.00010024568473454565\n",
      "Iteration 8: 8.197502756956965e-05\n",
      "Iteration 9: 6.734087219228968e-05\n",
      "Iteration 10: 5.5627755500609055e-05\n",
      "Iteration 11: 4.625439760275185e-05\n",
      "Iteration 12: 3.872895103995688e-05\n",
      "Iteration 13: 3.267884312663227e-05\n",
      "Iteration 14: 2.7761290766648017e-05\n",
      "Iteration 15: 2.3767666789353825e-05\n",
      "Iteration 16: 2.051913361356128e-05\n",
      "Iteration 17: 1.7821967048803344e-05\n",
      "Iteration 18: 1.5646355677745305e-05\n",
      "Iteration 19: 1.3783679605694488e-05\n",
      "Iteration 20: 1.2263739336049184e-05\n",
      "Iteration 21: 1.0967321941279806e-05\n",
      "Iteration 22: 9.89442560239695e-06\n",
      "Iteration 23: 8.970543603936676e-06\n",
      "Iteration 24: 8.165872714016587e-06\n",
      "Iteration 25: 7.480413842131384e-06\n",
      "Iteration 26: 6.899264008097816e-06\n",
      "Iteration 27: 6.407521595974686e-06\n",
      "Iteration 28: 5.975385647616349e-06\n",
      "Iteration 29: 5.587952728092205e-06\n",
      "Iteration 30: 5.260125362838153e-06\n",
      "Iteration 31: 4.932297997584101e-06\n",
      "Iteration 32: 4.678976893046638e-06\n",
      "Iteration 33: 4.395853011374129e-06\n",
      "Iteration 34: 4.231939783494454e-06\n",
      "Iteration 35: 4.023322162538534e-06\n",
      "Iteration 36: 3.829606612271164e-06\n",
      "Iteration 37: 3.6805943182116607e-06\n",
      "Iteration 38: 3.561384346539853e-06\n",
      "Iteration 39: 3.4421748296153964e-06\n",
      "Iteration 40: 3.322964857943589e-06\n",
      "Iteration 41: 3.21865650221298e-06\n",
      "Iteration 42: 3.1292493076762185e-06\n",
      "Iteration 43: 3.024940724571934e-06\n",
      "Iteration 44: 2.920632368841325e-06\n",
      "Iteration 45: 2.831224946930888e-06\n",
      "Iteration 46: 2.786521463349345e-06\n",
      "Iteration 47: 2.7120154300064314e-06\n",
      "Iteration 48: 2.667311719051213e-06\n",
      "Iteration 49: 2.5928056857082993e-06\n",
      "Iteration 50: 2.563003363320604e-06\n",
      "Iteration 51: 2.4735961687838426e-06\n",
      "Iteration 52: 2.4437938463961473e-06\n",
      "Iteration 53: 2.3692878130532335e-06\n",
      "Iteration 54: 2.354386651859386e-06\n",
      "Iteration 55: 2.324584102098015e-06\n",
      "Iteration 56: 2.2500780687551014e-06\n",
      "Iteration 57: 2.2500780687551014e-06\n",
      "Iteration 58: 2.2351769075612538e-06\n",
      "Iteration 59: 2.145769485650817e-06\n",
      "Iteration 60: 2.1308683244569693e-06\n",
      "Iteration 61: 2.1308683244569693e-06\n",
      "Iteration 62: 2.056362518487731e-06\n",
      "Iteration 63: 2.0265601961000357e-06\n",
      "Iteration 64: 2.0265601961000357e-06\n",
      "Iteration 65: 2.0116588075325126e-06\n",
      "Iteration 66: 1.9371530015632743e-06\n",
      "Iteration 67: 1.9371530015632743e-06\n",
      "Iteration 68: 1.907350679175579e-06\n",
      "Iteration 69: 1.907350679175579e-06\n",
      "Iteration 70: 1.8924494042948936e-06\n",
      "Iteration 71: 1.8328446458326653e-06\n",
      "Iteration 72: 1.8179434846388176e-06\n",
      "Iteration 73: 1.80304232344497e-06\n",
      "Iteration 74: 1.7881411622511223e-06\n",
      "Iteration 75: 1.7881411622511223e-06\n",
      "Iteration 76: 1.7136352425950463e-06\n",
      "Iteration 77: 1.7136352425950463e-06\n",
      "Iteration 78: 1.7136352425950463e-06\n",
      "Iteration 79: 1.698733967714361e-06\n",
      "Iteration 80: 1.6838328065205133e-06\n",
      "Iteration 81: 1.6689316453266656e-06\n",
      "Iteration 82: 1.6093267731775995e-06\n",
      "Iteration 83: 1.6093267731775995e-06\n",
      "Iteration 84: 1.5944256119837519e-06\n",
      "Iteration 85: 1.5944256119837519e-06\n",
      "Iteration 86: 1.5944256119837519e-06\n",
      "Iteration 87: 1.5795244507899042e-06\n",
      "Iteration 88: 1.5646232895960566e-06\n",
      "Iteration 89: 1.4901173699399806e-06\n",
      "Iteration 90: 1.4901173699399806e-06\n",
      "Iteration 91: 1.4901173699399806e-06\n",
      "Iteration 92: 1.475216208746133e-06\n",
      "Iteration 93: 1.475216208746133e-06\n",
      "Iteration 94: 1.475216208746133e-06\n",
      "Iteration 95: 1.475216208746133e-06\n",
      "Iteration 96: 1.4454137726715999e-06\n",
      "Iteration 97: 1.3858090142093715e-06\n",
      "Iteration 98: 1.3858090142093715e-06\n",
      "Iteration 99: 1.3709078530155239e-06\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.0005070384358987212\n",
      "Iteration 1: 0.0004226093296892941\n",
      "Iteration 2: 0.00035011349245905876\n",
      "Iteration 3: 0.0002885941066779196\n",
      "Iteration 4: 0.0002370504371356219\n",
      "Iteration 5: 0.00019430331303738058\n",
      "Iteration 6: 0.00015917410200927407\n",
      "Iteration 7: 0.00013051435234956443\n",
      "Iteration 8: 0.0001072355080395937\n",
      "Iteration 9: 8.839827933115885e-05\n",
      "Iteration 10: 7.324236503336579e-05\n",
      "Iteration 11: 6.0992642829660326e-05\n",
      "Iteration 12: 5.106781463837251e-05\n",
      "Iteration 13: 4.3065447243861854e-05\n",
      "Iteration 14: 3.6598037695512176e-05\n",
      "Iteration 15: 3.130791810690425e-05\n",
      "Iteration 16: 2.6941725081996992e-05\n",
      "Iteration 17: 2.3410046196659096e-05\n",
      "Iteration 18: 2.048934402409941e-05\n",
      "Iteration 19: 1.8030596038443036e-05\n",
      "Iteration 20: 1.6003998098312877e-05\n",
      "Iteration 21: 1.4275432477006689e-05\n",
      "Iteration 22: 1.2785292710759677e-05\n",
      "Iteration 23: 1.1593181625357829e-05\n",
      "Iteration 24: 1.0550087608862668e-05\n",
      "Iteration 25: 9.641104952606838e-06\n",
      "Iteration 26: 8.851335223880596e-06\n",
      "Iteration 27: 8.18077660369454e-06\n",
      "Iteration 28: 7.59962586016627e-06\n",
      "Iteration 29: 7.107882993295789e-06\n",
      "Iteration 30: 6.660844519501552e-06\n",
      "Iteration 31: 6.243609277589712e-06\n",
      "Iteration 32: 5.900879841647111e-06\n",
      "Iteration 33: 5.573052021645708e-06\n",
      "Iteration 34: 5.3048297559143975e-06\n",
      "Iteration 35: 5.08131097376463e-06\n",
      "Iteration 36: 4.842891030421015e-06\n",
      "Iteration 37: 4.634273409465095e-06\n",
      "Iteration 38: 4.440557404450374e-06\n",
      "Iteration 39: 4.291544883017195e-06\n",
      "Iteration 40: 4.097828878002474e-06\n",
      "Iteration 41: 3.978619133704342e-06\n",
      "Iteration 42: 3.859408934658859e-06\n",
      "Iteration 43: 3.7551008063019253e-06\n",
      "Iteration 44: 3.635891062003793e-06\n",
      "Iteration 45: 3.5315824788995087e-06\n",
      "Iteration 46: 3.4272738957952242e-06\n",
      "Iteration 47: 3.382570184840006e-06\n",
      "Iteration 48: 3.2931629903032444e-06\n",
      "Iteration 49: 3.2037555683928076e-06\n",
      "Iteration 50: 3.1739532460051123e-06\n",
      "Iteration 51: 3.0845458240946755e-06\n",
      "Iteration 52: 2.995138629557914e-06\n",
      "Iteration 53: 2.9653360797965433e-06\n",
      "Iteration 54: 2.890830273827305e-06\n",
      "Iteration 55: 2.8461265628720867e-06\n",
      "Iteration 56: 2.771620529529173e-06\n",
      "Iteration 57: 2.75671914096165e-06\n",
      "Iteration 58: 2.741817979767802e-06\n",
      "Iteration 59: 2.652410785231041e-06\n",
      "Iteration 60: 2.637509624037193e-06\n",
      "Iteration 61: 2.563003363320604e-06\n",
      "Iteration 62: 2.5481022021267563e-06\n",
      "Iteration 63: 2.518299879739061e-06\n",
      "Iteration 64: 2.4437938463961473e-06\n",
      "Iteration 65: 2.4288926852022996e-06\n",
      "Iteration 66: 2.4288926852022996e-06\n",
      "Iteration 67: 2.413991524008452e-06\n",
      "Iteration 68: 2.3245843294716906e-06\n",
      "Iteration 69: 2.3096829409041675e-06\n",
      "Iteration 70: 2.3096829409041675e-06\n",
      "Iteration 71: 2.29478177971032e-06\n",
      "Iteration 72: 2.220275746367406e-06\n",
      "Iteration 73: 2.220275746367406e-06\n",
      "Iteration 74: 2.2053745851735584e-06\n",
      "Iteration 75: 2.1904734239797108e-06\n",
      "Iteration 76: 2.1159676180104725e-06\n",
      "Iteration 77: 2.1010662294429494e-06\n",
      "Iteration 78: 2.1010662294429494e-06\n",
      "Iteration 79: 2.0861648408754263e-06\n",
      "Iteration 80: 2.0265601961000357e-06\n",
      "Iteration 81: 2.011659034906188e-06\n",
      "Iteration 82: 1.9967578737123404e-06\n",
      "Iteration 83: 1.9818567125184927e-06\n",
      "Iteration 84: 1.9818567125184927e-06\n",
      "Iteration 85: 1.966955551324645e-06\n",
      "Iteration 86: 1.9073504518019035e-06\n",
      "Iteration 87: 1.8924494042948936e-06\n",
      "Iteration 88: 1.8924494042948936e-06\n",
      "Iteration 89: 1.877548243101046e-06\n",
      "Iteration 90: 1.877548243101046e-06\n",
      "Iteration 91: 1.7881411622511223e-06\n",
      "Iteration 92: 1.7881411622511223e-06\n",
      "Iteration 93: 1.7881411622511223e-06\n",
      "Iteration 94: 1.7732397736835992e-06\n",
      "Iteration 95: 1.7732397736835992e-06\n",
      "Iteration 96: 1.7583386124897515e-06\n",
      "Iteration 97: 1.698733967714361e-06\n",
      "Iteration 98: 1.698733967714361e-06\n",
      "Iteration 99: 1.6838328065205133e-06\n",
      "------- Checking outputs (left) vs ground truth (right): -----\n",
      "tensor([[1.9073e-06, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00],\n",
      "        [1.9073e-06, 0.0000e+00]])\n",
      "--------------- LNN Parameters (post-training) ---------------\n",
      "OR (beta, argument weights): 9.477 [19.059 17.79 ]\n",
      "AND1 (beta, argument weights): 7.542 [14.488 15.328]\n",
      "AND2 (beta, argument weights): 7.761 [14.913 15.211]\n"
     ]
    }
   ],
   "source": [
    "#this is a hyperparameter\n",
    "alpha = 0.8\n",
    "\n",
    "op_and1 = and_lukasiewicz(alpha, 2, False)\n",
    "op_and2 = and_lukasiewicz(alpha, 2, False)\n",
    "op_or = or_lukasiewicz(alpha, 2, False)\n",
    "\n",
    "#to train a xor we need its truth table\n",
    "x = torch.from_numpy(np.array([[0, 0], \\\n",
    "                               [0, 1], \\\n",
    "                               [1, 0], \\\n",
    "                               [1, 1]])).float()\n",
    "\n",
    "#the target values for each row in the truth table (xor)\n",
    "y = torch.from_numpy(np.array([[0], \\\n",
    "                               [1], \\\n",
    "                               [1], \\\n",
    "                               [0]])).float()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam([{'params': op_or.parameters()}, \\\n",
    "                        {'params': op_and1.parameters()}, \\\n",
    "                        {'params': op_and2.parameters()}], lr=0.1)\n",
    "\n",
    "for iter in range(100):\n",
    "    op_or.train()\n",
    "    op_and1.train()\n",
    "    op_and2.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x0 = x[:,0].view(-1,1)\n",
    "    x1 = x[:,1].view(-1,1)\n",
    "    yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                            op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    print(\"Iteration \" + str(iter) + \": \" + str(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#check to see output of xor post-training\n",
    "x0 = x[:,0].view(-1,1)\n",
    "x1 = x[:,1].view(-1,1)\n",
    "yhat = op_or(torch.cat((op_and1(torch.cat((x0, negation(x1)), 1)), \\\n",
    "                        op_and2(torch.cat((negation(x0), x1), 1))), 1))\n",
    "check_values = torch.cat((yhat, y), 1)\n",
    "print(\"------- Checking outputs (left) vs ground truth (right): -----\")\n",
    "print(check_values.detach())\n",
    "\n",
    "#LNN parameters: post-training (we have 3 sets of beta, argument weights)\n",
    "print(\"--------------- LNN Parameters (post-training) ---------------\")\n",
    "beta_or, argument_wts_or = op_or.AND.cdd()\n",
    "beta_and1, argument_wts_and1 = op_and1.cdd()\n",
    "beta_and2, argument_wts_and2 = op_and2.cdd()\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"OR (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_or.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_or.detach().numpy()))\n",
    "print(\"AND1 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and1.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and1.detach().numpy()))\n",
    "print(\"AND2 (beta, argument weights): \" \\\n",
    "      + str(np.around(beta_and2.item(), decimals=3)) + \" \" \\\n",
    "      + str(argument_wts_and2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arity should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PureNameLNN(nn.Module):\n",
    "#     def __init__(self, alpha, sim_arity=4, rule_arity=2, slack=None):\n",
    "#         super(PureNameLNN, self).__init__()\n",
    "#         self.threshold = 0.5\n",
    "        \n",
    "#         self.sim_disjunction_or = or_lukasiewicz(alpha, sim_arity, slack)\n",
    "    \n",
    "#     def forward(self, x, mention_labels=None):\n",
    "#         \"\"\"\n",
    "#             x: scores['jw'], scores['jacc'], scores['lev'], scores['spacy'], \n",
    "#                normalized_ref_scores[ref_idx], normalized_ctx_scores[ctx_idx]\n",
    "#         \"\"\"\n",
    "#         yhat = None\n",
    "        \n",
    "#         # RULE 1: lookup predicate\n",
    "#         lookup_features = x[:,5]\n",
    "#         print(\"lookup_features\", lookup_features)\n",
    "        \n",
    "#         # RULE 2: similarity predicate(mention==label AND Jacc(m, lb) AND Lev(m, lb) AND Jaro(m, lb))\n",
    "#         feature_list = []\n",
    "#         # rule 2 (1) mention==label\n",
    "#         mentions = np.array([m[0].lower() for m in mention_labels])\n",
    "#         labels = np.array([m[1].lower() for m in mention_labels])\n",
    "#         exact_match_features = torch.from_numpy(np.array(mentions == labels).astype(float)).float()\n",
    "#         feature_list.append(exact_match_features)\n",
    "#         print(\"exact_match_features\", exact_match_features)\n",
    "        \n",
    "#         # rule 2 (2)-(4) Jaro(m, lb) AND Jacc(m, lb) AND Lev(m, lb))\n",
    "#         sim_features = x[:, 0:3]\n",
    "#         print(sim_features)\n",
    "\n",
    "#         return yhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
