{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../')\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from el_evaluation import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318020, 5)\n",
      "(32536, 5)\n"
     ]
    }
   ],
   "source": [
    "## data formatting\n",
    "\n",
    "df_train = pd.read_csv(\"data/lcquad/lcquad_blink_train.csv\")\n",
    "print(df_train.shape)\n",
    "df_train[\"Mention\"] = df_train.Mention_label.str.split(';').str[0]\n",
    "df_train['QuestionMention'] = df_train.Question + '--' + df_train.Mention\n",
    "df_train = df_train.sort_values(by=['QuestionMention', 'Label'])\n",
    "df_train.to_csv(\"data/lcquad/lcquad_train_sorted.csv\")\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"data/lcquad/lcquad_blink_test.csv\")\n",
    "print(df_test.shape)\n",
    "df_test[\"Mention\"] = df_test.Mention_label.str.split(';').str[0]\n",
    "df_test['QuestionMention'] = df_test.Question + '--' + df_test.Mention\n",
    "df_test = df_test.sort_values(by=['QuestionMention', 'Label'])\n",
    "df_test.to_csv(\"data/lcquad/lcquad_test_sorted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318020, 5)\n",
      "(32536, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Mention_label</th>\n",
       "      <th>Features</th>\n",
       "      <th>Label</th>\n",
       "      <th>Mention</th>\n",
       "      <th>QuestionMention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "      <td>Jean-Marc Germain;Jean-Marc Germain (politician)</td>\n",
       "      <td>[0.9133332967758179, 0.6666666666666666, 0.566...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jean-Marc Germain</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "      <td>Jean-Marc Germain;Jean-Marc Germain (businessman)</td>\n",
       "      <td>[0.9096774458885193, 0.6666666666666666, 0.548...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jean-Marc Germain</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "      <td>Jean-Marc Germain;List of French supercentenar...</td>\n",
       "      <td>[0.5002450942993164, 0.0, 0.25, 0.579229547897...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jean-Marc Germain</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "      <td>Jean-Marc Germain;Jean-Claude Germain</td>\n",
       "      <td>[0.9076986312866211, 0.3333333333333333, 0.736...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jean-Marc Germain</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "      <td>Jean-Marc Germain;2005 French riots</td>\n",
       "      <td>[0.5126050710678101, 0.0, 0.05882352941176472,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jean-Marc Germain</td>\n",
       "      <td>What is the place where the mayor's wife is Je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Question  \\\n",
       "0           0  What is the place where the mayor's wife is Je...   \n",
       "1           1  What is the place where the mayor's wife is Je...   \n",
       "2           2  What is the place where the mayor's wife is Je...   \n",
       "3           3  What is the place where the mayor's wife is Je...   \n",
       "4           4  What is the place where the mayor's wife is Je...   \n",
       "\n",
       "                                       Mention_label  \\\n",
       "0   Jean-Marc Germain;Jean-Marc Germain (politician)   \n",
       "1  Jean-Marc Germain;Jean-Marc Germain (businessman)   \n",
       "2  Jean-Marc Germain;List of French supercentenar...   \n",
       "3              Jean-Marc Germain;Jean-Claude Germain   \n",
       "4                Jean-Marc Germain;2005 French riots   \n",
       "\n",
       "                                            Features  Label  \\\n",
       "0  [0.9133332967758179, 0.6666666666666666, 0.566...      0   \n",
       "1  [0.9096774458885193, 0.6666666666666666, 0.548...      0   \n",
       "2  [0.5002450942993164, 0.0, 0.25, 0.579229547897...      0   \n",
       "3  [0.9076986312866211, 0.3333333333333333, 0.736...      0   \n",
       "4  [0.5126050710678101, 0.0, 0.05882352941176472,...      0   \n",
       "\n",
       "             Mention                                    QuestionMention  \n",
       "0  Jean-Marc Germain  What is the place where the mayor's wife is Je...  \n",
       "1  Jean-Marc Germain  What is the place where the mayor's wife is Je...  \n",
       "2  Jean-Marc Germain  What is the place where the mayor's wife is Je...  \n",
       "3  Jean-Marc Germain  What is the place where the mayor's wife is Je...  \n",
       "4  Jean-Marc Germain  What is the place where the mayor's wife is Je...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data formatting\n",
    "\n",
    "df_train = pd.read_csv(\"data/lcquad/lcquad_blink_train.csv\")\n",
    "print(df_train.shape)\n",
    "df_test = pd.read_csv(\"data/lcquad/lcquad_blink_test.csv\")\n",
    "print(df_test.shape)\n",
    "df_data = pd.concat([df_train, df_test])\n",
    "df_data[\"Mention\"] = df_data.Mention_label.str.split(';').str[0]\n",
    "df_data['QuestionMention'] = df_data.Question + '--' + df_data.Mention\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350556, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = df_data.sort_values(by=['QuestionMention', 'Label'])\n",
    "df_data.to_csv(\"data/lcquad/lcquad_train_test_sorted.csv\")\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== fold num -- 1 ===========\n",
      "Before filtering train set:\n",
      "Train QuestionMention pairs - 276361 Question - 3144\n",
      "After filtering train set:\n",
      "Train QuestionMention pairs - 243332 Question - 2858\n",
      "Test QuestionMention pairs - 74195 Question - 786\n",
      "=========== fold num -- 2 ===========\n",
      "Before filtering train set:\n",
      "Train QuestionMention pairs - 275536 Question - 3144\n",
      "After filtering train set:\n",
      "Train QuestionMention pairs - 242397 Question - 2848\n",
      "Test QuestionMention pairs - 75020 Question - 786\n",
      "=========== fold num -- 3 ===========\n",
      "Before filtering train set:\n",
      "Train QuestionMention pairs - 284364 Question - 3144\n",
      "After filtering train set:\n",
      "Train QuestionMention pairs - 249440 Question - 2857\n",
      "Test QuestionMention pairs - 66192 Question - 786\n",
      "=========== fold num -- 4 ===========\n",
      "Before filtering train set:\n",
      "Train QuestionMention pairs - 282068 Question - 3144\n",
      "After filtering train set:\n",
      "Train QuestionMention pairs - 250622 Question - 2866\n",
      "Test QuestionMention pairs - 68488 Question - 786\n",
      "=========== fold num -- 5 ===========\n",
      "Before filtering train set:\n",
      "Train QuestionMention pairs - 283895 Question - 3144\n",
      "After filtering train set:\n",
      "Train QuestionMention pairs - 252517 Question - 2871\n",
      "Test QuestionMention pairs - 66661 Question - 786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "\n",
    "df_data = pd.read_csv(\"data/lcquad/lcquad_train_test_sorted.csv\")\n",
    "question_list = np.array(df_data.Question.unique())\n",
    "questions_idxes = list(range(len(df_data.Question.unique())))\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "import os\n",
    "average_performance = []\n",
    "fold_num = 1\n",
    "for train_index, test_index in kf.split(questions_idxes):\n",
    "#     args.output_file_name = \"output/complex_kfolds_{}.csv\".format(fold_num)\n",
    "    train_ques_set = question_list[train_index]\n",
    "    test_ques_test = question_list[test_index]\n",
    "\n",
    "    df_train_split = df_data[df_data.Question.isin(train_ques_set)]\n",
    "    df_test_split = df_data[df_data.Question.isin(test_ques_test)]\n",
    "    \n",
    "    # filter out the questions with single positive or many negatives in trianing set\n",
    "    filtered_question_mentions = []\n",
    "    for qm in df_train_split.QuestionMention.unique():\n",
    "        df_ = df_train_split[df_train_split.QuestionMention == qm]\n",
    "        if df_.Label.sum() == 0:\n",
    "            filtered_question_mentions.append(qm)\n",
    "        if df_.Label.sum() == 1 and df_.shape[0] == 1:\n",
    "            filtered_question_mentions.append(qm)\n",
    "#             print(df_.Label.values)\n",
    "    df_train_split_filtered = df_train_split[~df_train_split.QuestionMention.isin(filtered_question_mentions)]\n",
    "    df_train_split_filtered = df_train_split_filtered.sort_values(by=['QuestionMention', 'Label'])\n",
    "#     print(list(df_train_split_filtered.Label.values))\n",
    "    \n",
    "    print(\"=========== fold num --\", fold_num, \"===========\")\n",
    "    print(\"Before filtering train set:\")\n",
    "    print('Train', 'QuestionMention pairs -', df_train_split.shape[0], 'Question -', df_train_split.Question.unique().shape[0])\n",
    "    print(\"After filtering train set:\")\n",
    "    print('Train', 'QuestionMention pairs -', df_train_split_filtered.shape[0], 'Question -', df_train_split_filtered.Question.unique().shape[0])\n",
    "    \n",
    "    print( 'Test', 'QuestionMention pairs -', df_test_split.shape[0], 'Question -', df_test_split.Question.unique().shape[0])\n",
    "\n",
    "# #     os.mkdir('data/qald-5-folds/split{}'.format(fold_num))\n",
    "    \n",
    "#     df_missing = pd.read_csv(\"data/missing.csv\", header=None)\n",
    "#     df_missing.columns = ['Unnamed:0', 'Question', 'Entities', 'Classes']\n",
    "#     df_missing = df_missing[['Question', 'Entities', 'Classes']]\n",
    "    Path('data/lcquad/lcquad-5-folds/split{}'.format(fold_num)).mkdir(parents=True, exist_ok=True)\n",
    "    df_test_split.to_csv('data/lcquad/lcquad-5-folds/split{}/test_fold.csv'.format(fold_num))\n",
    "    df_train_split.to_csv('data/lcquad/lcquad-5-folds/split{}/train_fold.csv'.format(fold_num))\n",
    "#     df_missing.to_csv('data/type-qald-5-folds/split{}/missing.csv'.format(fold_num))\n",
    "#     print( 'Test + missing questions', 'question pairs -', df_test.shape[0]+df_missing.shape[0], \n",
    "#           'questions -', df_test.Question.unique().shape[0]+df_missing.Question.shape[0])\n",
    "    fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from el_evaluation import *\n",
    "from utils import MyBatchSampler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils import QuestionSampler\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "random.seed(103)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"main training script for training lnn entity linking models\")\n",
    "parser.add_argument(\"--checkpoint_name\", type=str, default=\"checkpoint/best_model.pt\", help=\"checkpoint path\")\n",
    "parser.add_argument(\"--log_file_name\", type=str, default=\"log.txt\", help=\"log_file_name\")\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"exp_test\", help=\"checkpoint path\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"purename\", help=\"which model we choose\")\n",
    "parser.add_argument('--alpha', type=float, default=0.9, help='alpha for LNN')\n",
    "parser.add_argument('--num_epoch', type=int, default=100, help='training epochs for LNN')\n",
    "parser.add_argument(\"--use_fixed_threshold\", action=\"store_true\", help=\"default is to use binary`, otherwise use stem\")\n",
    "parser.add_argument(\"--use_kfolds\", action=\"store_true\", help=\"default is to use binary`, otherwise use stem\")\n",
    "parser.add_argument('--margin', type=float, default=0.601, help='margin for MarginRankingLoss')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument(\"-f\")  # to avoid breaking jupyter notebook\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.RuleLNN_nway_sigmoid_vec_type import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qald_metrics(pred_, m_labels_, ques_, mode='val'):\n",
    "    \"\"\"pred_ are 0/1 s after applying a threshold\"\"\"\n",
    "    rows = []\n",
    "    question_rows_map = {}\n",
    "    question_mention_set = set()\n",
    "    for i, pred in enumerate(pred_):\n",
    "        pred = pred.data.tolist()[0]\n",
    "        question = ques_[i]\n",
    "        if question not in question_rows_map:\n",
    "            question_rows_map[ques_[i]] = []\n",
    "        if pred:\n",
    "            men_entity_label = '_'.join(m_labels_[i].split(';')[-1].split())\n",
    "            men_entity_mention = '_'.join(m_labels_[i].split(';')[0].split())\n",
    "            if '-'.join([question, men_entity_mention]) in question_mention_set:\n",
    "                question_rows_map[ques_[i]][-1].add(\n",
    "                    ('http://dbpedia.org/resource/{}'.format(men_entity_label), pred))\n",
    "            else:\n",
    "                question_mention_set.add('-'.join([question, men_entity_mention]))\n",
    "                question_rows_map[ques_[i]].append(set())\n",
    "                question_rows_map[ques_[i]][-1].add(\n",
    "                    ('http://dbpedia.org/resource/{}'.format(men_entity_label), pred))\n",
    "    for key, preds_list_mentions in question_rows_map.items():\n",
    "        if len(preds_list_mentions) > 1:\n",
    "            rows.append([key, []])\n",
    "            for preds_set in preds_list_mentions:\n",
    "                sorted_values = sorted(list(preds_set), key=lambda x: x[1], reverse=True)[:5]\n",
    "                rows[-1][1].append(sorted_values)\n",
    "        elif len(preds_list_mentions) == 1:\n",
    "            sorted_values = sorted(list(preds_list_mentions[0]), key=lambda x: x[1], reverse=True)[:5]\n",
    "            rows.append([key, [sorted_values]])\n",
    "        else:\n",
    "            rows.append([key, []])\n",
    "\n",
    "    df_output = pd.DataFrame(rows, columns=['Question', 'Entities'])\n",
    "    df_output['Classes'] = str([])\n",
    "\n",
    "    # gold\n",
    "    benchmark = pd.read_csv('../../../data/gt_sparql.csv')\n",
    "    benchmark = benchmark.set_index('Question')\n",
    "    benchmark = benchmark.replace(np.nan, '', regex=True)\n",
    "    benchmark['Entities'] = benchmark['Entities'].astype(object)\n",
    "    is_qald_gt = True\n",
    "\n",
    "    # pred\n",
    "    predictions = df_output\n",
    "    # print(df_output.shape)\n",
    "    predictions = predictions.set_index('Question')\n",
    "    predictions['Entities'] = predictions['Entities']\n",
    "    predictions['Classes'] = predictions['Classes']\n",
    "\n",
    "    metrics = compute_metrics(benchmark=benchmark, predictions=predictions, limit=410, is_qald_gt=is_qald_gt,\n",
    "                              eval='full')\n",
    "\n",
    "    scores = metrics['macro']['named']\n",
    "    prec, recall, f1 = scores['precision'], scores['recall'], scores['f1']\n",
    "    return prec, recall, f1, df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking_loss(eval_model, x_, y_, m_labels_, ques_, loss_fn, mode='val'):\n",
    "    \"\"\"evaluate a model on validation data\"\"\"\n",
    "    eval_model.eval()\n",
    "\n",
    "    dataset_ = TensorDataset(x_, y_)\n",
    "    question_sampler = QuestionSampler(torch.utils.data.SequentialSampler(range(len(y_))), y_, False)\n",
    "    loader = DataLoader(dataset_, batch_sampler=question_sampler, shuffle=False)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ = eval_model(x_, m_labels_)\n",
    "\n",
    "        batch_num = 0\n",
    "        for xb, yb in loader:\n",
    "            if yb.shape[0] == 1:\n",
    "                # print(xb, yb)\n",
    "                continue\n",
    "            yhat = eval_model(xb, yb)\n",
    "            yb = yb.reshape(-1, 1)\n",
    "            yhat_pos = yhat[-1].repeat((len(yhat) - 1), 1)\n",
    "            yhat_neg = yhat[:-1]\n",
    "            loss = loss_fn(yhat_pos, yhat_neg, torch.ones((len(yhat) - 1), 1).to(device))\n",
    "            total_loss += loss.item() * (yb.shape[0] - 1)\n",
    "            batch_num += 1\n",
    "        avg_loss = total_loss / batch_num\n",
    "        prec, recall, f1, _ = get_qald_metrics(pred_, m_labels_, ques_, mode=mode)  # train and val both use 'val' mode\n",
    "\n",
    "    return avg_loss, f1, pred_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, checkpoint_name, num_epochs, margin, learning_rate):\n",
    "    \"\"\"train model and tune on validation set\"\"\"\n",
    "\n",
    "    # unwrapping the data\n",
    "    x_train, y_train, m_labels_train, ques_train = train_data\n",
    "    x_val, y_val, m_labels_val, ques_val = val_data\n",
    "    x_test, y_test, m_labels_test, ques_test = test_data\n",
    "\n",
    "    # initialize the loss function and optimizer\n",
    "    loss_fn = nn.MarginRankingLoss(margin=margin)  # MSELoss(), did not work neither\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_val_f1, best_val_loss = 0, 100000\n",
    "    best_train_val_loss = 1000000\n",
    "    # batch_size = 32\n",
    "\n",
    "    # stats before training\n",
    "    print(\"=========BEFORE TRAINING============\")\n",
    "    train_loss, train_f1, train_pred = evaluate_ranking_loss(model, x_train, y_train, m_labels_train, ques_train,\n",
    "                                                             loss_fn)\n",
    "    print(\"Train -- loss is {}; F1 is {}\".format(train_loss, train_f1))\n",
    "    val_loss, val_f1, val_pred = evaluate_ranking_loss(model, x_val, y_val, m_labels_val, ques_val, loss_fn)\n",
    "    print(\"Val --  loss is {}; F1 is {}\".format(val_loss, val_f1))\n",
    "    test_loss, test_f1, test_pred = evaluate_ranking_loss(model, x_test, y_test, m_labels_test, ques_test, loss_fn,\n",
    "                                                          mode='test')\n",
    "    print(\"Test -- loss is {}; F1 is {}\".format(test_loss, test_f1))\n",
    "\n",
    "    # start training\n",
    "    print(\"=========TRAINING============\")\n",
    "    dataset_train = TensorDataset(x_train, y_train)\n",
    "    question_sampler = QuestionSampler(torch.utils.data.SequentialSampler(range(len(y_train))), y_train, False)\n",
    "    loader = DataLoader(dataset_train, batch_sampler=question_sampler, shuffle=False)\n",
    "    # loader = DataLoader(dataset_train, sampler=torch.utils.data.SequentialSampler(dataset_train), batch_size=batch_size, shuffle=False)  # always False\n",
    "    # loader = DataLoader(dataset_train, sampler=torch.utils.data.WeightedRandomSampler(torch.FloatTensor([1, 100]), len(x_train), replacement=True), batch_size=64, shuffle=False)  # always False\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        idx = 0\n",
    "\n",
    "        # prev xb\n",
    "        prev_xb, prev_yb = None, None\n",
    "        for xb, yb in loader:\n",
    "            idx += 1\n",
    "            model.train()  # set train to true\n",
    "            optimizer.zero_grad()\n",
    "            # handle question with multiple positives\n",
    "            if yb.shape[0] == 1:\n",
    "                yb = prev_yb\n",
    "                # print(prev_xb)\n",
    "                # print(xb)\n",
    "                prev_xb[-1, :] = xb\n",
    "                xb = prev_xb\n",
    "                # print(xb)\n",
    "            yhat = model(xb, yb)\n",
    "            yb = yb.reshape(-1, 1)\n",
    "            yhat_pos = yhat[-1].repeat((len(yhat) - 1), 1)\n",
    "            yhat_neg = yhat[:-1]  # torch.mean(yhat[:-1], dim=0)\n",
    "\n",
    "            loss = loss_fn(yhat_pos, yhat_neg, torch.ones((len(yhat) - 1), 1).to(device))\n",
    "            if idx == 212:\n",
    "                print('loss', idx, loss)\n",
    "\n",
    "            total_loss += loss.item() * (yb.shape[0] - 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prev_xb, prev_yb = xb, yb\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, 'param -- data', param.data, 'grad -- ', param.grad)\n",
    "\n",
    "        # show status after each epoch\n",
    "        avg_loss = total_loss / idx\n",
    "        #         train_loss, train_f1, train_pred = evaluate(model, x_train, y_train, m_labels_train, ques_train, loss_fn)\n",
    "        print(\"Epoch \" + str(epoch) + \": avg train loss -- \" + str(avg_loss))\n",
    "        #         print(\"Train -- loss is {}; F1 is {}\".format(train_loss, train_f1))\n",
    "        val_loss, val_f1, val_pred = evaluate_ranking_loss(model, x_val, y_val, m_labels_val, ques_val, loss_fn)\n",
    "\n",
    "        if val_f1 >= best_val_f1:\n",
    "            # if val_f1 >= best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            # best_train_val_loss = val_loss + avg_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(best_model.state_dict(), checkpoint_name)\n",
    "        print(\"Val --  best loss is {}; F1 is {}\".format(best_val_loss, best_val_f1))\n",
    "\n",
    "        test_loss, test_f1, test_pred = evaluate_ranking_loss(model, x_test, y_test, m_labels_test, ques_test, loss_fn,\n",
    "                                                              mode='test')\n",
    "        print(\"Current Test -- loss is {}; F1 is {}\".format(test_loss, test_f1))\n",
    "\n",
    "    # show stats after training\n",
    "    print(\"=========AFTER TRAINING============\")\n",
    "    train_loss, train_f1, train_pred = evaluate_ranking_loss(best_model, x_train, y_train, m_labels_train, ques_train,\n",
    "                                                             loss_fn)\n",
    "    print(\"Train -- loss is {}; F1 is {}\".format(train_loss, train_f1))\n",
    "    val_loss, val_f1, val_pred = evaluate_ranking_loss(best_model, x_val, y_val, m_labels_val, ques_val, loss_fn)\n",
    "    print(\"Val --  loss is {}; F1 is {}\".format(val_loss, val_f1))\n",
    "    test_loss, test_f1, test_pred = evaluate_ranking_loss(best_model, x_test, y_test, m_labels_test, ques_test, loss_fn,\n",
    "                                                          mode='test')\n",
    "    print(\"Test -- loss is {}; F1 is {}\".format(test_loss, test_f1))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_test, m_labels_test, ques_test, alpha, checkpoint_name, model_name, output_file_name):\n",
    "    \"\"\"make predictions on test set\"\"\"\n",
    "    bestModel = pick_model(model_name, alpha)\n",
    "    bestModel.load_state_dict(torch.load(checkpoint_name))\n",
    "    bestModel.eval()\n",
    "    best_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pred = bestModel(x_test, m_labels_test)\n",
    "        prec, recall, f1, df_output = get_qald_metrics(test_pred, m_labels_test, ques_test, mode='test')\n",
    "        df_output.to_csv(output_file_name)\n",
    "        print(\"Test -- f1 is {} \".format(f1))\n",
    "        print(\"Test -- prec, recall, f1\", prec, recall, f1)\n",
    "        best_scores['precision'] = prec\n",
    "        best_scores['recall'] = recall\n",
    "        best_scores['f1'] = f1\n",
    "\n",
    "    # for name, mod in bestModel.named_modules():\n",
    "    #     if type(mod) == nn.ModuleList:\n",
    "    #         for name1, mod1 in mod.named_modules():\n",
    "    #             if 'cdd' not in name1 and 'AND' not in name1:\n",
    "    #                 if 'batch' in name1.lower():\n",
    "    #                     continue\n",
    "    #                 elif 'or_max' in name1.lower():\n",
    "    #                     continue\n",
    "    #                 elif 'and' in name1.lower():\n",
    "    #                     print(name1, mod1.cdd())\n",
    "    #                 elif 'or' in name1.lower():\n",
    "    #                     print(name1, mod1.AND.cdd())\n",
    "    #     else:\n",
    "    #         if 'cdd' not in name and 'AND' not in name:\n",
    "    #             if 'batch' in name.lower():\n",
    "    #                 continue\n",
    "    #             elif 'or_max' in name.lower():\n",
    "    #                 continue\n",
    "    #             elif 'and' in name.lower():\n",
    "    #                 print(name, mod.cdd())\n",
    "    #             elif 'or' in name.lower():\n",
    "    #                 print(name, mod.AND.cdd())\n",
    "    return test_pred, best_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_model(model_name, alpha):\n",
    "    if model_name == \"purename\":\n",
    "        return PureNameLNN(alpha, -1, False)\n",
    "    elif model_name == \"context\":\n",
    "        return ContextLNN(alpha, -1, False)\n",
    "    elif model_name == \"complex\":\n",
    "        return ComplexRuleLNN(alpha, -1, False)\n",
    "    else:\n",
    "        print(\"WRONG name input\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_file_name = \"output/complex_kfolds_{}.csv\".format(fold_num)\n",
    "\n",
    "df_train = pd.read_csv(\"./data/lcquad/type-lcquad-5-folds/split{}/train_fold.csv\".format(fold_num))\n",
    "df_test = pd.read_csv(\"./data/lcquad/type-lcquad-5-folds/split{}/test_fold.csv\".format(fold_num))\n",
    "\n",
    "# filter out the questions with single positive or many negatives in trianing set\n",
    "filtered_question_mentions = []\n",
    "for qm in df_train.QuestionMention.unique():\n",
    "    df_ = df_train[df_train.QuestionMention == qm]\n",
    "    if df_.Label.sum() == 0:\n",
    "        filtered_question_mentions.append(qm)\n",
    "    if df_.Label.sum() == 1 and df_.shape[0] == 1:\n",
    "        filtered_question_mentions.append(qm)\n",
    "#             print(df_.Label.values)\n",
    "df_train_split_filtered = df_train[~df_train.QuestionMention.isin(filtered_question_mentions)]\n",
    "df_train_split_filtered = df_train_split_filtered.sort_values(by=['QuestionMention', 'Label'])\n",
    "df_train = df_train_split_filtered\n",
    "\n",
    "# train\n",
    "features_train = np.array(\n",
    "    [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train.Features.values])\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(df_train.Label.values).float().reshape(-1, 1)\n",
    "m_labels_train = df_train.Mention_label.values\n",
    "ques_train = df_train.Question.values\n",
    "\n",
    "# test\n",
    "features_test = np.array(\n",
    "    [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_test.Features.values])\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(df_test.Label.values).float().reshape(-1, 1)\n",
    "m_labels_test = df_test.Mention_label.values\n",
    "ques_test = df_test.Question.values\n",
    "\n",
    "# train model and evaluate\n",
    "model = pick_model(args.model_name, args.alpha)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(\"model: \", args.model_name, args.alpha)\n",
    "print(model(x_train, m_labels_train))\n",
    "\n",
    "print(\"y_train sum\", sum(y_train), sum(y_train) / len(y_train))\n",
    "print(\"y_test sum\", sum(y_test), sum(y_test) / len(y_test))\n",
    "\n",
    "# aggregate the data into train, val, and test\n",
    "train_data = (x_train, y_train, m_labels_train, ques_train)\n",
    "print(\"train:\", x_train.shape, y_train.shape, m_labels_train.shape, ques_train.shape)\n",
    "test_data = (x_test, y_test, m_labels_test, ques_test)\n",
    "print(\"test:\", x_test.shape, y_test.shape, m_labels_test.shape, ques_test.shape)\n",
    "\n",
    "# check class distribution\n",
    "print(\"y_train sum\", sum(y_train), sum(y_train) / len(y_train))\n",
    "print(\"y_test sum\", sum(y_test), sum(y_test) / len(y_test))\n",
    "\n",
    "# train(model, train_data, test_data, test_data, args.checkpoint_name, args.num_epoch, args.margin,\n",
    "#       args.learning_rate)\n",
    "best_tuned_threshold = -1\n",
    "test_pred, best_scores = test(x_test, m_labels_test, ques_test, best_tuned_threshold, args.alpha, args.checkpoint_name, args.model_name, args.output_file_name)\n",
    "with open(\"output_w_spacy.txt\", 'a') as f:\n",
    "    f.write(\n",
    "        \"model={}; use_binary={}; alpha={}; p={}; r={}; f1={}\\n\".format(args.model_name,\n",
    "                                                                                          args.use_binary,\n",
    "                                                                                          args.alpha,\n",
    "                                                                                          best_scores[\n",
    "                                                                                              'precision'],\n",
    "                                                                                          best_scores[\n",
    "                                                                                              'recall'],\n",
    "                                                                                          best_scores['f1']))\n",
    "    print(\"model={}; use_binary={}; alpha={}; p={}; r={}; f1={}\\n\".format(args.model_name, args.use_binary,\n",
    "                                                                          args.alpha,\n",
    "                                                                          best_scores['precision'],\n",
    "                                                                          best_scores['recall'],\n",
    "                                                                          best_scores['f1']))\n",
    "average_performance.append([best_scores['precision'], best_scores['recall'], best_scores['f1']])\n",
    "fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg performance is prec - rec - f1:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/entity/lib/python3.6/site-packages/ipykernel_launcher.py:82: RuntimeWarning: Mean of empty slice.\n",
      "/Users/hjian42/opt/anaconda3/envs/entity/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# average_performance = []\n",
    "# for fold_num in range(1, 6):\n",
    "#     args.output_file_name = \"output/complex_kfolds_{}.csv\".format(fold_num)\n",
    "\n",
    "#     df_train = pd.read_csv(\"./data/lcquad/type-lcquad-5-folds/split{}/train_fold.csv\".format(fold_num))\n",
    "#     df_test = pd.read_csv(\"./data/lcquad/type-lcquad-5-folds/split{}/test_fold.csv\".format(fold_num))\n",
    "\n",
    "#     # filter out the questions with single positive or many negatives in trianing set\n",
    "#     filtered_question_mentions = []\n",
    "#     for qm in df_train.QuestionMention.unique():\n",
    "#         df_ = df_train[df_train.QuestionMention == qm]\n",
    "#         if df_.Label.sum() == 0:\n",
    "#             filtered_question_mentions.append(qm)\n",
    "#         if df_.Label.sum() == 1 and df_.shape[0] == 1:\n",
    "#             filtered_question_mentions.append(qm)\n",
    "#     #             print(df_.Label.values)\n",
    "#     df_train_split_filtered = df_train[~df_train.QuestionMention.isin(filtered_question_mentions)]\n",
    "#     df_train_split_filtered = df_train_split_filtered.sort_values(by=['QuestionMention', 'Label'])\n",
    "#     df_train = df_train_split_filtered\n",
    "    \n",
    "#     break\n",
    "#     # train\n",
    "#     features_train = np.array(\n",
    "#         [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train.Features.values])\n",
    "#     x_train = torch.from_numpy(features_train).float()\n",
    "#     y_train = torch.from_numpy(df_train.Label.values).float().reshape(-1, 1)\n",
    "#     m_labels_train = df_train.Mention_label.values\n",
    "#     ques_train = df_train.Question.values\n",
    "\n",
    "#     # test\n",
    "#     features_test = np.array(\n",
    "#         [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_test.Features.values])\n",
    "#     x_test = torch.from_numpy(features_test).float()\n",
    "#     y_test = torch.from_numpy(df_test.Label.values).float().reshape(-1, 1)\n",
    "#     m_labels_test = df_test.Mention_label.values\n",
    "#     ques_test = df_test.Question.values\n",
    "\n",
    "#     # train model and evaluate\n",
    "#     model = pick_model(args.model_name, args.alpha)\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     print(\"model: \", args.model_name, args.alpha)\n",
    "#     print(model(x_train, m_labels_train))\n",
    "\n",
    "#     print(\"y_train sum\", sum(y_train), sum(y_train) / len(y_train))\n",
    "#     print(\"y_test sum\", sum(y_test), sum(y_test) / len(y_test))\n",
    "\n",
    "#     # aggregate the data into train, val, and test\n",
    "#     train_data = (x_train, y_train, m_labels_train, ques_train)\n",
    "#     print(\"train:\", x_train.shape, y_train.shape, m_labels_train.shape, ques_train.shape)\n",
    "#     test_data = (x_test, y_test, m_labels_test, ques_test)\n",
    "#     print(\"test:\", x_test.shape, y_test.shape, m_labels_test.shape, ques_test.shape)\n",
    "\n",
    "#     # check class distribution\n",
    "#     print(\"y_train sum\", sum(y_train), sum(y_train) / len(y_train))\n",
    "#     print(\"y_test sum\", sum(y_test), sum(y_test) / len(y_test))\n",
    "\n",
    "#     # train(model, train_data, test_data, test_data, args.checkpoint_name, args.num_epoch, args.margin,\n",
    "#     #       args.learning_rate)\n",
    "#     best_tuned_threshold = -1\n",
    "#     test_pred, best_scores = test(x_test, m_labels_test, ques_test, best_tuned_threshold, args.alpha, args.checkpoint_name, args.model_name, args.output_file_name)\n",
    "#     with open(\"output_w_spacy.txt\", 'a') as f:\n",
    "#         f.write(\n",
    "#             \"model={}; use_binary={}; alpha={}; p={}; r={}; f1={}\\n\".format(args.model_name,\n",
    "#                                                                                               args.use_binary,\n",
    "#                                                                                               args.alpha,\n",
    "#                                                                                               best_scores[\n",
    "#                                                                                                   'precision'],\n",
    "#                                                                                               best_scores[\n",
    "#                                                                                                   'recall'],\n",
    "#                                                                                               best_scores['f1']))\n",
    "#         print(\"model={}; use_binary={}; alpha={}; p={}; r={}; f1={}\\n\".format(args.model_name, args.use_binary,\n",
    "#                                                                               args.alpha,\n",
    "#                                                                               best_scores['precision'],\n",
    "#                                                                               best_scores['recall'],\n",
    "#                                                                               best_scores['f1']))\n",
    "#     average_performance.append([best_scores['precision'], best_scores['recall'], best_scores['f1']])\n",
    "#     fold_num += 1\n",
    "\n",
    "# average_performance = np.array(average_performance)\n",
    "# print(\"Avg performance is prec - rec - f1: \", average_performance.mean(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjian42/opt/anaconda3/envs/entity/lib/python3.6/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "features_train = np.array(\n",
    "    [np.fromstring(s[1:-1], dtype=np.float, sep=', ') for s in df_train.Features.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        10\n",
       "1        10\n",
       "2        10\n",
       "3        10\n",
       "4        10\n",
       "         ..\n",
       "16181    10\n",
       "16182    10\n",
       "16183    10\n",
       "16184    10\n",
       "16185    10\n",
       "Name: Features, Length: 16186, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.Features.str.split(\",\").apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.9066666960716248, 0.5, 0.5333333333333333, 0.7606099446321317, 1.0, 1.0, 8.0, 1.0, 1.0, 0]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_train.Features[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c257dcc92b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "x_train = torch.from_numpy(features_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_el_kfolds_type.py \\\n",
    "--experiment_name exp_complex_pure_ctx_type \\\n",
    "--model_name complex_pure_ctx_type \\\n",
    "--num_epoch 25 \\\n",
    "--margin 0.601 \\\n",
    "--alpha 0.9 \\\n",
    "--learning_rate 0.001 \\\n",
    "--log_file_name exp_complex_pure_ctx_type.log \\\n",
    "--checkpoint_name checkpoint/best_model_complex_pure_ctx_type.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
